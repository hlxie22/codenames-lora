Directory structure:
└── codenames-lora/
    ├── .gitingestignore
    ├── configs/
    │   ├── default.yml
    │   └── environment.yml
    └── src/
        ├── __init__.py
        ├── build_vocab.py
        ├── eval.py
        ├── gen_cfg.py
        ├── generate_sft_data.py
        ├── guesser_prompt.py
        ├── io_utils.py
        ├── make_boards.py
        ├── metrics.py
        ├── model_wrappers.py
        ├── mp_utils.py
        ├── prompting.py
        ├── rollout.py
        ├── rules.py
        ├── spymaster_prompt.py
        ├── train_lora_sft.py
        └── utils.py

================================================
FILE: .gitingestignore
================================================
/run.slurm


================================================
FILE: configs/default.yml
================================================
vocab:
  lang: "en"
  stop_n: 300
  min_zipf: 3.0
  max_zipf: 6.0
  min_len: 3
  max_len: 12
  max_words: 30000
  token_regex: "^[a-z]+$"
  extra_banlist: null
  manifest_path: "data/vocab_manifest.json"

inference:
  backend: "vllm"   # "hf" or "vllm"

  num_processes: 2
  batch_size: 12

  vllm:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.90
    max_model_len: 4096
    dtype: "bfloat16"   # "auto" | "float16" | "bfloat16"
    trust_remote_code: true

    enable_lora: true
    max_lora_rank: 64

paths:
  data_dir: "data"
  outputs_dir: "outputs"
  vocab_path: "data/vocab.txt"
  boards_train_path: "data/boards_train.jsonl"
  boards_eval_path: "data/boards_eval.jsonl"
  sft_turns_path: "data/sft_turns.jsonl"
  sft_turns_raw_path: "data/sft_turns_raw.jsonl"

boards:
  sft_max_train_boards: 2000
  n_train_boards: 2000
  n_eval_boards: 400
  board_size: 25
  n_team: 9
  n_opp: 8
  n_neu: 7
  n_assassin: 1
  seed_train: 123
  seed_eval: 456
  allow_duplicates: false
  avoid_repeat_boards_within_split: true
  avoid_repeat_boards_across_splits: true

models:
  spymaster_model_id: "Qwen/Qwen3-8B"
  guesser_model_id: "Qwen/Qwen3-8B"
  embedding_model_id: "sentence-transformers/all-MiniLM-L6-v2"

qwen:
  use_chat_template: true
  enable_thinking_spymaster: true
  enable_thinking_guesser: true
  system_spymaster: "You are the Spymaster in a Codenames-like game."
  system_guesser: "You are the Guesser in a Codenames-like game."

decoding:
  progress_every: 20
  flush_every: 10

  spymaster_temperature: 0.6
  spymaster_top_p: 0.95
  spymaster_top_k: 20
  spymaster_max_new_tokens: 4096

  guesser_temperature: 0.6
  guesser_top_p: 0.95
  guesser_top_k: 20
  guesser_max_new_tokens: 2048

  n_candidates: 3
  max_resamples: 4

constraints:
  single_word_only: true
  ban_board_words: true
  ban_substrings: true
  enable_directness_check: false
  tau_direct: 0.52
  directness_metric: "cosine"

reward:
  team_correct: 1.0
  opp_wrong: -1.0
  neu_wrong: -0.5
  assassin_wrong: -3.0
  repeat_penalty: 0.0
  brevity_bonus: 0.0

filtering:
  mode: "rule_based"   # "top_percent" or "rule_based"
  top_percent: 0.4
  min_team_correct: 2
  require_no_assassin: true

training:
  seed: 999
  r: 16
  alpha: 32
  dropout: 0.05
  lr: 2.0e-4
  epochs: 2
  batch_size: 2
  grad_accum: 8
  max_seq_len: 4096
  output_adapter_dir: "outputs/sft/lora_spymaster"
  bf16: true
  fp16: false
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]


================================================
FILE: configs/environment.yml
================================================
name: codenames-lora
channels:
  - conda-forge

dependencies:
  - python=3.10
  - pip
  - git
  - git-lfs
  - setuptools
  - wheel

  # core utilities
  - numpy>=1.26
  - pyyaml>=6.0
  - tqdm
  - regex
  - scipy
  - pandas
  - psutil

  # keep these on conda to avoid pip/conda duplication
  - sentencepiece
  - protobuf

  - pip:
      # PyTorch CUDA wheels (keep as EXTRA index so PyPI still works)
      - --extra-index-url https://download.pytorch.org/whl/cu128
      - torch==2.7.0
      - torchvision==0.22.0
      - torchaudio==2.7.0

      # vLLM: pin to avoid pip grabbing latest (and trying to build from source)
      - vllm==0.9.2

      # HF stack
      - transformers==4.53.3
      - accelerate>=0.34.0
      - peft>=0.12.0
      - huggingface-hub>=0.25.0
      - safetensors>=0.4.3
      - tokenizers>=0.19.1

      # embeddings
      - sentence-transformers>=3.0.0

      # vocab builder
      - wordfreq>=3.1.0

      # tokenizer backend
      - tiktoken>=0.7.0


================================================
FILE: src/__init__.py
================================================
[Empty file]


================================================
FILE: src/build_vocab.py
================================================
#!/usr/bin/env python3
"""
Build a clean Codenames-style vocabulary file using wordfreq.

Approach (hybrid):
- remove ultra-common stopwords: top_n_list('en', STOP_N)
- keep only words within a Zipf frequency window: MIN_ZIPF <= zipf <= MAX_ZIPF
- keep only lowercase alphabetic tokens (default: ^[a-z]+$)
- filter by length
- apply a small curated "ban list" for months/days/numbers/meta-words
- write data/vocab.txt (one word per line)
- optionally write a manifest with stats

Usage:
  pip install wordfreq
  python -m src.build_vocab --out data/vocab.txt --manifest data/vocab_manifest.json

Notes:
- Output words are lowercased.
- Ordering is deterministic: sorted by (-zipf, word).
"""

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import Iterable, List, Set, Tuple

from .utils import load_yaml

from wordfreq import iter_wordlist, top_n_list, zipf_frequency


DEFAULT_BAN = {
    # days
    "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday",
    # months
    "january", "february", "march", "april", "may", "june", "july", "august",
    "september", "october", "november", "december",
    # numbers (word forms)
    "zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",
    "first", "second", "third",
    # ultra-generic/meta (often unfun boards)
    "thing", "things", "stuff", "object", "objects", "person", "people", "someone", "somebody",
    "anything", "everything", "nothing", "something",
    # directions (optional; remove if you like them)
    "north", "south", "east", "west", "left", "right",
}


def load_extra_banlist(path: str | None) -> Set[str]:
    if not path:
        return set()
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Banlist file not found: {path}")
    banned = set()
    for line in p.read_text(encoding="utf-8").splitlines():
        w = line.strip().lower()
        if not w or w.startswith("#"):
            continue
        banned.add(w)
    return banned


def build_vocab(
    *,
    lang: str,
    stop_n: int,
    min_zipf: float,
    max_zipf: float,
    min_len: int,
    max_len: int,
    token_regex: str,
    extra_ban: Set[str],
    max_words: int | None = None,
) -> Tuple[List[str], dict]:
    token_re = re.compile(token_regex)

    stop = set(top_n_list(lang, stop_n))
    banned = set(DEFAULT_BAN) | set(extra_ban)

    kept: List[Tuple[str, float]] = []
    seen = set()

    n_total = 0
    n_stop = 0
    n_badshape = 0
    n_len = 0
    n_zipf = 0
    n_banned = 0

    for w in iter_wordlist(lang):
        n_total += 1
        w = w.strip().lower()
        if not w:
            continue

        if w in seen:
            continue
        seen.add(w)

        if w in stop:
            n_stop += 1
            continue

        if w in banned:
            n_banned += 1
            continue

        if not token_re.match(w):
            n_badshape += 1
            continue

        if len(w) < min_len or len(w) > max_len:
            n_len += 1
            continue

        z = float(zipf_frequency(w, lang))
        if z < min_zipf or z > max_zipf:
            n_zipf += 1
            continue

        kept.append((w, z))

    # deterministic ordering: more frequent first, then alpha
    kept.sort(key=lambda t: (-t[1], t[0]))

    n_pre_cap = len(kept)
    if max_words is not None:
        if int(max_words) <= 0:
            raise ValueError(f"max_words must be > 0 (got {max_words})")
        kept = kept[: int(max_words)]

    vocab = [w for (w, _) in kept]
    zs = [z for (_, z) in kept]

    manifest = {
        "lang": lang,
        "stop_n": stop_n,
        "min_zipf": min_zipf,
        "max_zipf": max_zipf,
        "min_len": min_len,
        "max_len": max_len,
        "token_regex": token_regex,
        "max_words": int(max_words) if max_words is not None else None,
        "counts": {
            "total_seen": n_total,
            "kept_pre_cap": n_pre_cap,
            "kept": len(vocab),
            "capped": max(0, n_pre_cap - len(vocab)),
            "filtered_stopwords": n_stop,
            "filtered_banned": n_banned,
            "filtered_badshape": n_badshape,
            "filtered_length": n_len,
            "filtered_zipf": n_zipf,
        },
        "zipf_stats": {
            "min": min(zs) if zs else None,
            "max": max(zs) if zs else None,
            "mean": (sum(zs) / len(zs)) if zs else None,
            "p10": percentile(zs, 10) if zs else None,
            "p50": percentile(zs, 50) if zs else None,
            "p90": percentile(zs, 90) if zs else None,
        },
        "examples_top20": vocab[:20],
    }
    return vocab, manifest


def percentile(values: List[float], p: float) -> float:
    # small deterministic percentile helper without numpy
    if not values:
        return float("nan")
    xs = sorted(values)
    if p <= 0:
        return xs[0]
    if p >= 100:
        return xs[-1]
    k = (len(xs) - 1) * (p / 100.0)
    f = int(k)
    c = min(f + 1, len(xs) - 1)
    if f == c:
        return xs[f]
    d = k - f
    return xs[f] * (1 - d) + xs[c] * d


def write_vocab(out_path: str, vocab: List[str]) -> None:
    p = Path(out_path)
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text("\n".join(vocab) + "\n", encoding="utf-8")


def write_manifest(path: str, manifest: dict) -> None:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(manifest, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")

def main():
    # Internal defaults (used if neither CLI nor config provide values)
    OUT_DEFAULT = "data/vocab.txt"
    LANG_DEFAULT = "en"
    STOP_N_DEFAULT = 300
    MIN_ZIPF_DEFAULT = 3.0
    MAX_ZIPF_DEFAULT = 6.0
    MIN_LEN_DEFAULT = 3
    MAX_LEN_DEFAULT = 12
    TOKEN_REGEX_DEFAULT = r"^[a-z]+$"

    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default=None, help="Optional YAML config path (e.g., configs/default.yaml)")

    # If provided, CLI overrides config; if omitted, config (then defaults) are used.
    ap.add_argument("--out", default=None, help="Output vocab path (overrides config paths.vocab_path)")
    ap.add_argument("--manifest", default=None, help="Optional JSON manifest path (overrides config vocab.manifest_path)")

    ap.add_argument("--lang", default=None, help="Language code for wordfreq (overrides config vocab.lang)")
    ap.add_argument("--stop-n", type=int, default=None, help="Remove top-N most frequent words as stopwords")
    ap.add_argument("--min-zipf", type=float, default=None, help="Minimum Zipf frequency to keep")
    ap.add_argument("--max-zipf", type=float, default=None, help="Maximum Zipf frequency to keep (drops ultra-common)")
    ap.add_argument("--min-len", type=int, default=None, help="Minimum token length")
    ap.add_argument("--max-len", type=int, default=None, help="Maximum token length")
    ap.add_argument(
        "--max-words",
        type=int,
        default=None,
        help="Optional cap on vocab size after filtering/sorting (top-N by Zipf). Overrides config vocab.max_words.",
    )
    ap.add_argument(
        "--token-regex",
        default=None,
        help="Regex a token must match (default keeps only lowercase alphabetic words).",
    )
    ap.add_argument(
        "--extra-banlist",
        default=None,
        help="Optional path to a newline-separated banlist (overrides config vocab.extra_banlist).",
    )

    args = ap.parse_args()

    cfg = load_yaml(args.config) if args.config else {}
    vcfg = cfg.get("vocab", {}) if isinstance(cfg, dict) else {}
    pcfg = cfg.get("paths", {}) if isinstance(cfg, dict) else {}

    out_path = (
        args.out
        or pcfg.get("vocab_path")
        or vcfg.get("out")
        or OUT_DEFAULT
    )
    manifest_path = (
        args.manifest
        or vcfg.get("manifest_path")
        or None
    )

    lang = args.lang or vcfg.get("lang") or LANG_DEFAULT
    stop_n = args.stop_n if args.stop_n is not None else int(vcfg.get("stop_n", STOP_N_DEFAULT))
    min_zipf = args.min_zipf if args.min_zipf is not None else float(vcfg.get("min_zipf", MIN_ZIPF_DEFAULT))
    max_zipf = args.max_zipf if args.max_zipf is not None else float(vcfg.get("max_zipf", MAX_ZIPF_DEFAULT))
    min_len = args.min_len if args.min_len is not None else int(vcfg.get("min_len", MIN_LEN_DEFAULT))
    max_len = args.max_len if args.max_len is not None else int(vcfg.get("max_len", MAX_LEN_DEFAULT))
    token_regex = args.token_regex or vcfg.get("token_regex") or TOKEN_REGEX_DEFAULT
    max_words = args.max_words if args.max_words is not None else vcfg.get("max_words", None)
    if max_words is not None:
        max_words = int(max_words)

    extra_banlist_path = args.extra_banlist or vcfg.get("extra_banlist") or None
    extra_ban = load_extra_banlist(extra_banlist_path)

    vocab, manifest = build_vocab(
        lang=lang,
        stop_n=stop_n,
        min_zipf=min_zipf,
        max_zipf=max_zipf,
        min_len=min_len,
        max_len=max_len,
        token_regex=token_regex,
        extra_ban=extra_ban,
        max_words=max_words,
    )

    write_vocab(out_path, vocab)
    print(f"Wrote {len(vocab)} words -> {out_path}")

    if manifest_path:
        write_manifest(manifest_path, manifest)
        print(f"Wrote manifest -> {manifest_path}")
    else:
        # quick console summary
        print(json.dumps(manifest["counts"], indent=2))


if __name__ == "__main__":
    main()


================================================
FILE: src/eval.py
================================================
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List

import numpy as np

from .io_utils import shard_path_insert_before_suffix, merge_jsonl_shards
from .metrics import aggregate
from .model_wrappers import Embedder, make_text_generator, load_lora_on_generator
from .mp_utils import is_child_process, child_shard_info, launch_children
from .rollout import run_turns_batched
from .utils import (
    ensure_dir,
    load_yaml,
    read_jsonl,
    save_config_snapshot,
    save_run_meta,
    set_global_seed,
    write_jsonl,
)


def _shard_out_path(out_dir: str | Path, shard_id: int, num_shards: int) -> Path:
    out_dir = Path(out_dir)
    return shard_path_insert_before_suffix(out_dir / "per_board.jsonl", shard_id, num_shards)


def _merge_shards(out_dir: str | Path, num_shards: int) -> List[Dict[str, Any]]:
    out_dir = Path(out_dir)
    shard_paths = [_shard_out_path(out_dir, sid, num_shards) for sid in range(num_shards)]
    combined = merge_jsonl_shards(shard_paths, sort_key=lambda r: str(r.get("board_id", "")))
    return combined


# -------------------------
# vLLM LoRA toggling proxy (so we can share one engine)
# -------------------------

class _VLLMProxy:
    """
    Delegate to a base generator but force a particular LoRA request for calls.
    Works with VLLMTextGenerator which stores LoRARequest on `._lora_request`.
    """

    def __init__(self, base, lora_request):
        self._base = base
        self._lora_request = lora_request
        self.model_id = getattr(base, "model_id", "unknown")

    def format_chat(self, *args, **kwargs):
        return self._base.format_chat(*args, **kwargs)

    def generate(self, *args, **kwargs):
        old = getattr(self._base, "_lora_request", None)
        try:
            setattr(self._base, "_lora_request", self._lora_request)
            return self._base.generate(*args, **kwargs)
        finally:
            setattr(self._base, "_lora_request", old)

    def generate_batch(self, *args, **kwargs):
        old = getattr(self._base, "_lora_request", None)
        try:
            setattr(self._base, "_lora_request", self._lora_request)
            return self._base.generate_batch(*args, **kwargs)
        finally:
            setattr(self._base, "_lora_request", old)


# -------------------------
# Main
# -------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    ap.add_argument("--mode", choices=["baseline", "sft"], required=True)
    ap.add_argument("--out", required=True, help="Output directory (e.g., outputs/baselines/run1)")
    args = ap.parse_args()

    cfg = load_yaml(args.config)
    set_global_seed(int(cfg["training"].get("seed", 0)))

    num_procs = int(cfg.get("inference", {}).get("num_processes", 1))
    batch_size = int(cfg.get("inference", {}).get("batch_size", 1))

    out_dir = ensure_dir(args.out)

    # Master process: write run meta, spawn children, then merge + aggregate
    if num_procs > 1 and not is_child_process():
        save_config_snapshot(cfg, out_dir)
        save_run_meta(
            out_dir,
            command=["python", "-m", "src.eval", "--config", args.config, "--mode", args.mode, "--out", args.out],
        )

        if cfg.get("inference", {}).get("backend") == "vllm":
            tp = int(cfg.get("inference", {}).get("vllm", {}).get("tensor_parallel_size", 1))
            if tp != 1:
                print(
                    f"[warn] inference.vllm.tensor_parallel_size={tp} with inference.num_processes={num_procs}. "
                    f"Usually you want tensor_parallel_size=1 when using multiple processes."
                )

        # mp_utils.launch_children(module, argv, num_procs)
        launch_children("src.eval", ["--config", args.config, "--mode", args.mode, "--out", args.out], num_procs)

        # Merge shard outputs and compute metrics once
        per_board = _merge_shards(out_dir, num_procs)
        per_path = Path(out_dir) / "per_board.jsonl"
        write_jsonl(per_path, per_board)

        metrics = aggregate(per_board)
        with open(Path(out_dir) / "metrics.json", "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2)

        print(f"Wrote per-board -> {per_path}")
        print(f"Wrote metrics   -> {Path(out_dir) / 'metrics.json'}")
        return

    # Worker (or single process)
    if num_procs == 1 and not is_child_process():
        save_config_snapshot(cfg, out_dir)
        save_run_meta(
            out_dir,
            command=["python", "-m", "src.eval", "--config", args.config, "--mode", args.mode, "--out", args.out],
        )

    boards = read_jsonl(cfg["paths"]["boards_eval_path"])

    shard_id, num_shards = child_shard_info() if is_child_process() else (0, 1)
    if num_shards > 1:
        boards = [b for i, b in enumerate(boards) if (i % num_shards) == shard_id]
        print(f"[shard {shard_id}/{num_shards}] boards={len(boards)}")

    # Build generators (avoid two vLLM engines when model_ids match)
    backend = cfg.get("inference", {}).get("backend", "hf")
    sp_id = cfg["models"]["spymaster_model_id"]
    g_id = cfg["models"]["guesser_model_id"]

    if backend == "vllm" and sp_id == g_id:
        base = make_text_generator(sp_id, cfg)

        if args.mode == "sft":
            adapter_dir = cfg["training"]["output_adapter_dir"]
            base = load_lora_on_generator(base, adapter_dir)
            # capture LoRA request then disable it for guesser calls
            lora_req = getattr(base, "_lora_request", None)
            setattr(base, "_lora_request", None)

            spymaster = _VLLMProxy(base, lora_req)
            guesser = _VLLMProxy(base, None)
        else:
            spymaster = base
            guesser = base
    else:
        # General case: separate generators
        spymaster = make_text_generator(sp_id, cfg)
        if args.mode == "sft":
            adapter_dir = cfg["training"]["output_adapter_dir"]
            spymaster = load_lora_on_generator(spymaster, adapter_dir)
        guesser = make_text_generator(g_id, cfg)

    # Embedder
    use_embed = bool(cfg.get("constraints", {}).get("enable_directness_check", True))
    embedder = None
    if use_embed:
        device = "cuda" if __import__("torch").cuda.is_available() else "cpu"
        embedder = Embedder(cfg["models"]["embedding_model_id"], device=device)

    # Evaluate
    per_board: List[Dict[str, Any]] = []

    # Keep n_candidates=1 in eval
    n_candidates = 1

    for start in range(0, len(boards), max(1, batch_size)):
        batch = boards[start : start + max(1, batch_size)]

        bests, metas = run_turns_batched(
            batch,
            spymaster,
            guesser,
            embedder,
            cfg,
            n_candidates=n_candidates,
        )

        for b, best, meta in zip(batch, bests, metas):
            rec = {
                "board_id": b["board_id"],
                "reward": float(best.reward),
                "clue": best.clue,
                "num": int(best.num),
                "guess_words": best.guess_words,
                "stats": {**best.stats, "directness": float(best.directness)},
                "clue_meta": {
                    "valid": bool(best.valid),
                    "rejected_total": int(meta["rejected_total"]),
                    "rejection_counts": meta["rejection_counts"],
                },
            }
            per_board.append(rec)

        done = min(start + len(batch), len(boards))
        if done % 50 == 0 or done == len(boards):
            mr = float(np.mean([r["reward"] for r in per_board])) if per_board else 0.0
            print(f"[shard {shard_id}/{num_shards}] [{done}/{len(boards)}] mean_reward={mr:.3f}")

    # Write outputs
    if num_shards > 1 and is_child_process():
        shard_path = _shard_out_path(out_dir, shard_id, num_shards)
        write_jsonl(shard_path, per_board)
        print(f"[shard {shard_id}/{num_shards}] Wrote per-board shard -> {shard_path}")
        return

    # Single-process final outputs
    per_path = Path(out_dir) / "per_board.jsonl"
    write_jsonl(per_path, per_board)

    metrics = aggregate(per_board)
    with open(Path(out_dir) / "metrics.json", "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)

    print(f"Wrote per-board -> {per_path}")
    print(f"Wrote metrics   -> {Path(out_dir) / 'metrics.json'}")


if __name__ == "__main__":
    main()


================================================
FILE: src/gen_cfg.py
================================================
# src/gen_cfg.py
from .model_wrappers import GenerationConfig

def spymaster_gen_cfg(cfg) -> GenerationConfig:
    d = cfg["decoding"]
    return GenerationConfig(
        temperature=float(d["spymaster_temperature"]),
        top_p=float(d["spymaster_top_p"]),
        top_k=int(d.get("spymaster_top_k", 20)),
        max_new_tokens=int(d["spymaster_max_new_tokens"]),
    )

def guesser_gen_cfg(cfg) -> GenerationConfig:
    d = cfg["decoding"]
    return GenerationConfig(
        temperature=float(d["guesser_temperature"]),
        top_p=float(d["guesser_top_p"]),
        top_k=int(d.get("guesser_top_k", 20)),
        max_new_tokens=int(d["guesser_max_new_tokens"]),
    )


================================================
FILE: src/generate_sft_data.py
================================================
# src/generate_sft_data.py
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List

import numpy as np

from .io_utils import shard_path_append_to_suffix, merge_jsonl_shards
from .model_wrappers import Embedder, make_text_generator
from .mp_utils import launch_children, is_child_process, child_shard_info
from .prompting import render_prompt
from .rollout import run_turns_batched
from .spymaster_prompt import build_spymaster_messages
from .utils import load_yaml, read_jsonl, set_global_seed, write_jsonl, save_progress


# -------------------------
# Resume helpers
# -------------------------

def load_done_example_ids(raw_path: str | Path) -> set[str]:
    """Return example_ids already present in an existing raw jsonl (for resume)."""
    raw_path = Path(raw_path)
    done: set[str] = set()
    if not raw_path.exists():
        return done

    with raw_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                eid = obj.get("example_id") or obj.get("board_id")
                if eid:
                    done.add(str(eid))
            except Exception:
                # If a partial/corrupt last line exists, ignore it
                continue
    return done


def extract_think_block(text: str) -> str:
    # Keep the model-produced think block if present; otherwise return "".
    lo = text.lower().rfind("<think>")
    hi = text.lower().rfind("</think>")
    if lo != -1 and hi != -1 and hi > lo:
        return text[lo : hi + len("</think>")].strip()
    # Sometimes you might only see </think>; keep everything up to it as "thinking".
    if hi != -1:
        return text[: hi + len("</think>")].strip()
    return ""


# -------------------------
# Filtering
# -------------------------

def filter_examples(records: List[Dict[str, Any]], cfg: Dict[str, Any]) -> List[Dict[str, Any]]:
    fcfg = cfg["filtering"]
    mode = fcfg["mode"]

    if not records:
        return []

    if mode == "top_percent":
        top_p = float(fcfg["top_percent"])
        rewards = np.array([r["reward"] for r in records], dtype=np.float32)
        thr = float(np.quantile(rewards, 1.0 - top_p))
        return [r for r in records if float(r["reward"]) >= thr]

    if mode == "rule_based":
        min_team = int(fcfg.get("min_team_correct", 0))
        require_no_assassin = bool(fcfg.get("require_no_assassin", False))
        kept = []
        for r in records:
            st = r.get("stats", {})
            if int(st.get("n_team", 0)) < min_team:
                continue
            if require_no_assassin and int(st.get("assassin", 0)) > 0:
                continue
            kept.append(r)
        return kept

    raise ValueError(f"Unknown filtering mode: {mode}")


def _merge_shards_to_base(base_raw_path: str | Path, num_shards: int) -> List[Dict[str, Any]]:
    shard_paths = [shard_path_append_to_suffix(base_raw_path, sid, num_shards) for sid in range(num_shards)]
    combined = merge_jsonl_shards(shard_paths)
    write_jsonl(base_raw_path, combined)
    return combined


# -------------------------
# Main
# -------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()

    cfg = load_yaml(args.config)

    num_procs = int(cfg.get("inference", {}).get("num_processes", 1))
    batch_size = int(cfg.get("inference", {}).get("batch_size", 1))

    # Master: spawn workers and merge/filter when done
    if num_procs > 1 and not is_child_process():
        if cfg.get("inference", {}).get("backend") == "vllm":
            tp = int(cfg.get("inference", {}).get("vllm", {}).get("tensor_parallel_size", 1))
            if tp != 1:
                print(
                    f"[warn] inference.vllm.tensor_parallel_size={tp} with inference.num_processes={num_procs}. "
                    f"Usually you want tensor_parallel_size=1 when using multiple processes."
                )

        launch_children("src.generate_sft_data", ["--config", args.config], num_procs)

        base_raw_path = cfg.get("paths", {}).get("sft_turns_raw_path")
        if not base_raw_path:
            raise RuntimeError("paths.sft_turns_raw_path must be set when using inference.num_processes > 1")

        combined = _merge_shards_to_base(base_raw_path, num_procs)
        filtered = filter_examples(combined, cfg)
        write_jsonl(cfg["paths"]["sft_turns_path"], filtered)
        print(f"Merged {len(combined)} raw -> Filtered {len(filtered)} -> {cfg['paths']['sft_turns_path']}")
        return

    # Worker or single-process mode
    set_global_seed(int(cfg["training"].get("seed", 0)))

    boards = read_jsonl(cfg["paths"]["boards_train_path"])

    # Apply cap BEFORE sharding so total dataset size is bounded
    maxb = cfg.get("boards", {}).get("sft_max_train_boards", None)
    if maxb is not None:
        boards = boards[: int(maxb)]
        print(f"Using only first {len(boards)} train boards for SFT generation (boards.sft_max_train_boards={maxb}).")

    shard_id, num_shards = child_shard_info() if is_child_process() else (0, 1)

    # Shard boards by index
    if num_shards > 1:
        boards = [b for i, b in enumerate(boards) if (i % num_shards) == shard_id]
        print(f"[shard {shard_id}/{num_shards}] boards={len(boards)}")

    # Models
    gen = make_text_generator(cfg["models"]["spymaster_model_id"], cfg)
    spymaster = gen
    guesser = gen

    # Embedder (OPTIONAL via constraints.enable_directness_check)
    use_embed = bool(cfg.get("constraints", {}).get("enable_directness_check", True))
    embedder = None
    if use_embed:
        device = "cuda" if __import__("torch").cuda.is_available() else "cpu"
        embedder = Embedder(cfg["models"]["embedding_model_id"], device=device)

    n_candidates = int(cfg["decoding"]["n_candidates"])
    progress_every = int(cfg.get("decoding", {}).get("progress_every", 50))

    # Paths (use per-shard raw file to avoid concurrent appends)
    base_raw_path = cfg.get("paths", {}).get("sft_turns_raw_path")
    if not base_raw_path:
        raise RuntimeError("paths.sft_turns_raw_path must be set")

    raw_path = Path(base_raw_path)
    if num_shards > 1:
        raw_path = shard_path_append_to_suffix(raw_path, shard_id, num_shards)

    done_ids = load_done_example_ids(raw_path)
    if done_ids:
        print(f"[shard {shard_id}/{num_shards}] Resuming: found {len(done_ids)} already-written examples in {raw_path}")

    progress_path = raw_path.with_suffix(raw_path.suffix + ".progress.json")

    # -------------------------
    # Buffered writer
    # -------------------------
    raw_path.parent.mkdir(parents=True, exist_ok=True)
    f_raw = open(raw_path, "a", encoding="utf-8")
    writes_since_flush = 0
    FLUSH_EVERY = int(cfg.get("decoding", {}).get("flush_every", 10))

    # Track running reward without storing all records
    running_sum = 0.0
    running_n = 0

    # NOTE: To keep files small, we do NOT store raw texts for each candidate by default.
    # Flip this to True if you explicitly want the raw model outputs per candidate.
    INCLUDE_RAW_TEXTS = False

    try:
        for start in range(0, len(boards), max(1, batch_size)):
            batch = boards[start : start + max(1, batch_size)]

            bests, metas, all_cands = run_turns_batched(
                batch,
                spymaster,
                guesser,
                embedder,  # may be None if directness check disabled
                cfg,
                n_candidates=n_candidates,
                return_candidates=True,  # <-- variance logging
            )

            for b, best, meta, cands in zip(batch, bests, metas, all_cands):
                example_id = b["board_id"]
                if example_id in done_ids:
                    continue

                revealed = [False] * len(b["board_words"])
                msgs = build_spymaster_messages(b["board_words"], b["labels"], revealed, cfg)

                # Centralized prompt rendering (no duplicated chat-template/thinking logic)
                prompt = render_prompt(spymaster, msgs, cfg, role="spymaster")

                think_block = extract_think_block(best.raw_spymaster_text)
                completion = ""
                if think_block:
                    completion += think_block + "\n"
                completion += f"CLUE: {best.clue}\nNUM: {best.num}\n"

                # Best candidate index (object identity should match; fallback to value match)
                best_idx = None
                for j, c in enumerate(cands):
                    if c is best:
                        best_idx = j
                        break
                if best_idx is None:
                    for j, c in enumerate(cands):
                        if (
                            c.clue == best.clue
                            and int(c.num) == int(best.num)
                            and float(c.reward) == float(best.reward)
                            and bool(c.valid) == bool(best.valid)
                        ):
                            best_idx = j
                            break

                cand_payload: List[Dict[str, Any]] = []
                for c in cands:
                    cd: Dict[str, Any] = {
                        "clue": c.clue,
                        "num": int(c.num),
                        "valid": bool(c.valid),
                        "rejection_reason": c.rejection_reason,
                        "directness": float(c.directness),
                        "reward": float(c.reward),
                        "stats": c.stats,
                        "guess_words": c.guess_words,
                    }
                    if INCLUDE_RAW_TEXTS:
                        cd["raw_spymaster_text"] = c.raw_spymaster_text
                        cd["raw_guesser_text"] = c.raw_guesser_text
                    cand_payload.append(cd)

                rec = {
                    "example_id": example_id,
                    "board_id": b["board_id"],
                    "prompt": prompt,
                    "completion": completion,
                    "reward": float(best.reward),
                    "stats": {**best.stats, "directness": float(best.directness)},
                    "clue_meta": {
                        "clue": best.clue,
                        "num": int(best.num),
                        "valid": bool(best.valid),
                        "rejected_candidates": int(meta["rejected_total"]),
                        "rejection_counts": meta["rejection_counts"],
                    },
                    "debug": {"guess_words": best.guess_words},
                    # NEW: variance logging
                    "best_candidate_idx": best_idx,
                    "candidates": cand_payload,
                }

                f_raw.write(json.dumps(rec, ensure_ascii=False) + "\n")
                writes_since_flush += 1
                if writes_since_flush >= FLUSH_EVERY:
                    f_raw.flush()
                    writes_since_flush = 0

                done_ids.add(example_id)

                running_sum += float(best.reward)
                running_n += 1

                if len(done_ids) % progress_every == 0:
                    mean_r = (running_sum / running_n) if running_n else None
                    save_progress(
                        progress_path,
                        done=len(done_ids),
                        total=len(boards),
                        last_example_id=rec["example_id"],
                        last_board_id=rec["board_id"],
                        mean_reward=mean_r,
                        extra={
                            "n_candidates": int(n_candidates),
                            "max_resamples": int(cfg["decoding"]["max_resamples"]),
                            "batch_size": int(batch_size),
                            "shard_id": int(shard_id),
                            "num_shards": int(num_shards),
                            "include_raw_texts": bool(INCLUDE_RAW_TEXTS),
                        },
                    )

        f_raw.flush()

    finally:
        try:
            f_raw.flush()
        except Exception:
            pass
        try:
            f_raw.close()
        except Exception:
            pass

    # Child workers stop here; master will merge + filter.
    if is_child_process() and num_shards > 1:
        print(f"[shard {shard_id}/{num_shards}] done; skipping global filter/merge (master will handle).")
        return

    # Single-process mode: filter and write final file
    raw_for_filter = read_jsonl(raw_path) if raw_path.exists() else []
    filtered = filter_examples(raw_for_filter, cfg)
    write_jsonl(cfg["paths"]["sft_turns_path"], filtered)
    print(f"Filtered {len(filtered)}/{len(raw_for_filter)} -> {cfg['paths']['sft_turns_path']}")

    mean_r = (running_sum / running_n) if running_n else None
    save_progress(
        progress_path,
        done=len(done_ids),
        total=len(boards),
        last_example_id=None,
        last_board_id=None,
        mean_reward=mean_r,
        extra={"status": "finished"},
    )


if __name__ == "__main__":
    main()


================================================
FILE: src/guesser_prompt.py
================================================
from typing import Dict, List, Any

def build_guesser_messages(
    board_words: List[str],
    revealed_mask: List[bool],
    clue: str,
    num: int,
    cfg: Dict[str, Any],
) -> List[Dict[str, str]]:
    system = cfg.get("qwen", {}).get("system_guesser", "You are the Guesser in a Codenames-like game.")
    visible = [w for w, rev in zip(board_words, revealed_mask) if not rev]

    user = f"""You only see the board words and the clue.
 
 BOARD WORDS:
 {", ".join(visible)}

CLUE: {clue}
NUM: {num}

Return at least {num} guesses if possible.

OUTPUT FORMAT (exactly):
GUESSES: word1, word2, word3
"""
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]



================================================
FILE: src/io_utils.py
================================================
# src/io_utils.py
from __future__ import annotations

from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional

from .utils import read_jsonl


def shard_tag(shard_id: int, num_shards: int) -> str:
    return f"shard{shard_id:02d}-of-{num_shards:02d}"


def shard_path_insert_before_suffix(path: str | Path, shard_id: int, num_shards: int) -> Path:
    """
    per_board.jsonl -> per_board.shard00-of-02.jsonl
    (matches eval.py's existing convention)
    """
    p = Path(path)
    tag = shard_tag(shard_id, num_shards)
    if p.suffix:
        return p.with_name(f"{p.stem}.{tag}{p.suffix}")
    return p.with_name(f"{p.name}.{tag}")


def shard_path_append_to_suffix(path: str | Path, shard_id: int, num_shards: int) -> Path:
    """
    foo.jsonl -> foo.jsonl.shard00-of-02
    (matches generate_sft_data.py's existing convention)
    """
    p = Path(path)
    tag = shard_tag(shard_id, num_shards)
    if p.suffix:
        return p.with_suffix(p.suffix + f".{tag}")
    return p.with_name(f"{p.name}.{tag}")


def merge_jsonl_shards(
    shard_paths: Iterable[str | Path],
    *,
    sort_key: Optional[Callable[[Dict[str, Any]], Any]] = None,
) -> List[Dict[str, Any]]:
    combined: List[Dict[str, Any]] = []
    for sp in shard_paths:
        p = Path(sp)
        if p.exists():
            combined.extend(read_jsonl(p))
    if sort_key is not None:
        combined.sort(key=sort_key)
    return combined


================================================
FILE: src/make_boards.py
================================================
from __future__ import annotations

import argparse
import hashlib
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np

from .utils import load_yaml, set_global_seed, write_jsonl, ensure_dir


def load_vocab(path: str | Path) -> List[str]:
    vocab: List[str] = []
    seen = set()
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            w = line.strip()
            if not w:
                continue
            wl = w.lower()
            if wl in seen:
                continue
            seen.add(wl)
            vocab.append(w)
    return vocab


def sample_board(vocab: List[str], rng: np.random.Generator, board_size: int, allow_duplicates: bool) -> List[str]:
    if allow_duplicates:
        idx = rng.integers(0, len(vocab), size=board_size).tolist()
        return [vocab[i] for i in idx]
    else:
        if len(vocab) < board_size:
            raise ValueError(f"Vocab too small: {len(vocab)} < {board_size}")
        idx = rng.choice(len(vocab), size=board_size, replace=False).tolist()
        return [vocab[i] for i in idx]


def assign_labels(
    rng: np.random.Generator,
    board_size: int,
    n_team: int,
    n_opp: int,
    n_neu: int,
    n_assassin: int,
) -> List[str]:
    labels = (["TEAM"] * n_team) + (["OPP"] * n_opp) + (["NEU"] * n_neu) + (["ASSASSIN"] * n_assassin)
    if len(labels) != board_size:
        raise ValueError("Label counts do not sum to board_size.")
    rng.shuffle(labels)
    return labels


def board_hash(board_words: List[str]) -> str:
    s = "||".join(sorted([w.lower() for w in board_words]))
    return hashlib.sha256(s.encode("utf-8")).hexdigest()[:16]


def make_split(
    split_name: str,
    n_boards: int,
    vocab: List[str],
    cfg: Dict[str, Any],
    seed: int,
    global_seen_hashes: set[str],
) -> List[Dict[str, Any]]:
    bcfg = cfg["boards"]
    rng = np.random.default_rng(seed)
    records: List[Dict[str, Any]] = []
    local_seen = set()

    for i in range(n_boards):
        # try until unique (if requested)
        while True:
            board_seed = int(rng.integers(0, 2**31 - 1))
            brng = np.random.default_rng(board_seed)
            words = sample_board(vocab, brng, int(bcfg["board_size"]), bool(bcfg["allow_duplicates"]))
            h = board_hash(words)

            if bcfg.get("avoid_repeat_boards_within_split", True) and h in local_seen:
                continue
            if bcfg.get("avoid_repeat_boards_across_splits", True) and h in global_seen_hashes:
                continue

            local_seen.add(h)
            global_seen_hashes.add(h)
            labels = assign_labels(
                brng,
                int(bcfg["board_size"]),
                int(bcfg["n_team"]),
                int(bcfg["n_opp"]),
                int(bcfg["n_neu"]),
                int(bcfg["n_assassin"]),
            )
            rec = {
                "board_id": f"{split_name}_{i+1:06d}",
                "board_words": words,
                "labels": labels,
                "seed": board_seed,
                "board_hash": h,
            }
            records.append(rec)
            break

    return records


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()

    cfg = load_yaml(args.config)
    vocab = load_vocab(cfg["paths"]["vocab_path"])

    set_global_seed(0)

    seen_hashes: set[str] = set()
    train = make_split(
        "train",
        int(cfg["boards"]["n_train_boards"]),
        vocab,
        cfg,
        int(cfg["boards"]["seed_train"]),
        seen_hashes,
    )
    eval_ = make_split(
        "eval",
        int(cfg["boards"]["n_eval_boards"]),
        vocab,
        cfg,
        int(cfg["boards"]["seed_eval"]),
        seen_hashes,
    )

    write_jsonl(cfg["paths"]["boards_train_path"], train)
    write_jsonl(cfg["paths"]["boards_eval_path"], eval_)

    print(f"Wrote {len(train)} train boards -> {cfg['paths']['boards_train_path']}")
    print(f"Wrote {len(eval_)} eval boards  -> {cfg['paths']['boards_eval_path']}")


if __name__ == "__main__":
    main()


================================================
FILE: src/metrics.py
================================================
from __future__ import annotations

from typing import Any, Dict, List, Tuple
import math
import numpy as np


def compute_clue_diversity(clues: List[str]) -> Dict[str, Any]:
    clues = [c.strip().lower() for c in clues if c and c.strip()]
    n = len(clues)
    if n == 0:
        return {"n": 0, "distinct": 0, "distinct_rate": 0.0, "entropy": 0.0}

    from collections import Counter
    ctr = Counter(clues)
    distinct = len(ctr)
    probs = np.array([v / n for v in ctr.values()], dtype=np.float64)
    entropy = float(-(probs * np.log(probs + 1e-12)).sum())
    return {"n": n, "distinct": distinct, "distinct_rate": float(distinct / n), "entropy": entropy}


def bootstrap_ci(values: List[float], n: int = 1000, alpha: float = 0.05, seed: int = 0) -> Tuple[float, float]:
    rng = np.random.default_rng(seed)
    arr = np.array(values, dtype=np.float64)
    if len(arr) == 0:
        return (float("nan"), float("nan"))
    means = []
    for _ in range(n):
        samp = rng.choice(arr, size=len(arr), replace=True)
        means.append(float(np.mean(samp)))
    lo = float(np.quantile(means, alpha / 2))
    hi = float(np.quantile(means, 1 - alpha / 2))
    return lo, hi


def aggregate(per_board_records: List[Dict[str, Any]]) -> Dict[str, Any]:
    rewards = [float(r["reward"]) for r in per_board_records]
    n_team = [int(r["stats"].get("n_team", 0)) for r in per_board_records]
    n_opp = [int(r["stats"].get("n_opp", 0)) for r in per_board_records]
    n_neu = [int(r["stats"].get("n_neu", 0)) for r in per_board_records]
    assassin = [int(r["stats"].get("assassin", 0)) for r in per_board_records]
    directness = [float(r["stats"].get("directness", 0.0)) for r in per_board_records]
    clues = [r.get("clue", "") for r in per_board_records]

    assassin_rate = float(np.mean([1.0 if a > 0 else 0.0 for a in assassin])) if assassin else 0.0

    out: Dict[str, Any] = {
        "n_boards": len(per_board_records),
        "reward_mean": float(np.mean(rewards)) if rewards else 0.0,
        "reward_median": float(np.median(rewards)) if rewards else 0.0,
        "reward_ci95": bootstrap_ci(rewards, n=500, seed=1) if rewards else (0.0, 0.0),
        "team_mean": float(np.mean(n_team)) if n_team else 0.0,
        "opp_mean": float(np.mean(n_opp)) if n_opp else 0.0,
        "neu_mean": float(np.mean(n_neu)) if n_neu else 0.0,
        "assassin_rate": assassin_rate,
        "directness_mean": float(np.mean(directness)) if directness else 0.0,
        "directness_p90": float(np.quantile(directness, 0.9)) if directness else 0.0,
        "clue_diversity": compute_clue_diversity(clues),
    }
    return out


================================================
FILE: src/model_wrappers.py
================================================
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Protocol, runtime_checkable, Union

import numpy as np

@dataclass
class GenerationConfig:
    temperature: float
    top_p: float
    max_new_tokens: int
    top_k: int | None = None

@runtime_checkable
class TextGenerator(Protocol):
    model_id: str

    def format_chat(
        self,
        messages: List[Dict[str, str]],
        *,
        add_generation_prompt: bool = True,
        enable_thinking: bool = True,
    ) -> str: ...

    def generate(
        self,
        prompt_or_messages: str | List[Dict[str, str]],
        gen_cfg: GenerationConfig,
        seed: Optional[int] = None,
        *,
        use_chat_template: bool = False,
        enable_thinking: bool = True,
    ) -> str: ...

class VLLMTextGenerator:
    """
    Thin wrapper around vLLM offline inference.
    """
    def __init__(
        self,
        model_id: str,
        *,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.90,
        max_model_len: int | None = None,
        dtype: str = "auto",
        trust_remote_code: bool = True,
        enable_lora: bool = False,
        max_lora_rank: int = 64,
    ):
        from transformers import AutoTokenizer
        from vllm import LLM

        self.model_id = model_id
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            use_fast=True,
            trust_remote_code=trust_remote_code,
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # vLLM engine (offline)
        self.llm = LLM(
            model=model_id,
            tokenizer=model_id,
            tensor_parallel_size=int(tensor_parallel_size),
            gpu_memory_utilization=float(gpu_memory_utilization),
            max_model_len=max_model_len,
            dtype=dtype,
            trust_remote_code=trust_remote_code,
            enable_lora=bool(enable_lora),
            max_lora_rank=int(max_lora_rank),
        )

        self._lora_request = None  # set by load_lora_on_generator()

    def format_chat(
        self,
        messages: List[Dict[str, str]],
        *,
        add_generation_prompt: bool = True,
        enable_thinking: bool = True,
    ) -> str:
        try:
            return self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=add_generation_prompt,
                enable_thinking=enable_thinking,
            )
        except TypeError:
            return self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=add_generation_prompt,
            )

    def generate(
        self,
        prompt_or_messages: str | List[Dict[str, str]],
        gen_cfg: GenerationConfig,
        seed: Optional[int] = None,
        *,
        use_chat_template: bool = False,
        enable_thinking: bool = True,
    ) -> str:
        from vllm import SamplingParams

        if use_chat_template:
            assert isinstance(prompt_or_messages, list)
            prompt = self.format_chat(
                prompt_or_messages,
                add_generation_prompt=True,
                enable_thinking=enable_thinking,
            )
        else:
            assert isinstance(prompt_or_messages, str)
            prompt = prompt_or_messages

        # vLLM: max_tokens == your max_new_tokens
        sp = SamplingParams(
            temperature=float(gen_cfg.temperature),
            top_p=float(gen_cfg.top_p),
            top_k=int(gen_cfg.top_k) if gen_cfg.top_k is not None else -1,
            max_tokens=int(gen_cfg.max_new_tokens),
            seed=seed,
        )

        outs = self.llm.generate([prompt], sp, lora_request=self._lora_request)
        # vLLM returns RequestOutput objects; generated text is output.outputs[0].text
        return outs[0].outputs[0].text

    def generate_batch(
        self,
        prompts: List[str],
        gen_cfg: GenerationConfig,
        seeds: Optional[List[Optional[int]]] = None,
    ) -> List[str]:
        from vllm import SamplingParams

        if seeds is None:
            seeds = [None] * len(prompts)
        assert len(seeds) == len(prompts)

        sps = [
            SamplingParams(
                temperature=float(gen_cfg.temperature),
                top_p=float(gen_cfg.top_p),
                top_k=int(gen_cfg.top_k) if gen_cfg.top_k is not None else -1,
                max_tokens=int(gen_cfg.max_new_tokens),
                seed=sd,
            )
            for sd in seeds
        ]

        outs = self.llm.generate(prompts, sps, lora_request=self._lora_request)
        return [o.outputs[0].text for o in outs]

class HFTextGenerator:
    """
    Thin wrapper around transformers generation for reproducible calls.
    """
    def __init__(self, model_id: str, device_map: str = "auto", torch_dtype: str | None = None):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        self.model_id = model_id

        dtype = None
        if torch_dtype:
            dtype = getattr(torch, torch_dtype)

        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map=device_map,
            torch_dtype=dtype,
        )
        self.model.eval()

    def format_chat(
        self,
        messages: List[Dict[str, str]],
        *,
        add_generation_prompt: bool = True,
        enable_thinking: bool = True,
    ) -> str:
        # Qwen3 supports enable_thinking in apply_chat_template; fall back if tokenizer doesn't accept it.
        try:
            return self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=add_generation_prompt,
                enable_thinking=enable_thinking,
            )
        except TypeError:
            return self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=add_generation_prompt,
            )

    def generate(
        self,
        prompt_or_messages: str | List[Dict[str, str]],
        gen_cfg: GenerationConfig,
        seed: Optional[int] = None,
        *,
        use_chat_template: bool = False,
        enable_thinking: bool = True,
    ) -> str:
        import torch

        if seed is not None:
            torch.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)

        if use_chat_template:
            assert isinstance(prompt_or_messages, list)
            text = self.format_chat(prompt_or_messages, add_generation_prompt=True, enable_thinking=enable_thinking)
        else:
            assert isinstance(prompt_or_messages, str)
            text = prompt_or_messages

        inputs = self.tokenizer(text, return_tensors="pt", padding=False)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        do_sample = gen_cfg.temperature is not None and gen_cfg.temperature > 1e-6
        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                do_sample=do_sample,
                temperature=float(gen_cfg.temperature) if do_sample else None,
                top_p=float(gen_cfg.top_p) if do_sample else None,
                top_k=int(gen_cfg.top_k) if (do_sample and gen_cfg.top_k is not None) else None,
                max_new_tokens=int(gen_cfg.max_new_tokens),
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        prompt_len = inputs["input_ids"].shape[1]
        gen_ids = out[0][prompt_len:]
        return self.tokenizer.decode(gen_ids, skip_special_tokens=True)


def load_lora_on_generator(base_generator: TextGenerator, adapter_dir: str) -> TextGenerator:
    """
    HF path: wraps with PeftModel
    vLLM path: sets a LoRARequest that will be used on each generate() call
    """
    if isinstance(base_generator, HFTextGenerator):
        from peft import PeftModel
        base_generator.model = PeftModel.from_pretrained(base_generator.model, adapter_dir)
        base_generator.model.eval()
        return base_generator

    if isinstance(base_generator, VLLMTextGenerator):
        from vllm.lora.request import LoRARequest
        # stable-ish ID (vLLM requires a globally unique int per adapter)
        lora_int_id = abs(hash(adapter_dir)) % (2**31)
        base_generator._lora_request = LoRARequest("codenames-lora", int(lora_int_id), adapter_dir)
        return base_generator

    raise TypeError(f"Unsupported generator type: {type(base_generator)}")


def make_text_generator(model_id: str, cfg: Dict[str, Any]) -> TextGenerator:
    backend = cfg.get("inference", {}).get("backend", "hf")
    if backend == "vllm":
        vcfg = cfg.get("inference", {}).get("vllm", {})
        return VLLMTextGenerator(model_id, **vcfg)
    return HFTextGenerator(model_id, device_map="auto")


class Embedder:
    """
    Embeds single tokens/words/short strings with caching.
    Uses sentence-transformers if available; otherwise uses a HF encoder with mean pooling.
    """
    def __init__(self, model_id: str, device: str = "cpu"):
        self.model_id = model_id
        self.device = device
        self._cache: Dict[str, np.ndarray] = {}

        self._mode = None
        self._st_model = None
        self._hf_tok = None
        self._hf_model = None

        # Prefer sentence-transformers if installed and model_id looks like one
        try:
            from sentence_transformers import SentenceTransformer  # type: ignore
            self._st_model = SentenceTransformer(model_id, device=device)
            self._mode = "st"
        except Exception:
            self._mode = "hf"
            from transformers import AutoModel, AutoTokenizer
            import torch
            self._hf_tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
            self._hf_model = AutoModel.from_pretrained(model_id)
            self._hf_model.to(device)
            self._hf_model.eval()

    def embed(self, text: str) -> np.ndarray:
        key = text.strip().lower()
        if key in self._cache:
            return self._cache[key]

        if self._mode == "st":
            vec = np.asarray(self._st_model.encode([text], normalize_embeddings=True)[0], dtype=np.float32)
        else:
            # HF encoder mean pooling
            import torch
            assert self._hf_tok is not None and self._hf_model is not None
            with torch.no_grad():
                toks = self._hf_tok([text], return_tensors="pt", padding=True, truncation=True).to(self.device)
                out = self._hf_model(**toks)
                last = out.last_hidden_state  # (B,T,H)
                mask = toks["attention_mask"].unsqueeze(-1)  # (B,T,1)
                pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)
                vec = pooled[0].detach().cpu().numpy().astype(np.float32)
                # normalize
                n = np.linalg.norm(vec) + 1e-12
                vec = vec / n

        self._cache[key] = vec
        return vec

    @staticmethod
    def cosine(a: np.ndarray, b: np.ndarray) -> float:
        return float(np.dot(a, b) / ((np.linalg.norm(a) + 1e-12) * (np.linalg.norm(b) + 1e-12)))


================================================
FILE: src/mp_utils.py
================================================
# src/mp_utils.py
from __future__ import annotations
import os, sys, subprocess
from dataclasses import dataclass
from typing import List, Tuple, Optional

ENV_CHILD = "CODENAMES_CHILD"
ENV_SHARD = "CODENAMES_SHARD_ID"
ENV_NSHARDS = "CODENAMES_NUM_SHARDS"

def parse_visible_gpus() -> List[str]:
    cvd = os.environ.get("CUDA_VISIBLE_DEVICES", "").strip()
    if cvd:
        return [x.strip() for x in cvd.split(",") if x.strip()]
    try:
        import torch
        n = torch.cuda.device_count()
    except Exception:
        n = 0
    return [str(i) for i in range(n)]

def is_child_process() -> bool:
    return os.environ.get(ENV_CHILD, "") == "1"

def child_shard_info() -> Tuple[int, int]:
    sid = int(os.environ.get(ENV_SHARD, "0"))
    n = int(os.environ.get(ENV_NSHARDS, "1"))
    return sid, n

def launch_children(module: str, argv: List[str], num_procs: int) -> None:
    gpus = parse_visible_gpus()
    if len(gpus) < num_procs:
        raise RuntimeError(f"Need {num_procs} visible GPUs, but only see {len(gpus)}: {gpus}")

    procs: List[subprocess.Popen] = []
    for sid in range(num_procs):
        env = os.environ.copy()
        env[ENV_CHILD] = "1"
        env[ENV_SHARD] = str(sid)
        env[ENV_NSHARDS] = str(num_procs)
        env["CUDA_VISIBLE_DEVICES"] = gpus[sid]
        cmd = [sys.executable, "-m", module, *argv]
        procs.append(subprocess.Popen(cmd, env=env))

    rc = 0
    for p in procs:
        rc = rc or p.wait()
    if rc != 0:
        raise SystemExit(rc)


================================================
FILE: src/prompting.py
================================================
# src/prompting.py
from typing import Any, Dict, List

def render_prompt(generator, messages: List[Dict[str, str]], cfg: Dict[str, Any], *, role: str) -> str:
    use_chat = bool(cfg.get("qwen", {}).get("use_chat_template", False))
    if not use_chat:
        return messages[-1]["content"]

    think_key = "enable_thinking_spymaster" if role == "spymaster" else "enable_thinking_guesser"
    enable_thinking = bool(cfg.get("qwen", {}).get(think_key, True))
    return generator.format_chat(messages, add_generation_prompt=True, enable_thinking=enable_thinking)


================================================
FILE: src/rollout.py
================================================
# src/rollout.py
from __future__ import annotations

import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, overload, Literal

from .gen_cfg import guesser_gen_cfg, spymaster_gen_cfg
from .model_wrappers import Embedder, TextGenerator
from .prompting import render_prompt
from .rules import is_valid_clue, normalize_word, score_turn
from .spymaster_prompt import build_spymaster_messages
from .guesser_prompt import build_guesser_messages

# -------------------------
# Parsing helpers
# -------------------------

_CLUE_RE = re.compile(r"CLUE\s*:\s*(.+)", re.IGNORECASE)
_NUM_RE = re.compile(r"NUM\s*:\s*([0-9]+)", re.IGNORECASE)
_GUESSES_RE = re.compile(r"GUESSES\s*:\s*(.+)", re.IGNORECASE)


def parse_spymaster_output(text: str) -> Tuple[Optional[str], Optional[int]]:
    """
    Extract the last CLUE/NUM pair from a completion.
    """
    clues = _CLUE_RE.findall(text)
    nums = _NUM_RE.findall(text)
    clue = clues[-1].strip().splitlines()[0].strip() if clues else None
    num = int(nums[-1]) if nums else None
    return clue, num


def parse_guesser_output(text: str) -> List[str]:
    """
    Extract guesses from "GUESSES: a, b, c" (last match wins),
    otherwise fall back to first non-empty line.
    """
    ms = _GUESSES_RE.findall(text)
    raw = ms[-1] if ms else ""
    if not raw:
        raw = text.strip().splitlines()[0] if text.strip() else ""

    parts = [p.strip() for p in raw.replace("\n", " ").split(",")]
    return [p for p in parts if p]


def map_guesses_to_board(guesses: List[str], board_words: List[str]) -> List[str]:
    """
    Normalize and keep only guesses that match board words exactly after normalization.
    Ensures uniqueness while preserving order.

    Returns normalized board-word strings (consistent with score_turn / labels_by_word keys).
    """
    board_norm = {normalize_word(w) for w in board_words}
    out: List[str] = []
    seen = set()
    for g in guesses:
        gn = normalize_word(g)
        if gn in board_norm and gn not in seen:
            out.append(gn)
            seen.add(gn)
    return out


# -------------------------
# Core data structure
# -------------------------

@dataclass
class CandidateResult:
    clue: str
    num: int
    valid: bool
    rejection_reason: str
    directness: float
    reward: float
    stats: Dict[str, Any]
    guess_words: List[str]
    raw_spymaster_text: str
    raw_guesser_text: str


# -------------------------
# Single-candidate rollout
# -------------------------

def run_one_candidate(
    board_record: Dict[str, Any],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    seed: Optional[int] = None,
) -> CandidateResult:
    board_words = board_record["board_words"]
    labels = board_record["labels"]
    revealed_mask = [False] * len(board_words)

    target_words = [w for w, lab in zip(board_words, labels) if lab == "TEAM"]
    labels_by_word = {normalize_word(w): lab for w, lab in zip(board_words, labels)}

    # --- spymaster
    sp_msgs = build_spymaster_messages(board_words, labels, revealed_mask, cfg)
    sp_gen = spymaster_gen_cfg(cfg)

    use_chat = bool(cfg.get("qwen", {}).get("use_chat_template", False))
    sp_think = bool(cfg.get("qwen", {}).get("enable_thinking_spymaster", True))

    sp_text = spymaster.generate(
        sp_msgs if use_chat else sp_msgs[-1]["content"],
        sp_gen,
        seed=seed,
        use_chat_template=use_chat,
        enable_thinking=sp_think,
    )
    clue, num = parse_spymaster_output(sp_text)

    if clue is None or num is None:
        return CandidateResult(
            clue=clue or "",
            num=num or 0,
            valid=False,
            rejection_reason="parse_fail",
            directness=0.0,
            reward=0.0,
            stats={"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0},
            guess_words=[],
            raw_spymaster_text=sp_text,
            raw_guesser_text="",
        )

    valid, reason, d = is_valid_clue(clue, board_words, target_words, embedder, cfg)
    if not valid:
        return CandidateResult(
            clue=clue,
            num=int(num),
            valid=False,
            rejection_reason=reason,
            directness=float(d),
            reward=0.0,
            stats={"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0},
            guess_words=[],
            raw_spymaster_text=sp_text,
            raw_guesser_text="",
        )

    # --- guesser
    g_msgs = build_guesser_messages(board_words, revealed_mask, clue, int(num), cfg)
    g_gen = guesser_gen_cfg(cfg)
    g_think = bool(cfg.get("qwen", {}).get("enable_thinking_guesser", True))

    g_text = guesser.generate(
        g_msgs if use_chat else g_msgs[-1]["content"],
        g_gen,
        seed=seed,
        use_chat_template=use_chat,
        enable_thinking=g_think,
    )

    guesses_raw = parse_guesser_output(g_text)
    guesses = map_guesses_to_board(guesses_raw, board_words)

    k = max(0, min(int(num), len(guesses)))
    scored = guesses[:k]
    reward, stats = score_turn(scored, labels_by_word, cfg)

    return CandidateResult(
        clue=clue,
        num=int(num),
        valid=True,
        rejection_reason="ok",
        directness=float(d),
        reward=float(reward),
        stats=stats,
        guess_words=scored,
        raw_spymaster_text=sp_text,
        raw_guesser_text=g_text,
    )


def run_turn(
    board_record: Dict[str, Any],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    n_candidates: int = 1,
    seed: Optional[int] = None,
) -> Tuple[List[CandidateResult], Dict[str, Any]]:
    """
    Samples up to n_candidates, with validity-enforced resampling.
    Returns candidates and metadata including rejected counts.
    """
    max_resamples = int(cfg["decoding"]["max_resamples"])
    candidates: List[CandidateResult] = []
    rejection_counts: Dict[str, int] = {}
    total_rejected = 0

    for ci in range(n_candidates):
        got: Optional[CandidateResult] = None
        last_res: Optional[CandidateResult] = None

        for ri in range(max_resamples):
            s = None if seed is None else (seed + ci * 1000 + ri)
            res = run_one_candidate(board_record, spymaster, guesser, embedder, cfg, seed=s)
            last_res = res

            if res.valid:
                got = res
                break

            total_rejected += 1
            rejection_counts[res.rejection_reason] = rejection_counts.get(res.rejection_reason, 0) + 1

        if got is None:
            # record the last invalid result if we never got a valid one
            got = last_res if last_res is not None else CandidateResult(
                clue="",
                num=0,
                valid=False,
                rejection_reason="exhausted",
                directness=0.0,
                reward=0.0,
                stats={"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0},
                guess_words=[],
                raw_spymaster_text="",
                raw_guesser_text="",
            )
        candidates.append(got)

    meta = {"rejected_total": total_rejected, "rejection_counts": rejection_counts}
    return candidates, meta


def select_best_candidate(candidates: List[CandidateResult]) -> CandidateResult:
    """
    Primary: reward
    Secondary: lower directness (more indirect) if tie
    """
    best = candidates[0]
    for c in candidates[1:]:
        if c.reward > best.reward:
            best = c
        elif c.reward == best.reward and c.directness < best.directness:
            best = c
    return best


# -------------------------
# Batched rollout (shared by eval + SFT generation)
# -------------------------

@overload
def run_turns_batched(
    boards_batch: List[Dict[str, Any]],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    *,
    n_candidates: int,
    return_candidates: Literal[False] = False,
) -> Tuple[List[CandidateResult], List[Dict[str, Any]]]: ...

@overload
def run_turns_batched(
    boards_batch: List[Dict[str, Any]],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    *,
    n_candidates: int,
    return_candidates: Literal[True],
) -> Tuple[List[CandidateResult], List[Dict[str, Any]], List[List[CandidateResult]]]: ...


def run_turns_batched(
    boards_batch: List[Dict[str, Any]],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    *,
    n_candidates: int,
    return_candidates: bool = False,
) -> Any:
    """
    Returns (default):
      - best CandidateResult per board
      - meta per board: rejected_total + rejection_counts (matching run_turn)

    If return_candidates=True:
      - also returns all candidates per board (length == n_candidates)
        so you can analyze variance across candidates.
    """
    # Sequential fallback if no batch API
    if not (hasattr(spymaster, "generate_batch") and hasattr(guesser, "generate_batch")):
        bests: List[CandidateResult] = []
        metas: List[Dict[str, Any]] = []
        all_cands: List[List[CandidateResult]] = []
        for b in boards_batch:
            seed = int(b.get("seed", 0))
            cands, meta = run_turn(b, spymaster, guesser, embedder, cfg, n_candidates=n_candidates, seed=seed)
            all_cands.append(cands)
            bests.append(select_best_candidate(cands))
            metas.append(meta)
        if return_candidates:
            return bests, metas, all_cands
        return bests, metas

    sp_gen = spymaster_gen_cfg(cfg)
    g_gen = guesser_gen_cfg(cfg)
    max_resamples = int(cfg["decoding"]["max_resamples"])

    # Precompute prompts + bookkeeping per board
    sp_prompts: List[str] = []
    board_words_list: List[List[str]] = []
    target_words_list: List[List[str]] = []
    labels_by_word_list: List[Dict[str, str]] = []
    seeds0: List[int] = []

    for b in boards_batch:
        board_words = b["board_words"]
        labels = b["labels"]
        revealed_mask = [False] * len(board_words)

        sp_msgs = build_spymaster_messages(board_words, labels, revealed_mask, cfg)
        sp_prompts.append(render_prompt(spymaster, sp_msgs, cfg, role="spymaster"))

        target_words = [w for w, lab in zip(board_words, labels) if lab == "TEAM"]
        labels_by_word = {normalize_word(w): lab for w, lab in zip(board_words, labels)}

        board_words_list.append(board_words)
        target_words_list.append(target_words)
        labels_by_word_list.append(labels_by_word)
        seeds0.append(int(b.get("seed", 0)))

    # Storage
    all_candidates_per_board: List[List[CandidateResult]] = [[] for _ in boards_batch]
    rej_counts_per_board: List[Dict[str, int]] = [{} for _ in boards_batch]
    rej_total_per_board: List[int] = [0 for _ in boards_batch]

    for ci in range(n_candidates):
        # tuple: (clue, num, is_valid, reason, directness, raw_spymaster_text, used_seed)
        got: List[Optional[Tuple[str, int, bool, str, float, str, int]]] = [None] * len(boards_batch)

        pending = list(range(len(boards_batch)))
        last_text: Dict[int, str] = {}
        last_seed: Dict[int, int] = {}

        # Batched spymaster resampling
        for ri in range(max_resamples):
            if not pending:
                break

            prompts = [sp_prompts[i] for i in pending]
            seeds = [seeds0[i] + ci * 1000 + ri for i in pending]
            texts = spymaster.generate_batch(prompts, sp_gen, seeds)  # type: ignore[attr-defined]

            for idx_in_list, bi in enumerate(pending):
                txt = texts[idx_in_list]
                s_used = seeds[idx_in_list]
                last_text[bi] = txt
                last_seed[bi] = s_used

                clue, num = parse_spymaster_output(txt)
                if clue is None or num is None:
                    reason = "parse_fail"
                    rej_total_per_board[bi] += 1
                    rej_counts_per_board[bi][reason] = rej_counts_per_board[bi].get(reason, 0) + 1
                    continue

                valid, reason, d = is_valid_clue(clue, board_words_list[bi], target_words_list[bi], embedder, cfg)
                if not valid:
                    rej_total_per_board[bi] += 1
                    rej_counts_per_board[bi][reason] = rej_counts_per_board[bi].get(reason, 0) + 1
                    continue

                got[bi] = (clue, int(num), True, "ok", float(d), txt, s_used)

            pending = [bi for bi in pending if got[bi] is None]

        # Fill any missing with last attempt (invalid)
        for bi in range(len(boards_batch)):
            if got[bi] is None:
                txt = last_text.get(bi, "")
                s_used = last_seed.get(bi, seeds0[bi] + ci * 1000)
                clue, num = parse_spymaster_output(txt)
                got[bi] = (
                    clue or "",
                    int(num) if num is not None else 0,
                    False,
                    "exhausted",
                    0.0,
                    txt,
                    s_used,
                )

        # Batched guesser for valid ones only
        valid_bis = [bi for bi in range(len(boards_batch)) if got[bi][2]]  # type: ignore[index]
        g_prompts: List[str] = []
        g_seeds: List[int] = []

        for bi in valid_bis:
            clue, num, _, _, _, _, s_used = got[bi]  # type: ignore[misc]
            revealed_mask = [False] * len(board_words_list[bi])
            g_msgs = build_guesser_messages(board_words_list[bi], revealed_mask, clue, int(num), cfg)
            g_prompts.append(render_prompt(guesser, g_msgs, cfg, role="guesser"))
            g_seeds.append(int(s_used))

        g_texts = guesser.generate_batch(g_prompts, g_gen, g_seeds) if g_prompts else []  # type: ignore[attr-defined]

        gi = 0
        for bi in range(len(boards_batch)):
            clue, num, is_valid, reason, d, sp_txt, _ = got[bi]  # type: ignore[misc]

            if not is_valid:
                all_candidates_per_board[bi].append(
                    CandidateResult(
                        clue=clue,
                        num=int(num),
                        valid=False,
                        rejection_reason=reason,
                        directness=float(d),
                        reward=0.0,
                        stats={"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0},
                        guess_words=[],
                        raw_spymaster_text=sp_txt,
                        raw_guesser_text="",
                    )
                )
                continue

            g_txt = g_texts[gi]
            gi += 1

            guesses_raw = parse_guesser_output(g_txt)
            guesses = map_guesses_to_board(guesses_raw, board_words_list[bi])

            k = max(0, min(int(num), len(guesses)))
            scored = guesses[:k]
            reward, stats = score_turn(scored, labels_by_word_list[bi], cfg)

            all_candidates_per_board[bi].append(
                CandidateResult(
                    clue=clue,
                    num=int(num),
                    valid=True,
                    rejection_reason="ok",
                    directness=float(d),
                    reward=float(reward),
                    stats=stats,
                    guess_words=scored,
                    raw_spymaster_text=sp_txt,
                    raw_guesser_text=g_txt,
                )
            )

    bests = [select_best_candidate(cands) for cands in all_candidates_per_board]
    metas = [
        {"rejected_total": rej_total_per_board[i], "rejection_counts": rej_counts_per_board[i]}
        for i in range(len(boards_batch))
    ]

    if return_candidates:
        return bests, metas, all_candidates_per_board
    return bests, metas


================================================
FILE: src/rules.py
================================================
from __future__ import annotations

import re
from typing import Dict, List, Tuple, Any, Optional

from .model_wrappers import Embedder


_WORD_RE = re.compile(r"^[A-Za-z][A-Za-z0-9_-]*$")


def normalize_word(s: str) -> str:
    return s.strip().lower()


def is_single_word(s: str) -> bool:
    s = s.strip()
    if not s:
        return False
    # “single word” in the practical sense: no whitespace
    return len(s.split()) == 1


def violates_board_overlap(clue: str, board_words: List[str]) -> bool:
    c = normalize_word(clue)
    bw = {normalize_word(w) for w in board_words}
    return c in bw


def violates_substring_ban(clue: str, board_words: List[str]) -> bool:
    c = normalize_word(clue)
    for w in board_words:
        ww = normalize_word(w)
        if not c or not ww:
            continue
        if c in ww or ww in c:
            return True
    return False


def looks_like_word_token(clue: str) -> bool:
    # avoid punctuation / multi-token-ish outputs
    c = clue.strip()
    return bool(_WORD_RE.match(c))


def directness_score(clue: str, target_words: List[str], embedder: Embedder) -> float:
    if embedder is None:
        return 0.0
    cvec = embedder.embed(clue)
    sims = []
    for t in target_words:
        tvec = embedder.embed(t)
        sims.append(Embedder.cosine(cvec, tvec))
    return float(max(sims) if sims else 0.0)


def is_valid_clue(
    clue: str,
    board_words: List[str],
    target_words: List[str],
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
) -> Tuple[bool, str, float]:
    cons = cfg["constraints"]
    clue = clue.strip()

    if cons.get("single_word_only", True) and not is_single_word(clue):
        return False, "not_single_word", 0.0

    if not looks_like_word_token(clue):
        return False, "bad_token_shape", 0.0

    if cons.get("ban_board_words", True) and violates_board_overlap(clue, board_words):
        return False, "board_word", 0.0

    if cons.get("ban_substrings", True) and violates_substring_ban(clue, board_words):
        return False, "substring", 0.0

    # Optional embedding-based directness constraint
    if not bool(cons.get("enable_directness_check", True)):
        return True, "ok", 0.0

    d = directness_score(clue, target_words, embedder)
    if d >= float(cons["tau_direct"]):
        return False, "too_direct", float(d)

    return True, "ok", float(d)


def score_turn(
    guesses: List[str],
    labels_by_word: Dict[str, str],
    cfg: Dict[str, Any],
) -> Tuple[float, Dict[str, Any]]:
    """
    guesses: normalized board words guessed in order (unique)
    labels_by_word: normalized_word -> label
    """
    r = cfg["reward"]
    stats = {"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0}

    reward = 0.0
    for g in guesses:
        lab = labels_by_word.get(g)
        if lab is None:
            continue
        if lab == "TEAM":
            reward += float(r["team_correct"])
            stats["n_team"] += 1
        elif lab == "OPP":
            reward += float(r["opp_wrong"])
            stats["n_opp"] += 1
        elif lab == "NEU":
            reward += float(r["neu_wrong"])
            stats["n_neu"] += 1
        elif lab == "ASSASSIN":
            reward += float(r["assassin_wrong"])
            stats["assassin"] += 1

    # optional shaping (off by default)
    reward += float(r.get("repeat_penalty", 0.0)) * 0.0
    reward += float(r.get("brevity_bonus", 0.0)) * 0.0

    return float(reward), stats


================================================
FILE: src/spymaster_prompt.py
================================================
from __future__ import annotations

from typing import Dict, List, Any


def build_spymaster_messages(
    board_words: List[str],
    labels: List[str],
    revealed_mask: List[bool],
    cfg: Dict[str, Any],
) -> List[Dict[str, str]]:
    system = cfg.get("qwen", {}).get("system_spymaster", "You are the Spymaster in a Codenames-like game.")

    words_lines = []
    for i, w in enumerate(board_words):
        tag = "REVEALED" if revealed_mask[i] else "HIDDEN"
        words_lines.append(f"{i+1:02d}. {w} [{tag}]")

    team_words = [w for w, lab, rev in zip(board_words, labels, revealed_mask) if (lab == "TEAM" and not rev)]
    opp_words = [w for w, lab, rev in zip(board_words, labels, revealed_mask) if (lab == "OPP" and not rev)]
    neu_words = [w for w, lab, rev in zip(board_words, labels, revealed_mask) if (lab == "NEU" and not rev)]
    ass_words = [w for w, lab, rev in zip(board_words, labels, revealed_mask) if (lab == "ASSASSIN" and not rev)]

    tau = cfg["constraints"]["tau_direct"]

    user = f"""
BOARD WORDS (25):
{chr(10).join(words_lines)}

YOUR TARGETS (TEAM) — choose a clue that points to some of these:
{", ".join(team_words)}

DANGEROUS / FORBIDDEN (avoid leading the guesser to these):
OPP: {", ".join(opp_words)}
NEU: {", ".join(neu_words)}
ASSASSIN: {", ".join(ass_words)}

CONSTRAINTS:
- Output exactly ONE clue word (single token-like word).
- The clue must NOT be any board word.
- Avoid substrings of board words and vice versa.

GOAL:
Produce an indirect but helpful clue that helps a frozen Guesser pick TEAM words while avoiding OPP/NEU/ASSASSIN.

OUTPUT FORMAT (exactly):
<think>
...
</think>
CLUE: <one_word>
NUM: <integer>

Now produce your clue.
"""
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]


================================================
FILE: src/train_lora_sft.py
================================================
from __future__ import annotations

import argparse
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List

import torch
from torch.utils.data import Dataset
from transformers.trainer_utils import get_last_checkpoint

from .utils import load_yaml, read_jsonl, ensure_dir, save_config_snapshot, save_run_meta, set_global_seed


class SFTDataset(Dataset):
    """
    Dataset backed by pre-tokenized examples:
      item = {"input_ids": List[int], "labels": List[int]}
    """

    def __init__(self, tokenized: List[Dict[str, Any]]):
        self.items = tokenized

    def __len__(self) -> int:
        return len(self.items)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        ex = self.items[idx]
        input_ids = ex["input_ids"]
        labels = ex["labels"]
        attn = [1] * len(input_ids)

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long),
            "attention_mask": torch.tensor(attn, dtype=torch.long),
        }


@dataclass
class PadCollator:
    tokenizer: Any

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        input_ids = [b["input_ids"] for b in batch]
        labels = [b["labels"] for b in batch]
        attn = [b["attention_mask"] for b in batch]

        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)
        attn = torch.nn.utils.rnn.pad_sequence(attn, batch_first=True, padding_value=0)

        return {"input_ids": input_ids, "labels": labels, "attention_mask": attn}


def _load_or_build_tokcache(
    *,
    cache_path: Path,
    records: List[Dict[str, Any]],
    tokenizer,
    model_id: str,
    max_len: int,
) -> List[Dict[str, Any]]:
    """
    Cache format:
      {"model_id": str, "max_len": int, "tokenized": [{"input_ids": [...], "labels": [...]}, ...]}
    """
    if cache_path.exists():
        obj = torch.load(cache_path, map_location="cpu")
        if (
            isinstance(obj, dict)
            and obj.get("model_id") == model_id
            and int(obj.get("max_len", -1)) == int(max_len)
            and isinstance(obj.get("tokenized"), list)
        ):
            print(f"Loaded tokenization cache -> {cache_path}")
            return obj["tokenized"]

    # Build cache
    tokenized: List[Dict[str, Any]] = []
    eos = tokenizer.eos_token_id

    for r in records:
        p = tokenizer(r["prompt"], add_special_tokens=False)["input_ids"]
        c = tokenizer(r["completion"], add_special_tokens=False)["input_ids"]

        input_ids = p + c + [eos]
        labels = [-100] * len(p) + c + [eos]

        input_ids = input_ids[:max_len]
        labels = labels[:max_len]

        tokenized.append({"input_ids": input_ids, "labels": labels})

    cache_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save({"model_id": model_id, "max_len": int(max_len), "tokenized": tokenized}, cache_path)
    print(f"Saved tokenization cache -> {cache_path}")

    return tokenized


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()

    cfg = load_yaml(args.config)
    tcfg = cfg["training"]
    set_global_seed(int(tcfg.get("seed", 0)))

    # Load data
    records = read_jsonl(cfg["paths"]["sft_turns_path"])
    if not records:
        raise RuntimeError("No SFT records found. Did generate_sft_data produce an empty filtered set?")

    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
    from peft import LoraConfig, get_peft_model

    model_id = cfg["models"]["spymaster_model_id"]
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # dtype
    torch_dtype = None
    if bool(tcfg.get("bf16", False)) and torch.cuda.is_available():
        torch_dtype = torch.bfloat16
    elif bool(tcfg.get("fp16", False)) and torch.cuda.is_available():
        torch_dtype = torch.float16

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch_dtype,
    )

    lora_cfg = LoraConfig(
        r=int(tcfg["r"]),
        lora_alpha=int(tcfg["alpha"]),
        lora_dropout=float(tcfg["dropout"]),
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=tcfg.get("target_modules", None),
    )
    model = get_peft_model(model, lora_cfg)

    out_dir = ensure_dir(tcfg["output_adapter_dir"])

    # -------------------------
    # Pre-tokenize once + cache
    # -------------------------
    max_len = int(tcfg["max_seq_len"])
    # Put cache next to the SFT file so it follows the dataset
    sft_path = Path(cfg["paths"]["sft_turns_path"])
    cache_path = sft_path.with_suffix(sft_path.suffix + f".tokcache.maxlen{max_len}.pt")

    tokenized = _load_or_build_tokcache(
        cache_path=cache_path,
        records=records,
        tokenizer=tokenizer,
        model_id=model_id,
        max_len=max_len,
    )

    ds = SFTDataset(tokenized)
    collator = PadCollator(tokenizer)

    args_tr = TrainingArguments(
        output_dir=str(out_dir),
        per_device_train_batch_size=int(tcfg["batch_size"]),
        gradient_accumulation_steps=int(tcfg["grad_accum"]),
        num_train_epochs=float(tcfg["epochs"]),
        learning_rate=float(tcfg["lr"]),
        logging_steps=20,
        save_steps=200,
        save_total_limit=2,
        report_to=[],
        bf16=bool(tcfg.get("bf16", False)),
        fp16=bool(tcfg.get("fp16", False)),
        optim="adamw_torch",
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        ddp_find_unused_parameters=False,
    )

    trainer = Trainer(
        model=model,
        args=args_tr,
        train_dataset=ds,
        data_collator=collator,
        tokenizer=tokenizer,
    )

    last_ckpt = get_last_checkpoint(str(out_dir))
    trainer.train(resume_from_checkpoint=last_ckpt)

    # Save adapter
    if trainer.is_world_process_zero():
        m = trainer.model
        if hasattr(m, "module"):  # DDP wrap
            m = m.module
        m.save_pretrained(str(out_dir))
        tokenizer.save_pretrained(str(out_dir))

        # Save config + run meta
        save_config_snapshot(cfg, out_dir)
        save_run_meta(out_dir, command=["python", "-m", "src.train_lora_sft", "--config", args.config])

    print(f"Saved LoRA adapter -> {out_dir}")


if __name__ == "__main__":
    main()


================================================
FILE: src/utils.py
================================================
import json
import os
import random
import subprocess
import time
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional

import numpy as np
import yaml

def now_iso() -> str:
    # Nice for logs/progress: 2026-02-20T13:45:12
    return time.strftime("%Y-%m-%dT%H:%M:%S")


def save_progress(
    progress_path: str | Path,
    *,
    done: int,
    total: int,
    last_example_id: Optional[str] = None,
    last_board_id: Optional[str] = None,
    mean_reward: Optional[float] = None,
    extra: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Atomic JSON progress writer (safe for crashes).
    """
    progress_path = Path(progress_path)
    progress_path.parent.mkdir(parents=True, exist_ok=True)

    payload: Dict[str, Any] = {
        "timestamp": now_iso(),
        "done": int(done),
        "total": int(total),
        "pct": float(done / total) if total else 0.0,
        "last_example_id": last_example_id,
        "last_board_id": last_board_id,
        "mean_reward_running": float(mean_reward) if mean_reward is not None else None,
    }
    if extra:
        payload.update(extra)

    tmp = progress_path.with_suffix(progress_path.suffix + ".tmp")
    tmp.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    os.replace(tmp, progress_path)

def load_yaml(path: str | Path) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        obj = yaml.safe_load(f) or {}
    if not isinstance(obj, dict):
        raise ValueError(f"YAML config must be a mapping/dict: {path}")
    return obj


def ensure_dir(path: str | Path) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p


def read_jsonl(path: str | Path) -> List[Dict[str, Any]]:
    records: List[Dict[str, Any]] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            records.append(json.loads(line))
    return records


def write_jsonl(path: str | Path, records: Iterable[Dict[str, Any]]) -> None:
    path = Path(path)
    ensure_dir(path.parent)
    with open(path, "w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def append_jsonl(path: str | Path, obj: Dict[str, Any], *, do_fsync: bool = True) -> None:
    path = Path(path)
    if path.parent and str(path.parent) != "":
        ensure_dir(path.parent)

    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")
        f.flush()
        if do_fsync:
            os.fsync(f.fileno())

def set_global_seed(seed: int) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    try:
        import torch
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    except Exception:
        pass


def now_ts() -> str:
    return time.strftime("%Y%m%d_%H%M%S")


def try_get_git_hash() -> Optional[str]:
    try:
        out = subprocess.check_output(["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL)
        return out.decode("utf-8").strip()
    except Exception:
        return None


def save_config_snapshot(cfg: Dict[str, Any], out_dir: str | Path, filename: str = "config_snapshot.yaml") -> None:
    out_dir = ensure_dir(out_dir)
    p = out_dir / filename
    with open(p, "w", encoding="utf-8") as f:
        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)


def save_run_meta(out_dir: str | Path, command: List[str] | None = None, extra: Dict[str, Any] | None = None) -> None:
    out_dir = ensure_dir(out_dir)
    meta: Dict[str, Any] = {
        "timestamp": now_ts(),
        "git_hash": try_get_git_hash(),
        "command": command,
    }
    if extra:
        meta.update(extra)
    with open(out_dir / "run_meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

