Directory structure:
└── codenames-lora/
    ├── run.slurm
    ├── .gitingestignore
    ├── configs/
    │   ├── default.yml
    │   └── environment.yml
    └── src/
        ├── __init__.py
        ├── build_vocab.py
        ├── chat_formatting.py
        ├── epoch_eval.py
        ├── eval.py
        ├── gen_cfg.py
        ├── generate_sft_data.py
        ├── guesser_prompt.py
        ├── io_utils.py
        ├── make_boards.py
        ├── metrics.py
        ├── model_wrappers.py
        ├── mp_utils.py
        ├── prompting.py
        ├── rollout.py
        ├── rules.py
        ├── spymaster_prompt.py
        ├── think_utils.py
        ├── train_lora_sft.py
        └── utils.py

================================================
FILE: run.slurm
================================================
#!/bin/bash
#SBATCH --job-name=codenames-sftpipe
#SBATCH --gres=gpu:2
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --time=06:00:00
#SBATCH --requeue
#SBATCH --signal=B:USR1@900
#SBATCH --open-mode=append
#SBATCH --output=logs/%x-%j.out
#SBATCH --error=logs/%x-%j.err
#SBATCH --mail-user=henryxie@college.harvard.edu
#SBATCH --mail-type=END,FAIL

set -euo pipefail

mkdir -p logs data outputs

SCRIPT_PATH="$(readlink -f "$0" 2>/dev/null || realpath "$0")"
ORIG_JOB_ID="${ORIG_JOB_ID:-${SLURM_JOB_ID:-unknown}}"

REQUEUE_SENT=0
MAIN_PID=""

handle_usr1() {
  if (( REQUEUE_SENT == 1 )); then return 0; fi
  REQUEUE_SENT=1

  echo "[$(date)] USR1 received: requesting requeue, then stopping workload..."

  set +e
  scontrol requeue "$SLURM_JOB_ID"
  rc=$?
  if (( rc != 0 )); then
    echo "[$(date)] requeue failed (rc=$rc) -> resubmitting with dependency"
    sbatch --dependency=afterany:"$SLURM_JOB_ID" \
          --export=ALL,ORIG_JOB_ID="$ORIG_JOB_ID" \
          --open-mode=append \
          --output="logs/%x-${ORIG_JOB_ID}.out" \
          --error="logs/%x-${ORIG_JOB_ID}.err" \
          "$SCRIPT_PATH"
  fi

  if [[ -n "${MAIN_PID}" ]] && kill -0 "${MAIN_PID}" 2>/dev/null; then
    kill -TERM -- -"${MAIN_PID}" 2>/dev/null || kill -TERM "${MAIN_PID}" 2>/dev/null
    for _ in $(seq 1 120); do
      kill -0 "${MAIN_PID}" 2>/dev/null || break
      sleep 1
    done
    kill -KILL -- -"${MAIN_PID}" 2>/dev/null || true
  fi
  set -e
  exit 0
}
trap handle_usr1 USR1

module load miniforge/25.11.0-0
source /orcd/software/core/001/pkg/miniforge/24.3.0-0/etc/profile.d/conda.sh
conda activate codenames-lora

export VLLM_ATTENTION_BACKEND=TRITON
export VLLM_USE_PRECOMPILED=1
export TOKENIZERS_PARALLELISM=false

# Cache to local scratch (ephemeral per run)
export HF_HOME="${SLURM_TMPDIR:-/tmp}/hf"
export HF_DATASETS_CACHE="$HF_HOME/datasets"
mkdir -p "$HF_HOME" "$HF_DATASETS_CACHE"

CONFIG="configs/default.yml"

# Optional: set CLEAN=1 to wipe prior artifacts and regenerate from scratch
# (otherwise SFT generation will resume using existing raw jsonl)
CLEAN="${CLEAN:-0}"

# set +e
# setsid bash -lc "
#   set -euo pipefail

#   if [[ \"${CLEAN}\" == \"1\" ]]; then
#     echo '[clean] Removing existing data artifacts...'
#     rm -f data/vocab.txt data/vocab_manifest.json
#     rm -f data/boards_train.jsonl data/boards_eval.jsonl
#     rm -f data/sft_turns.jsonl data/sft_turns_raw.jsonl
#     rm -f data/sft_turns_raw.jsonl.shard*-of-* 2>/dev/null || true
#     rm -f data/sft_turns_raw.jsonl.*.progress.json 2>/dev/null || true
#   fi

#   # 1) Build vocab (idempotent-ish: skip if exists unless CLEAN=1)
#   if [[ ! -f data/vocab.txt ]]; then
#     echo '[step 1/3] build_vocab'
#     python -m src.build_vocab --config \"${CONFIG}\"
#   else
#     echo '[step 1/3] build_vocab: data/vocab.txt exists, skipping'
#   fi

#   # 2) Make boards (skip if exists unless CLEAN=1)
#   if [[ ! -f data/boards_train.jsonl || ! -f data/boards_eval.jsonl ]]; then
#     echo '[step 2/3] make_boards'
#     python -m src.make_boards --config \"${CONFIG}\"
#   else
#     echo '[step 2/3] make_boards: boards files exist, skipping'
#   fi

#   # 3) Generate SFT data (GPU; spawns inference.num_processes children itself)
#   echo '[step 3/3] generate_sft_data'
#   python -m src.generate_sft_data --config \"${CONFIG}\"

#   echo '[done] pipeline complete (up to SFT data)'
# " &
# MAIN_PID=$!
# wait "$MAIN_PID"
# RC=$?
# set -e

# exit "$RC"

# Run workload in its own session so we can kill the whole group on USR1 (torchrun-friendly)
set +e
setsid torchrun --standalone --nnodes=1 --nproc_per_node=2 \
  -m src.train_lora_sft --config configs/default.yml &
MAIN_PID=$!
wait "$MAIN_PID"
RC=$?
set -e

exit "$RC"

# set +e

# setsid bash -lc "
#   python -m src.eval --config configs/default.yml --mode baseline --out outputs/eval/baseline &&
#   python -m src.eval --config configs/default.yml --mode sft      --out outputs/eval/sft
# " &
# MAIN_PID=$!

# wait "$MAIN_PID"
# RC=$?
# set -e

# exit "$RC"


================================================
FILE: .gitingestignore
================================================
# /run.slurm
/data/
/outputs/


================================================
FILE: configs/default.yml
================================================
vocab:
  lang: "en"
  stop_n: 300
  min_zipf: 3.0
  max_zipf: 6.0
  min_len: 3
  max_len: 12
  max_words: 30000
  token_regex: "^[a-z]+$"
  extra_banlist: null
  manifest_path: "data/vocab_manifest.json"

inference:
  backend: "vllm"   # "hf" or "vllm"

  num_processes: 2
  batch_size: 12

  vllm:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.90
    max_model_len: 4096
    dtype: "bfloat16"   # "auto" | "float16" | "bfloat16"
    trust_remote_code: true

    enable_lora: true
    max_lora_rank: 64

paths:
  data_dir: "data"
  outputs_dir: "outputs"
  vocab_path: "data/vocab.txt"
  boards_train_path: "data/boards_train.jsonl"
  boards_eval_path: "data/boards_eval.jsonl"
  sft_turns_path: "data/sft_turns.jsonl"
  sft_turns_raw_path: "data/sft_turns_raw.jsonl"

boards:
  sft_max_train_boards: 2000
  n_train_boards: 2000
  n_eval_boards: 400
  board_size: 25
  n_team: 9
  n_opp: 8
  n_neu: 7
  n_assassin: 1
  seed_train: 123
  seed_eval: 456
  allow_duplicates: false
  avoid_repeat_boards_within_split: true
  avoid_repeat_boards_across_splits: true

models:
  spymaster_model_id: "Qwen/Qwen3-8B"
  guesser_model_id: "Qwen/Qwen3-8B"
  embedding_model_id: "sentence-transformers/all-MiniLM-L6-v2"

qwen:
  use_chat_template: true
  enable_thinking_spymaster: true
  enable_thinking_guesser: true
  system_spymaster: "You are the Spymaster in a Codenames-like game."
  system_guesser: "You are the Guesser in a Codenames-like game."

decoding:
  progress_every: 20
  flush_every: 10

  spymaster_temperature: 0.6
  spymaster_top_p: 0.95
  spymaster_top_k: 20
  spymaster_max_new_tokens: 4096

  guesser_temperature: 0.6
  guesser_top_p: 0.95
  guesser_top_k: 20
  guesser_max_new_tokens: 2048

  n_candidates: 6
  max_resamples: 2

constraints:
  single_word_only: true
  ban_board_words: true
  ban_substrings: true
  enable_directness_check: false
  tau_direct: 0.52
  directness_metric: "cosine"

reward:
  team_correct: 1.0
  opp_wrong: -1.0
  neu_wrong: -0.5
  assassin_wrong: -3.0
  repeat_penalty: 0.0
  brevity_bonus: 0.0

filtering:
  mode: "rule_based"
  require_no_assassin: true
  min_reward: 3.0

training:
  seed: 999
  r: 16
  alpha: 32
  dropout: 0.05
  lr: 2.0e-4
  epochs: 10
  batch_size: 2
  grad_accum: 8
  max_seq_len: 2048
  output_adapter_dir: "outputs/sft/lora_spymaster"
  bf16: true
  fp16: false
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

  attn_implementation: "sdpa"
  gradient_checkpointing: true
  gradient_checkpointing_use_reentrant: false

epoch_eval:
  gsm8k_n: 10
  gsm8k_max_new_tokens: 2048

  humaneval_n: 10
  humaneval_max_new_tokens: 2048
  humaneval_timeout_s: 3

  wikitext_n: 20
  wikitext_block_size: 512
  wikitext_max_blocks: 80

  codenames_n_boards: 20

  codenames_spymaster_max_new_tokens: 4096
  codenames_guesser_max_new_tokens: 2048


================================================
FILE: configs/environment.yml
================================================
name: codenames-lora
channels:
  - conda-forge

dependencies:
  - python=3.10
  - pip
  - git
  - git-lfs
  - setuptools
  - wheel
  - matplotlib

  # core utilities
  - numpy>=1.26
  - pyyaml>=6.0
  - tqdm
  - regex
  - scipy
  - pandas
  - psutil

  # keep these on conda to avoid pip/conda duplication
  - sentencepiece
  - protobuf

  - pip:
      # PyTorch CUDA wheels (keep as EXTRA index so PyPI still works)
      - --extra-index-url https://download.pytorch.org/whl/cu128
      - torch==2.7.0
      - torchvision==0.22.0
      - torchaudio==2.7.0

      # vLLM: pin to avoid pip grabbing latest (and trying to build from source)
      - vllm==0.9.2

      # HF stack
      - transformers==4.53.3
      - accelerate>=0.34.0
      - peft>=0.12.0
      - huggingface-hub>=0.25.0
      - safetensors>=0.4.3
      - tokenizers>=0.19.1
      - datasets>=2.20.0

      - kernels

      # embeddings
      - sentence-transformers>=3.0.0

      # vocab builder
      - wordfreq>=3.1.0

      # tokenizer backend
      - tiktoken>=0.7.0


================================================
FILE: src/__init__.py
================================================
[Empty file]


================================================
FILE: src/build_vocab.py
================================================
#!/usr/bin/env python3
"""
Build a clean Codenames-style vocabulary file using wordfreq.

Approach (hybrid):
- remove ultra-common stopwords: top_n_list('en', STOP_N)
- keep only words within a Zipf frequency window: MIN_ZIPF <= zipf <= MAX_ZIPF
- keep only lowercase alphabetic tokens (default: ^[a-z]+$)
- filter by length
- apply a small curated "ban list" for months/days/numbers/meta-words
- write data/vocab.txt (one word per line)
- optionally write a manifest with stats

Usage:
  pip install wordfreq
  python -m src.build_vocab --out data/vocab.txt --manifest data/vocab_manifest.json

Notes:
- Output words are lowercased.
- Ordering is deterministic: sorted by (-zipf, word).
"""

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import Iterable, List, Set, Tuple

from .utils import load_yaml

from wordfreq import iter_wordlist, top_n_list, zipf_frequency


DEFAULT_BAN = {
    # days
    "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday",
    # months
    "january", "february", "march", "april", "may", "june", "july", "august",
    "september", "october", "november", "december",
    # numbers (word forms)
    "zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",
    "first", "second", "third",
    # ultra-generic/meta (often unfun boards)
    "thing", "things", "stuff", "object", "objects", "person", "people", "someone", "somebody",
    "anything", "everything", "nothing", "something",
    # directions (optional; remove if you like them)
    "north", "south", "east", "west", "left", "right",
}


def load_extra_banlist(path: str | None) -> Set[str]:
    if not path:
        return set()
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Banlist file not found: {path}")
    banned = set()
    for line in p.read_text(encoding="utf-8").splitlines():
        w = line.strip().lower()
        if not w or w.startswith("#"):
            continue
        banned.add(w)
    return banned


def build_vocab(
    *,
    lang: str,
    stop_n: int,
    min_zipf: float,
    max_zipf: float,
    min_len: int,
    max_len: int,
    token_regex: str,
    extra_ban: Set[str],
    max_words: int | None = None,
) -> Tuple[List[str], dict]:
    token_re = re.compile(token_regex)

    stop = set(top_n_list(lang, stop_n))
    banned = set(DEFAULT_BAN) | set(extra_ban)

    kept: List[Tuple[str, float]] = []
    seen = set()

    n_total = 0
    n_stop = 0
    n_badshape = 0
    n_len = 0
    n_zipf = 0
    n_banned = 0

    for w in iter_wordlist(lang):
        n_total += 1
        w = w.strip().lower()
        if not w:
            continue

        if w in seen:
            continue
        seen.add(w)

        if w in stop:
            n_stop += 1
            continue

        if w in banned:
            n_banned += 1
            continue

        if not token_re.match(w):
            n_badshape += 1
            continue

        if len(w) < min_len or len(w) > max_len:
            n_len += 1
            continue

        z = float(zipf_frequency(w, lang))
        if z < min_zipf or z > max_zipf:
            n_zipf += 1
            continue

        kept.append((w, z))

    # deterministic ordering: more frequent first, then alpha
    kept.sort(key=lambda t: (-t[1], t[0]))

    n_pre_cap = len(kept)
    if max_words is not None:
        if int(max_words) <= 0:
            raise ValueError(f"max_words must be > 0 (got {max_words})")
        kept = kept[: int(max_words)]

    vocab = [w for (w, _) in kept]
    zs = [z for (_, z) in kept]

    manifest = {
        "lang": lang,
        "stop_n": stop_n,
        "min_zipf": min_zipf,
        "max_zipf": max_zipf,
        "min_len": min_len,
        "max_len": max_len,
        "token_regex": token_regex,
        "max_words": int(max_words) if max_words is not None else None,
        "counts": {
            "total_seen": n_total,
            "kept_pre_cap": n_pre_cap,
            "kept": len(vocab),
            "capped": max(0, n_pre_cap - len(vocab)),
            "filtered_stopwords": n_stop,
            "filtered_banned": n_banned,
            "filtered_badshape": n_badshape,
            "filtered_length": n_len,
            "filtered_zipf": n_zipf,
        },
        "zipf_stats": {
            "min": min(zs) if zs else None,
            "max": max(zs) if zs else None,
            "mean": (sum(zs) / len(zs)) if zs else None,
            "p10": percentile(zs, 10) if zs else None,
            "p50": percentile(zs, 50) if zs else None,
            "p90": percentile(zs, 90) if zs else None,
        },
        "examples_top20": vocab[:20],
    }
    return vocab, manifest


def percentile(values: List[float], p: float) -> float:
    # small deterministic percentile helper without numpy
    if not values:
        return float("nan")
    xs = sorted(values)
    if p <= 0:
        return xs[0]
    if p >= 100:
        return xs[-1]
    k = (len(xs) - 1) * (p / 100.0)
    f = int(k)
    c = min(f + 1, len(xs) - 1)
    if f == c:
        return xs[f]
    d = k - f
    return xs[f] * (1 - d) + xs[c] * d


def write_vocab(out_path: str, vocab: List[str]) -> None:
    p = Path(out_path)
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text("\n".join(vocab) + "\n", encoding="utf-8")


def write_manifest(path: str, manifest: dict) -> None:
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(manifest, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")

def main():
    # Internal defaults (used if neither CLI nor config provide values)
    OUT_DEFAULT = "data/vocab.txt"
    LANG_DEFAULT = "en"
    STOP_N_DEFAULT = 300
    MIN_ZIPF_DEFAULT = 3.0
    MAX_ZIPF_DEFAULT = 6.0
    MIN_LEN_DEFAULT = 3
    MAX_LEN_DEFAULT = 12
    TOKEN_REGEX_DEFAULT = r"^[a-z]+$"

    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default=None, help="Optional YAML config path (e.g., configs/default.yaml)")

    # If provided, CLI overrides config; if omitted, config (then defaults) are used.
    ap.add_argument("--out", default=None, help="Output vocab path (overrides config paths.vocab_path)")
    ap.add_argument("--manifest", default=None, help="Optional JSON manifest path (overrides config vocab.manifest_path)")

    ap.add_argument("--lang", default=None, help="Language code for wordfreq (overrides config vocab.lang)")
    ap.add_argument("--stop-n", type=int, default=None, help="Remove top-N most frequent words as stopwords")
    ap.add_argument("--min-zipf", type=float, default=None, help="Minimum Zipf frequency to keep")
    ap.add_argument("--max-zipf", type=float, default=None, help="Maximum Zipf frequency to keep (drops ultra-common)")
    ap.add_argument("--min-len", type=int, default=None, help="Minimum token length")
    ap.add_argument("--max-len", type=int, default=None, help="Maximum token length")
    ap.add_argument(
        "--max-words",
        type=int,
        default=None,
        help="Optional cap on vocab size after filtering/sorting (top-N by Zipf). Overrides config vocab.max_words.",
    )
    ap.add_argument(
        "--token-regex",
        default=None,
        help="Regex a token must match (default keeps only lowercase alphabetic words).",
    )
    ap.add_argument(
        "--extra-banlist",
        default=None,
        help="Optional path to a newline-separated banlist (overrides config vocab.extra_banlist).",
    )

    args = ap.parse_args()

    cfg = load_yaml(args.config) if args.config else {}
    vcfg = cfg.get("vocab", {}) if isinstance(cfg, dict) else {}
    pcfg = cfg.get("paths", {}) if isinstance(cfg, dict) else {}

    out_path = (
        args.out
        or pcfg.get("vocab_path")
        or vcfg.get("out")
        or OUT_DEFAULT
    )
    manifest_path = (
        args.manifest
        or vcfg.get("manifest_path")
        or None
    )

    lang = args.lang or vcfg.get("lang") or LANG_DEFAULT
    stop_n = args.stop_n if args.stop_n is not None else int(vcfg.get("stop_n", STOP_N_DEFAULT))
    min_zipf = args.min_zipf if args.min_zipf is not None else float(vcfg.get("min_zipf", MIN_ZIPF_DEFAULT))
    max_zipf = args.max_zipf if args.max_zipf is not None else float(vcfg.get("max_zipf", MAX_ZIPF_DEFAULT))
    min_len = args.min_len if args.min_len is not None else int(vcfg.get("min_len", MIN_LEN_DEFAULT))
    max_len = args.max_len if args.max_len is not None else int(vcfg.get("max_len", MAX_LEN_DEFAULT))
    token_regex = args.token_regex or vcfg.get("token_regex") or TOKEN_REGEX_DEFAULT
    max_words = args.max_words if args.max_words is not None else vcfg.get("max_words", None)
    if max_words is not None:
        max_words = int(max_words)

    extra_banlist_path = args.extra_banlist or vcfg.get("extra_banlist") or None
    extra_ban = load_extra_banlist(extra_banlist_path)

    vocab, manifest = build_vocab(
        lang=lang,
        stop_n=stop_n,
        min_zipf=min_zipf,
        max_zipf=max_zipf,
        min_len=min_len,
        max_len=max_len,
        token_regex=token_regex,
        extra_ban=extra_ban,
        max_words=max_words,
    )

    write_vocab(out_path, vocab)
    print(f"Wrote {len(vocab)} words -> {out_path}")

    if manifest_path:
        write_manifest(manifest_path, manifest)
        print(f"Wrote manifest -> {manifest_path}")
    else:
        # quick console summary
        print(json.dumps(manifest["counts"], indent=2))


if __name__ == "__main__":
    main()


================================================
FILE: src/chat_formatting.py
================================================
# src/chat_formatting.py
from __future__ import annotations

from typing import Any, Dict, List, Literal

Role = Literal["spymaster", "guesser", "generic"]


def cfg_use_chat_template(cfg: Dict[str, Any]) -> bool:
    return bool(cfg.get("qwen", {}).get("use_chat_template", False))


def cfg_enable_thinking(cfg: Dict[str, Any], role: Role) -> bool:
    q = cfg.get("qwen", {}) or {}
    if role == "spymaster":
        return bool(q.get("enable_thinking_spymaster", True))
    if role == "guesser":
        return bool(q.get("enable_thinking_guesser", True))
    return bool(q.get("enable_thinking", True))


def apply_chat_template(
    tokenizer: Any,
    messages: List[Dict[str, str]],
    *,
    add_generation_prompt: bool = True,
    enable_thinking: bool = True,
) -> str:
    """
    One canonical place to handle tokenizer.apply_chat_template signature drift.
    """
    try:
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=add_generation_prompt,
            enable_thinking=enable_thinking,
        )
    except TypeError:
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=add_generation_prompt,
        )


================================================
FILE: src/epoch_eval.py
================================================
# src/epoch_eval.py
from __future__ import annotations

import json
import math
import os
import random
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch

from .model_wrappers import apply_chat_template, GenerationConfig  
from .think_utils import extract_think as _extract_think, strip_think_blocks as _strip_think_blocks

# --------- small helpers ---------

_FINAL_RE = re.compile(r"FINAL\s*:\s*(.+)", re.IGNORECASE)
_GSM8K_REF_RE = re.compile(r"####\s*([^\n]+)")


def extract_think_span(text: str) -> str:
    """
    Back-compat wrapper (previously regex-based in this file).
    Returns inner content of the FIRST closed <think>...</think> block.
    """
    return _extract_think(text, mode="inner", which="first", allow_partial=False)


def strip_think_blocks(text: str) -> str:
    """
    Back-compat wrapper (previously regex-based in this file).
    Removes all CLOSED <think>...</think> blocks.
    """
    return _strip_think_blocks(text, remove_dangling_tags=True)


def count_tokens(tokenizer, text: str) -> int:
    if not text:
        return 0
    return len(tokenizer(text, add_special_tokens=False)["input_ids"])


def normalize_number_str(s: str) -> str:
    s = (s or "").strip()
    s = s.replace(",", "")
    s = s.strip().strip("`").strip()
    try:
        if re.fullmatch(r"[-+]?\d+(\.\d+)?", s):
            if "." in s:
                f = float(s)
                if abs(f - round(f)) < 1e-9:
                    return str(int(round(f)))
                return str(f).rstrip("0").rstrip(".")
            return str(int(s))
    except Exception:
        pass
    return s


def parse_gsm8k_ref(answer: str) -> str:
    m = _GSM8K_REF_RE.search(answer or "")
    if not m:
        return normalize_number_str(answer)
    return normalize_number_str(m.group(1))


def parse_gsm8k_pred(text: str) -> Optional[str]:
    text = text or ""
    m = _FINAL_RE.search(text)
    if m:
        line = m.group(1).strip().splitlines()[0].strip()
        return normalize_number_str(line.strip().rstrip("."))
    nums2 = re.findall(r"[-+]?\d+(?:\.\d+)?", text.replace(",", ""))
    if nums2:
        return normalize_number_str(nums2[-1])
    return None


def extract_code_from_completion(text: str) -> str:
    """
    HumanEval: return python code only.
    - removes <think> blocks
    - extracts from ```python ... ``` if present
    - else extracts starting at first 'def '
    """
    t = strip_think_blocks(text or "").strip()

    fence = re.search(r"```(?:python)?\s*(.*?)```", t, re.DOTALL | re.IGNORECASE)
    if fence:
        return fence.group(1).strip()

    idx = t.find("def ")
    if idx != -1:
        return t[idx:].strip()

    return t


# --------- HumanEval sandbox runner ---------

_ALLOWED_IMPORTS = {
    "math",
    "itertools",
    "functools",
    "collections",
    "re",
    "string",
    "heapq",
    "bisect",
    "typing",
    "statistics",
    "fractions",
    "decimal",
}


def _limited_import(name, globals=None, locals=None, fromlist=(), level=0):
    root = (name or "").split(".")[0]
    if root in _ALLOWED_IMPORTS:
        return __import__(name, globals, locals, fromlist, level)
    raise ImportError(f"Import blocked: {name}")


def _safe_builtins() -> Dict[str, Any]:
    allowed = {
        "abs": abs,
        "all": all,
        "any": any,
        "bool": bool,
        "dict": dict,
        "enumerate": enumerate,
        "float": float,
        "int": int,
        "len": len,
        "list": list,
        "max": max,
        "min": min,
        "range": range,
        "reversed": reversed,
        "round": round,
        "set": set,
        "sorted": sorted,
        "str": str,
        "sum": sum,
        "tuple": tuple,
        "zip": zip,
        "__import__": _limited_import,
    }
    return allowed


def _humaneval_worker(payload: Dict[str, Any], q) -> None:
    """
    Runs in a subprocess. Returns (passed: bool, err: str|None)
    """
    try:
        import signal

        signal.signal(signal.SIGALRM, lambda *_: (_ for _ in ()).throw(TimeoutError("timeout")))
        signal.alarm(int(payload.get("timeout_s", 3)))

        env: Dict[str, Any] = {"__builtins__": _safe_builtins()}
        code = payload["code"]
        test = payload["test"]
        entry_point = payload["entry_point"]

        exec(code, env, env)
        if entry_point not in env:
            q.put((False, f"missing entry_point: {entry_point}"))
            return

        exec(test, env, env)

        check = env.get("check", None)
        if check is None:
            q.put((False, "missing check() in test"))
            return

        check(env[entry_point])
        q.put((True, None))
    except Exception as e:
        q.put((False, f"{type(e).__name__}: {e}"))


def run_humaneval_case(code: str, test: str, entry_point: str, timeout_s: int = 3) -> Tuple[bool, Optional[str]]:
    """
    Executes in a separate process with:
      - import whitelist
      - limited builtins
      - hard timeout
    """
    import multiprocessing as mp

    ctx = mp.get_context("spawn")
    q = ctx.Queue()
    p = ctx.Process(
        target=_humaneval_worker,
        args=({"code": code, "test": test, "entry_point": entry_point, "timeout_s": timeout_s}, q),
    )
    p.start()
    p.join(timeout_s + 1)
    if p.is_alive():
        p.kill()
        return (False, "Timeout: killed")
    try:
        passed, err = q.get_nowait()
        return (bool(passed), err)
    except Exception:
        return (False, "No result")


# --------- Model wrapper for codenames rollout ---------

class InMemoryHFGenerator:
    """
    Minimal TextGenerator-like wrapper around an in-memory (Peft) model + tokenizer.
    Implements generate() and generate_batch() (sequential per-item seeds).

    IMPORTANT: generate_batch signature matches rollout.run_turns_batched():
      generate_batch(prompts: List[str], gen_cfg: GenerationConfig, seeds: Optional[List[Optional[int]]])
    """

    def __init__(self, model, tokenizer, *, disable_adapter: bool = False):
        self.model = model
        self.tokenizer = tokenizer
        self.model_id = getattr(getattr(model, "config", None), "_name_or_path", "in_memory")
        self._disable_adapter = disable_adapter

        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def format_chat(self, messages: List[Dict[str, str]], *, add_generation_prompt=True, enable_thinking=True) -> str:
        return apply_chat_template(
            self.tokenizer,
            messages,
            add_generation_prompt=add_generation_prompt,
            enable_thinking=enable_thinking,
        )

    def _maybe_disable_adapter_ctx(self):
        m = self.model
        if not self._disable_adapter:
            return _NullCtx()
        if hasattr(m, "disable_adapter"):
            try:
                return m.disable_adapter()
            except TypeError:
                return m.disable_adapter()
        return _NullCtx()

    @torch.no_grad()
    def generate(
        self,
        prompt: str,
        *,
        temperature: float,
        top_p: float,
        top_k: int | None,
        max_new_tokens: int,
        seed: Optional[int] = None,
    ) -> str:
        if seed is not None:
            torch.manual_seed(int(seed))
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(int(seed))

        inputs = self.tokenizer(prompt, return_tensors="pt", padding=False)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        do_sample = temperature is not None and float(temperature) > 1e-6

        gen_kwargs = dict(
            do_sample=do_sample,
            max_new_tokens=int(max_new_tokens),
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
        )
        if do_sample:
            gen_kwargs.update(
                dict(
                    temperature=float(temperature),
                    top_p=float(top_p),
                    top_k=int(top_k) if top_k is not None else None,
                )
            )

        with self._maybe_disable_adapter_ctx():
            out = self.model.generate(**inputs, **gen_kwargs)

        prompt_len = inputs["input_ids"].shape[1]
        gen_ids = out[0][prompt_len:]
        return self.tokenizer.decode(gen_ids, skip_special_tokens=True)

    def generate_batch(
        self,
        prompts: List[str],
        gen_cfg: GenerationConfig,
        seeds: Optional[List[Optional[int]]] = None,
    ) -> List[str]:
        if seeds is None:
            seeds = [None] * len(prompts)
        outs: List[str] = []
        for p, sd in zip(prompts, seeds):
            outs.append(
                self.generate(
                    p,
                    temperature=float(gen_cfg.temperature),
                    top_p=float(gen_cfg.top_p),
                    top_k=(int(gen_cfg.top_k) if gen_cfg.top_k is not None else None),
                    max_new_tokens=int(gen_cfg.max_new_tokens),
                    seed=sd,
                )
            )
        return outs


class _NullCtx:
    def __enter__(self):
        return None

    def __exit__(self, exc_type, exc, tb):
        return False


# --------- Eval routines ---------

@dataclass
class ProbeSamples:
    gsm8k: List[Dict[str, Any]]
    humaneval: List[Dict[str, Any]]
    wikitext: List[str]


def load_probe_samples(
    *,
    seed: int,
    gsm8k_n: int,
    humaneval_n: int,
    wikitext_n: int,
) -> ProbeSamples:
    from datasets import load_dataset

    rng = random.Random(seed)

    gsm = load_dataset("gsm8k", "main", split="test")
    gsm_idx = rng.sample(range(len(gsm)), k=min(gsm8k_n, len(gsm)))
    gsm_samp = [gsm[i] for i in gsm_idx]

    he = load_dataset("openai/openai_humaneval", split="test")
    he_idx = rng.sample(range(len(he)), k=min(humaneval_n, len(he)))
    he_samp = [he[i] for i in he_idx]

    wt = load_dataset("wikitext", "wikitext-2-raw-v1", split="validation")
    texts = [t for t in wt["text"] if isinstance(t, str) and t.strip()]
    wt_idx = rng.sample(range(len(texts)), k=min(wikitext_n, len(texts)))
    wt_samp = [texts[i] for i in wt_idx]

    return ProbeSamples(gsm8k=gsm_samp, humaneval=he_samp, wikitext=wt_samp)


@torch.no_grad()
def eval_gsm8k(
    *,
    model,
    tokenizer,
    samples: List[Dict[str, Any]],
    device: torch.device,
    max_new_tokens: int = 384,
) -> Dict[str, Any]:
    model.eval()
    correct = 0
    n = 0
    think_tokens: List[int] = []

    for ex in samples:
        q = ex["question"]
        ref = parse_gsm8k_ref(ex["answer"])

        messages = [
            {"role": "system", "content": "You are a careful math assistant."},
            {
                "role": "user",
                "content": (
                    "Solve the problem.\n"
                    "Put your reasoning inside <think>...</think>.\n"
                    "Then output exactly one line: FINAL: <number>\n\n"
                    f"PROBLEM:\n{q}\n"
                ),
            },
        ]

        prompt = apply_chat_template(tokenizer, messages, add_generation_prompt=True, enable_thinking=True)

        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        out = model.generate(
            **inputs,
            do_sample=True,
            temperature=0.2,
            top_p=0.95,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
        gen = tokenizer.decode(out[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True)

        think = extract_think_span(gen)
        think_tokens.append(count_tokens(tokenizer, think))

        pred = parse_gsm8k_pred(gen)
        if pred is not None and pred == ref:
            correct += 1
        n += 1

    acc = correct / max(1, n)
    return {
        "gsm8k_n": n,
        "gsm8k_acc": float(acc),
        "gsm8k_think_tokens_mean": float(sum(think_tokens) / max(1, len(think_tokens))),
        "gsm8k_think_tokens_p90": float(sorted(think_tokens)[int(0.9 * (len(think_tokens) - 1))]) if think_tokens else 0.0,
    }


@torch.no_grad()
def eval_humaneval(
    *,
    model,
    tokenizer,
    samples: List[Dict[str, Any]],
    device: torch.device,
    max_new_tokens: int = 384,
    timeout_s: int = 3,
) -> Dict[str, Any]:
    model.eval()
    passed = 0
    n = 0
    think_tokens: List[int] = []

    for ex in samples:
        prompt = ex["prompt"]
        test = ex["test"]
        entry = ex["entry_point"]

        messages = [
            {"role": "system", "content": "You write correct Python functions."},
            {
                "role": "user",
                "content": (
                    "Write the Python function described below.\n"
                    "Optional: if you think, put it in <think>...</think>.\n"
                    "Then output ONLY valid Python code (no backticks, no extra text).\n\n"
                    f"{prompt}"
                ),
            },
        ]

        chat = apply_chat_template(tokenizer, messages, add_generation_prompt=True, enable_thinking=True)

        inputs = tokenizer(chat, return_tensors="pt").to(device)
        out = model.generate(
            **inputs,
            do_sample=True,
            temperature=0.2,
            top_p=0.95,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
        gen = tokenizer.decode(out[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True)

        think = extract_think_span(gen)
        think_tokens.append(count_tokens(tokenizer, think))

        code_only = extract_code_from_completion(gen)
        full_code = prompt + "\n" + code_only + "\n"
        ok, _err = run_humaneval_case(full_code, test, entry_point=entry, timeout_s=timeout_s)

        passed += 1 if ok else 0
        n += 1

    return {
        "humaneval_n": n,
        "humaneval_pass_rate": float(passed / max(1, n)),
        "humaneval_think_tokens_mean": float(sum(think_tokens) / max(1, len(think_tokens))),
        "humaneval_think_tokens_p90": float(sorted(think_tokens)[int(0.9 * (len(think_tokens) - 1))]) if think_tokens else 0.0,
    }


@torch.no_grad()
def eval_wikitext2_ppl(
    *,
    model,
    tokenizer,
    texts: List[str],
    device: torch.device,
    block_size: int = 512,
    max_blocks: int = 80,
) -> Dict[str, Any]:
    model.eval()
    total_nll = 0.0
    total_tokens = 0

    blocks_done = 0
    for t in texts:
        enc = tokenizer(t, return_tensors="pt", add_special_tokens=False)
        input_ids = enc["input_ids"].to(device)
        if input_ids.numel() < 2:
            continue

        seq_len = input_ids.shape[1]
        for i in range(0, seq_len, block_size):
            if blocks_done >= max_blocks:
                break
            chunk = input_ids[:, i : i + block_size]
            if chunk.shape[1] < 2:
                continue
            labels = chunk.clone()
            out = model(chunk, labels=labels)
            n_tokens = labels.numel()
            total_nll += float(out.loss) * n_tokens
            total_tokens += n_tokens
            blocks_done += 1
        if blocks_done >= max_blocks:
            break

    ppl = math.exp(total_nll / max(1, total_tokens))
    return {
        "wikitext2_blocks": int(blocks_done),
        "wikitext2_tokens": int(total_tokens),
        "wikitext2_ppl": float(ppl),
    }


def eval_codenames_subset(
    *,
    cfg: Dict[str, Any],
    model,
    tokenizer,
    device: torch.device,
    n_boards: int,
    seed: int,
) -> Dict[str, Any]:
    """
    Computes the same key fields as src/metrics.py aggregate() but on a sampled subset of eval boards.
    """
    from .utils import read_jsonl
    from .rollout import run_turns_batched
    from .metrics import aggregate

    rng = random.Random(seed)

    boards = read_jsonl(cfg["paths"]["boards_eval_path"])
    if not boards:
        return {"codenames_n_boards": 0}

    idx = rng.sample(range(len(boards)), k=min(n_boards, len(boards)))
    subset = [boards[i] for i in idx]

    sp_max = min(int(cfg["decoding"].get("spymaster_max_new_tokens", 512)), 512)
    g_max = min(int(cfg["decoding"].get("guesser_max_new_tokens", 256)), 256)

    sp_gen = InMemoryHFGenerator(model, tokenizer, disable_adapter=False)
    g_gen = InMemoryHFGenerator(model, tokenizer, disable_adapter=True)

    bests, metas = run_turns_batched(
        subset,
        sp_gen,
        g_gen,
        embedder=None,
        cfg={
            **cfg,
            "decoding": {
                **cfg["decoding"],
                "n_candidates": 1,
                "spymaster_max_new_tokens": sp_max,
                "guesser_max_new_tokens": g_max,
            },
        },
        n_candidates=1,
    )

    per_board = []
    think_tok = []
    for b, best, meta in zip(subset, bests, metas):
        think = extract_think_span(best.raw_spymaster_text)
        think_tok.append(count_tokens(tokenizer, think))
        per_board.append(
            {
                "board_id": b["board_id"],
                "reward": float(best.reward),
                "clue": best.clue,
                "stats": {**best.stats, "directness": float(best.directness)},
            }
        )

    m = aggregate(per_board)
    return {
        "codenames_n_boards": int(m.get("n_boards", len(per_board))),
        "reward_mean": float(m.get("reward_mean", 0.0)),
        "reward_median": float(m.get("reward_median", 0.0)),
        "reward_ci95_lo": float(m.get("reward_ci95", (0.0, 0.0))[0]),
        "reward_ci95_hi": float(m.get("reward_ci95", (0.0, 0.0))[1]),
        "assassin_rate": float(m.get("assassin_rate", 0.0)),
        "team_mean": float(m.get("team_mean", 0.0)),
        "opp_mean": float(m.get("opp_mean", 0.0)),
        "neu_mean": float(m.get("neu_mean", 0.0)),
        "codenames_spymaster_think_tokens_mean": float(sum(think_tok) / max(1, len(think_tok))),
        "codenames_spymaster_think_tokens_p90": float(sorted(think_tok)[int(0.9 * (len(think_tok) - 1))]) if think_tok else 0.0,
    }


# --------- plotting ---------

def _read_jsonl(path: Path) -> List[Dict[str, Any]]:
    if not path.exists():
        return []
    out = []
    for line in path.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line:
            continue
        out.append(json.loads(line))
    return out


def plot_epoch_history(history_path: Path, out_dir: Path) -> None:
    """
    Writes a small set of PNG plots vs epoch.
    """
    import matplotlib.pyplot as plt

    rows = _read_jsonl(history_path)
    if not rows:
        return

    out_dir.mkdir(parents=True, exist_ok=True)

    epochs = [r["epoch"] for r in rows]

    def series(key: str) -> List[float]:
        xs = []
        for r in rows:
            v = r.get(key, None)
            xs.append(float(v) if v is not None else float("nan"))
        return xs

    plt.figure()
    plt.plot(epochs, series("gsm8k_acc"), label="GSM8K acc")
    plt.plot(epochs, series("humaneval_pass_rate"), label="HumanEval pass rate")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("score")
    plt.title("Correctness vs epoch")
    plt.tight_layout()
    plt.savefig(out_dir / "correctness.png")
    plt.close()

    plt.figure()
    plt.plot(epochs, series("gsm8k_think_tokens_mean"), label="GSM8K think tok (mean)")
    plt.plot(epochs, series("humaneval_think_tokens_mean"), label="HumanEval think tok (mean)")
    plt.plot(epochs, series("codenames_spymaster_think_tokens_mean"), label="Codenames spymaster think tok (mean)")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("tokens")
    plt.title("<think> verbosity vs epoch")
    plt.tight_layout()
    plt.savefig(out_dir / "think_verbosity.png")
    plt.close()

    plt.figure()
    plt.plot(epochs, series("wikitext2_ppl"))
    plt.xlabel("epoch")
    plt.ylabel("perplexity")
    plt.title("WikiText-2 PPL vs epoch")
    plt.tight_layout()
    plt.savefig(out_dir / "wikitext2_ppl.png")
    plt.close()

    plt.figure()
    plt.plot(epochs, series("train_loss_epoch_mean"), label="train loss (epoch mean)")
    if any(not math.isnan(x) for x in series("train_loss_epoch_ema")):
        plt.plot(epochs, series("train_loss_epoch_ema"), label="train loss (EMA)")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("loss")
    plt.title("Train loss vs epoch")
    plt.tight_layout()
    plt.savefig(out_dir / "train_loss.png")
    plt.close()

    grad = series("grad_norm_epoch_mean")
    if any(not math.isnan(x) for x in grad):
        plt.figure()
        plt.plot(epochs, grad)
        plt.xlabel("epoch")
        plt.ylabel("grad_norm")
        plt.title("Gradient norm vs epoch")
        plt.tight_layout()
        plt.savefig(out_dir / "grad_norm.png")
        plt.close()

    plt.figure()
    rm = series("reward_mean")
    rlo = series("reward_ci95_lo")
    rhi = series("reward_ci95_hi")
    plt.plot(epochs, rm, label="reward_mean")
    try:
        import numpy as np

        x = np.array(epochs, dtype=float)
        lo = np.array(rlo, dtype=float)
        hi = np.array(rhi, dtype=float)
        plt.fill_between(x, lo, hi, alpha=0.2, label="CI95")
    except Exception:
        pass
    plt.plot(epochs, series("reward_median"), label="reward_median")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("reward")
    plt.title("Codenames reward vs epoch (subset eval)")
    plt.tight_layout()
    plt.savefig(out_dir / "codenames_reward.png")
    plt.close()

    plt.figure()
    plt.plot(epochs, series("assassin_rate"), label="assassin_rate")
    plt.plot(epochs, series("team_mean"), label="team_mean")
    plt.plot(epochs, series("opp_mean"), label="opp_mean")
    plt.plot(epochs, series("neu_mean"), label="neu_mean")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("value")
    plt.title("Codenames behavior metrics vs epoch (subset eval)")
    plt.tight_layout()
    plt.savefig(out_dir / "codenames_behavior.png")
    plt.close()


================================================
FILE: src/eval.py
================================================
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List

import numpy as np

from .io_utils import shard_path_insert_before_suffix, merge_jsonl_shards
from .metrics import aggregate
from .model_wrappers import Embedder, make_text_generator, load_lora_on_generator
from .mp_utils import is_child_process, child_shard_info, launch_children
from .rollout import run_turns_batched
from .utils import (
    ensure_dir,
    load_yaml,
    read_jsonl,
    save_config_snapshot,
    save_run_meta,
    set_global_seed,
    write_jsonl,
)


def _shard_out_path(out_dir: str | Path, shard_id: int, num_shards: int) -> Path:
    out_dir = Path(out_dir)
    return shard_path_insert_before_suffix(out_dir / "per_board.jsonl", shard_id, num_shards)


def _merge_shards(out_dir: str | Path, num_shards: int) -> List[Dict[str, Any]]:
    out_dir = Path(out_dir)
    shard_paths = [_shard_out_path(out_dir, sid, num_shards) for sid in range(num_shards)]
    combined = merge_jsonl_shards(shard_paths, sort_key=lambda r: str(r.get("board_id", "")))
    return combined


# -------------------------
# vLLM LoRA toggling proxy (so we can share one engine)
# -------------------------

class _VLLMProxy:
    """
    Delegate to a base generator but force a particular LoRA request for calls.
    Works with VLLMTextGenerator which stores LoRARequest on `._lora_request`.
    """

    def __init__(self, base, lora_request):
        self._base = base
        self._lora_request = lora_request
        self.model_id = getattr(base, "model_id", "unknown")

    def format_chat(self, *args, **kwargs):
        return self._base.format_chat(*args, **kwargs)

    def generate(self, *args, **kwargs):
        old = getattr(self._base, "_lora_request", None)
        try:
            setattr(self._base, "_lora_request", self._lora_request)
            return self._base.generate(*args, **kwargs)
        finally:
            setattr(self._base, "_lora_request", old)

    def generate_batch(self, *args, **kwargs):
        old = getattr(self._base, "_lora_request", None)
        try:
            setattr(self._base, "_lora_request", self._lora_request)
            return self._base.generate_batch(*args, **kwargs)
        finally:
            setattr(self._base, "_lora_request", old)


# -------------------------
# Main
# -------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    ap.add_argument("--mode", choices=["baseline", "sft"], required=True)
    ap.add_argument("--out", required=True, help="Output directory (e.g., outputs/baselines/run1)")
    args = ap.parse_args()

    cfg = load_yaml(args.config)
    set_global_seed(int(cfg["training"].get("seed", 0)))

    num_procs = int(cfg.get("inference", {}).get("num_processes", 1))
    batch_size = int(cfg.get("inference", {}).get("batch_size", 1))

    out_dir = ensure_dir(args.out)

    # Master process: write run meta, spawn children, then merge + aggregate
    if num_procs > 1 and not is_child_process():
        save_config_snapshot(cfg, out_dir)
        save_run_meta(
            out_dir,
            command=["python", "-m", "src.eval", "--config", args.config, "--mode", args.mode, "--out", args.out],
        )

        if cfg.get("inference", {}).get("backend") == "vllm":
            tp = int(cfg.get("inference", {}).get("vllm", {}).get("tensor_parallel_size", 1))
            if tp != 1:
                print(
                    f"[warn] inference.vllm.tensor_parallel_size={tp} with inference.num_processes={num_procs}. "
                    f"Usually you want tensor_parallel_size=1 when using multiple processes."
                )

        # mp_utils.launch_children(module, argv, num_procs)
        launch_children("src.eval", ["--config", args.config, "--mode", args.mode, "--out", args.out], num_procs)

        # Merge shard outputs and compute metrics once
        per_board = _merge_shards(out_dir, num_procs)
        per_path = Path(out_dir) / "per_board.jsonl"
        write_jsonl(per_path, per_board)

        metrics = aggregate(per_board)
        with open(Path(out_dir) / "metrics.json", "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2)

        print(f"Wrote per-board -> {per_path}")
        print(f"Wrote metrics   -> {Path(out_dir) / 'metrics.json'}")
        return

    # Worker (or single process)
    if num_procs == 1 and not is_child_process():
        save_config_snapshot(cfg, out_dir)
        save_run_meta(
            out_dir,
            command=["python", "-m", "src.eval", "--config", args.config, "--mode", args.mode, "--out", args.out],
        )

    boards = read_jsonl(cfg["paths"]["boards_eval_path"])

    shard_id, num_shards = child_shard_info() if is_child_process() else (0, 1)
    if num_shards > 1:
        boards = [b for i, b in enumerate(boards) if (i % num_shards) == shard_id]
        print(f"[shard {shard_id}/{num_shards}] boards={len(boards)}")

    # Build generators (avoid two vLLM engines when model_ids match)
    backend = cfg.get("inference", {}).get("backend", "hf")
    sp_id = cfg["models"]["spymaster_model_id"]
    g_id = cfg["models"]["guesser_model_id"]

    if backend == "vllm" and sp_id == g_id:
        base = make_text_generator(sp_id, cfg)

        if args.mode == "sft":
            adapter_dir = cfg["training"]["output_adapter_dir"]
            base = load_lora_on_generator(base, adapter_dir)
            # capture LoRA request then disable it for guesser calls
            lora_req = getattr(base, "_lora_request", None)
            setattr(base, "_lora_request", None)

            spymaster = _VLLMProxy(base, lora_req)
            guesser = _VLLMProxy(base, None)
        else:
            spymaster = base
            guesser = base
    else:
        # General case: separate generators
        spymaster = make_text_generator(sp_id, cfg)
        if args.mode == "sft":
            adapter_dir = cfg["training"]["output_adapter_dir"]
            spymaster = load_lora_on_generator(spymaster, adapter_dir)
        guesser = make_text_generator(g_id, cfg)

    # Embedder
    use_embed = bool(cfg.get("constraints", {}).get("enable_directness_check", True))
    embedder = None
    if use_embed:
        device = "cuda" if __import__("torch").cuda.is_available() else "cpu"
        embedder = Embedder(cfg["models"]["embedding_model_id"], device=device)

    # Evaluate
    per_board: List[Dict[str, Any]] = []

    # Keep n_candidates=1 in eval
    n_candidates = 1

    for start in range(0, len(boards), max(1, batch_size)):
        batch = boards[start : start + max(1, batch_size)]

        bests, metas = run_turns_batched(
            batch,
            spymaster,
            guesser,
            embedder,
            cfg,
            n_candidates=n_candidates,
        )

        for b, best, meta in zip(batch, bests, metas):
            rec = {
                "board_id": b["board_id"],
                "reward": float(best.reward),
                "clue": best.clue,
                "num": int(best.num),
                "guess_words": best.guess_words,
                "stats": {**best.stats, "directness": float(best.directness)},
                "clue_meta": {
                    "valid": bool(best.valid),
                    "rejected_total": int(meta["rejected_total"]),
                    "rejection_counts": meta["rejection_counts"],
                },
            }
            per_board.append(rec)

        done = min(start + len(batch), len(boards))
        if done % 50 == 0 or done == len(boards):
            mr = float(np.mean([r["reward"] for r in per_board])) if per_board else 0.0
            print(f"[shard {shard_id}/{num_shards}] [{done}/{len(boards)}] mean_reward={mr:.3f}")

    # Write outputs
    if num_shards > 1 and is_child_process():
        shard_path = _shard_out_path(out_dir, shard_id, num_shards)
        write_jsonl(shard_path, per_board)
        print(f"[shard {shard_id}/{num_shards}] Wrote per-board shard -> {shard_path}")
        return

    # Single-process final outputs
    per_path = Path(out_dir) / "per_board.jsonl"
    write_jsonl(per_path, per_board)

    metrics = aggregate(per_board)
    with open(Path(out_dir) / "metrics.json", "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)

    print(f"Wrote per-board -> {per_path}")
    print(f"Wrote metrics   -> {Path(out_dir) / 'metrics.json'}")


if __name__ == "__main__":
    main()


================================================
FILE: src/gen_cfg.py
================================================
# src/gen_cfg.py
from .model_wrappers import GenerationConfig

def spymaster_gen_cfg(cfg) -> GenerationConfig:
    d = cfg["decoding"]
    return GenerationConfig(
        temperature=float(d["spymaster_temperature"]),
        top_p=float(d["spymaster_top_p"]),
        top_k=int(d.get("spymaster_top_k", 20)),
        max_new_tokens=int(d["spymaster_max_new_tokens"]),
    )

def guesser_gen_cfg(cfg) -> GenerationConfig:
    d = cfg["decoding"]
    return GenerationConfig(
        temperature=float(d["guesser_temperature"]),
        top_p=float(d["guesser_top_p"]),
        top_k=int(d.get("guesser_top_k", 20)),
        max_new_tokens=int(d["guesser_max_new_tokens"]),
    )


================================================
FILE: src/generate_sft_data.py
================================================
# src/generate_sft_data.py
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np

from .io_utils import shard_path_append_to_suffix, merge_jsonl_shards
from .model_wrappers import Embedder, make_text_generator
from .mp_utils import launch_children, is_child_process, child_shard_info
from .prompting import render_prompt
from .rollout import run_turns_batched
from .spymaster_prompt import build_spymaster_messages
from .think_utils import extract_think as _extract_think
from .utils import load_yaml, read_jsonl, set_global_seed, write_jsonl, save_progress


# -------------------------
# Resume helpers
# -------------------------

def load_done_example_ids(raw_path: str | Path) -> set[str]:
    """Return example_ids already present in an existing raw jsonl (for resume)."""
    raw_path = Path(raw_path)
    done: set[str] = set()
    if not raw_path.exists():
        return done

    with raw_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                eid = obj.get("example_id") or obj.get("board_id")
                if eid:
                    done.add(str(eid))
            except Exception:
                # If a partial/corrupt last line exists, ignore it
                continue
    return done


def _try_load_progress(progress_path: Path) -> Optional[Dict[str, Any]]:
    try:
        if progress_path.exists():
            return json.loads(progress_path.read_text(encoding="utf-8"))
    except Exception:
        return None
    return None


def extract_think_block(text: str) -> str:
    """
    Back-compat wrapper (this file previously had a custom "last block + partial tolerant" parser).
    Returns the FULL <think>...</think> block (or best-effort partial) from the LAST block.
    """
    return _extract_think(text, mode="full", which="last", allow_partial=True)


# -------------------------
# Filtering
# -------------------------

def filter_examples(records: List[Dict[str, Any]], cfg: Dict[str, Any]) -> List[Dict[str, Any]]:
    fcfg = cfg["filtering"]
    mode = fcfg["mode"]

    if not records:
        return []

    min_reward = fcfg.get("min_reward", None)
    min_reward = float(min_reward) if min_reward is not None else None

    def passes_min_reward(r: Dict[str, Any]) -> bool:
        if min_reward is None:
            return True
        return float(r.get("reward", 0.0)) >= min_reward

    if mode == "top_percent":
        top_p = float(fcfg["top_percent"])
        rewards = np.array([r["reward"] for r in records], dtype=np.float32)
        thr = float(np.quantile(rewards, 1.0 - top_p))
        return [r for r in records if float(r["reward"]) >= thr and passes_min_reward(r)]

    if mode == "rule_based":
        min_team = int(fcfg.get("min_team_correct", 0))
        require_no_assassin = bool(fcfg.get("require_no_assassin", False))
        kept = []
        for r in records:
            if not passes_min_reward(r):
                continue
            st = r.get("stats", {})
            if int(st.get("n_team", 0)) < min_team:
                continue
            if require_no_assassin and int(st.get("assassin", 0)) > 0:
                continue
            kept.append(r)
        return kept

    raise ValueError(f"Unknown filtering mode: {mode}")


def _merge_shards_to_base(base_raw_path: str | Path, num_shards: int) -> List[Dict[str, Any]]:
    shard_paths = [shard_path_append_to_suffix(base_raw_path, sid, num_shards) for sid in range(num_shards)]
    combined = merge_jsonl_shards(shard_paths)
    write_jsonl(base_raw_path, combined)
    return combined


# -------------------------
# Main
# -------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()

    cfg = load_yaml(args.config)

    num_procs = int(cfg.get("inference", {}).get("num_processes", 1))
    batch_size = int(cfg.get("inference", {}).get("batch_size", 1))

    # Master: spawn workers and merge/filter when done
    if num_procs > 1 and not is_child_process():
        if cfg.get("inference", {}).get("backend") == "vllm":
            tp = int(cfg.get("inference", {}).get("vllm", {}).get("tensor_parallel_size", 1))
            if tp != 1:
                print(
                    f"[warn] inference.vllm.tensor_parallel_size={tp} with inference.num_processes={num_procs}. "
                    f"Usually you want tensor_parallel_size=1 when using multiple processes."
                )

        launch_children("src.generate_sft_data", ["--config", args.config], num_procs)

        base_raw_path = cfg.get("paths", {}).get("sft_turns_raw_path")
        if not base_raw_path:
            raise RuntimeError("paths.sft_turns_raw_path must be set when using inference.num_processes > 1")

        combined = _merge_shards_to_base(base_raw_path, num_procs)
        filtered = filter_examples(combined, cfg)
        write_jsonl(cfg["paths"]["sft_turns_path"], filtered)
        print(f"Merged {len(combined)} raw -> Filtered {len(filtered)} -> {cfg['paths']['sft_turns_path']}")
        return

    # Worker or single-process mode
    set_global_seed(int(cfg["training"].get("seed", 0)))

    boards = read_jsonl(cfg["paths"]["boards_train_path"])

    # Apply cap BEFORE sharding so total dataset size is bounded
    maxb = cfg.get("boards", {}).get("sft_max_train_boards", None)
    if maxb is not None:
        boards = boards[: int(maxb)]
        print(f"Using only first {len(boards)} train boards for SFT generation (boards.sft_max_train_boards={maxb}).")

    shard_id, num_shards = child_shard_info() if is_child_process() else (0, 1)

    # Shard boards by index
    if num_shards > 1:
        boards = [b for i, b in enumerate(boards) if (i % num_shards) == shard_id]
        print(f"[shard {shard_id}/{num_shards}] boards={len(boards)}")

    # IMPORTANT: per-shard total is defined BEFORE resume filtering
    shard_total = len(boards)

    # Paths (use per-shard raw file to avoid concurrent appends)
    base_raw_path = cfg.get("paths", {}).get("sft_turns_raw_path")
    if not base_raw_path:
        raise RuntimeError("paths.sft_turns_raw_path must be set")

    raw_path = Path(base_raw_path)
    if num_shards > 1:
        raw_path = shard_path_append_to_suffix(raw_path, shard_id, num_shards)

    progress_path = raw_path.with_suffix(raw_path.suffix + ".progress.json")

    # Resume: detect already-written examples
    done_ids = load_done_example_ids(raw_path)
    if done_ids:
        # CRITICAL FIX:
        # Skip compute for already-done boards BEFORE calling run_turns_batched
        before = len(boards)
        boards = [b for b in boards if str(b.get("board_id")) not in done_ids]
        after = len(boards)
        print(
            f"[shard {shard_id}/{num_shards}] Resuming: found {len(done_ids)} already-written examples in {raw_path}. "
            f"Remaining={after} (was {before}), shard_total={shard_total}."
        )
    else:
        print(f"[shard {shard_id}/{num_shards}] Starting fresh: shard_total={shard_total} -> {raw_path}")

    # Refresh / initialize progress at startup so timestamp updates even before new writes
    prev_prog = _try_load_progress(progress_path) or {}
    prev_mean = prev_prog.get("mean_reward_running", None)
    prev_done = prev_prog.get("done", None)

    # Running mean reward of BEST (one per board)
    running_sum = 0.0
    running_n = 0

    # Running stats for best-of-N uplift vs average candidate
    uplift_sum = 0.0
    uplift_n = 0
    cand_avg_sum = 0.0
    cand_std_sum = 0.0
    cand_valid_frac_sum = 0.0

    # Valid-only variants
    uplift_valid_sum = 0.0
    uplift_valid_n = 0
    cand_avg_valid_sum = 0.0

    # If prior progress is consistent with what we found, carry forward mean approx
    try:
        if prev_mean is not None and prev_done is not None and int(prev_done) == len(done_ids):
            running_n = int(prev_done)
            running_sum = float(prev_mean) * float(prev_done)

            # Carry forward uplift stats if they exist in the progress file.
            prev_cand_avg = prev_prog.get("mean_candidate_reward_running", None)
            prev_uplift = prev_prog.get("mean_best_minus_avg_reward_running", None)
            prev_cand_std = prev_prog.get("candidate_reward_std_running", None)
            prev_valid_frac = prev_prog.get("candidate_valid_frac_running", None)

            if prev_cand_avg is not None and prev_uplift is not None:
                uplift_n = int(prev_done)
                cand_avg_sum = float(prev_cand_avg) * float(prev_done)
                uplift_sum = float(prev_uplift) * float(prev_done)

            if prev_cand_std is not None:
                cand_std_sum = float(prev_cand_std) * float(prev_done)

            if prev_valid_frac is not None:
                cand_valid_frac_sum = float(prev_valid_frac) * float(prev_done)

            prev_cand_avg_valid = prev_prog.get("mean_candidate_reward_valid_running", None)
            prev_uplift_valid = prev_prog.get("mean_best_minus_avg_reward_valid_running", None)
            prev_uplift_valid_done = prev_prog.get("uplift_valid_done", None)
            if (
                prev_cand_avg_valid is not None
                and prev_uplift_valid is not None
                and prev_uplift_valid_done is not None
            ):
                uplift_valid_n = int(prev_uplift_valid_done)
                cand_avg_valid_sum = float(prev_cand_avg_valid) * float(uplift_valid_n)
                uplift_valid_sum = float(prev_uplift_valid) * float(uplift_valid_n)
    except Exception:
        running_sum = 0.0
        running_n = 0

        uplift_sum = 0.0
        uplift_n = 0
        cand_avg_sum = 0.0
        cand_std_sum = 0.0
        cand_valid_frac_sum = 0.0

        uplift_valid_sum = 0.0
        uplift_valid_n = 0
        cand_avg_valid_sum = 0.0

    save_progress(
        progress_path,
        done=len(done_ids),
        total=shard_total,
        last_example_id=prev_prog.get("last_example_id"),
        last_board_id=prev_prog.get("last_board_id"),
        mean_reward=(running_sum / running_n) if running_n else None,
        extra={
            "status": "running",
            "n_candidates": int(cfg["decoding"]["n_candidates"]),
            "max_resamples": int(cfg["decoding"]["max_resamples"]),
            "batch_size": int(batch_size),
            "shard_id": int(shard_id),
            "num_shards": int(num_shards),
            "include_raw_texts": False,

            # best-of-N vs avg candidate stats (running)
            "uplift_done": int(uplift_n),
            "mean_candidate_reward_running": (cand_avg_sum / uplift_n) if uplift_n else None,
            "mean_best_minus_avg_reward_running": (uplift_sum / uplift_n) if uplift_n else None,
            "candidate_reward_std_running": (cand_std_sum / uplift_n) if uplift_n else None,
            "candidate_valid_frac_running": (cand_valid_frac_sum / uplift_n) if uplift_n else None,

            # valid-only variant
            "uplift_valid_done": int(uplift_valid_n),
            "mean_candidate_reward_valid_running": (cand_avg_valid_sum / uplift_valid_n) if uplift_valid_n else None,
            "mean_best_minus_avg_reward_valid_running": (uplift_valid_sum / uplift_valid_n) if uplift_valid_n else None,
        },
    )

    # Models
    gen = make_text_generator(cfg["models"]["spymaster_model_id"], cfg)
    spymaster = gen
    guesser = gen

    # Embedder (OPTIONAL via constraints.enable_directness_check)
    use_embed = bool(cfg.get("constraints", {}).get("enable_directness_check", True))
    embedder = None
    if use_embed:
        device = "cuda" if __import__("torch").cuda.is_available() else "cpu"
        embedder = Embedder(cfg["models"]["embedding_model_id"], device=device)

    n_candidates = int(cfg["decoding"]["n_candidates"])
    progress_every = int(cfg.get("decoding", {}).get("progress_every", 50))

    # -------------------------
    # Buffered writer
    # -------------------------
    raw_path.parent.mkdir(parents=True, exist_ok=True)
    f_raw = open(raw_path, "a", encoding="utf-8")
    writes_since_flush = 0
    FLUSH_EVERY = int(cfg.get("decoding", {}).get("flush_every", 10))

    # NOTE: To keep files small, we do NOT store raw texts for each candidate by default.
    INCLUDE_RAW_TEXTS = False

    try:
        # If everything is already done, exit quickly but still mark progress
        if not boards and len(done_ids) >= shard_total:
            print(f"[shard {shard_id}/{num_shards}] Nothing to do: done={len(done_ids)}/{shard_total}")
        else:
            for start in range(0, len(boards), max(1, batch_size)):
                batch = boards[start: start + max(1, batch_size)]

                # With the resume fix, batch should contain only not-yet-done examples.
                bests, metas, all_cands = run_turns_batched(
                    batch,
                    spymaster,
                    guesser,
                    embedder,  # may be None if directness check disabled
                    cfg,
                    n_candidates=n_candidates,
                    return_candidates=True,  # <-- variance logging
                )

                for b, best, meta, cands in zip(batch, bests, metas, all_cands):
                    example_id = str(b["board_id"])
                    # Safety check (should rarely hit after filtering)
                    if example_id in done_ids:
                        continue

                    revealed = [False] * len(b["board_words"])
                    msgs = build_spymaster_messages(b["board_words"], b["labels"], revealed, cfg)

                    # Centralized prompt rendering (no duplicated chat-template/thinking logic)
                    prompt = render_prompt(spymaster, msgs, cfg, role="spymaster")

                    # Consolidated think parsing (shared util)
                    think_block = extract_think_block(best.raw_spymaster_text)

                    completion = ""
                    if think_block:
                        completion += think_block + "\n"
                    completion += f"CLUE: {best.clue}\nNUM: {best.num}\n"

                    # Best candidate index (object identity should match; fallback to value match)
                    best_idx = None
                    for j, c in enumerate(cands):
                        if c is best:
                            best_idx = j
                            break
                    if best_idx is None:
                        for j, c in enumerate(cands):
                            if (
                                c.clue == best.clue
                                and int(c.num) == int(best.num)
                                and float(c.reward) == float(best.reward)
                                and bool(c.valid) == bool(best.valid)
                            ):
                                best_idx = j
                                break

                    # --- best-of-N uplift stats for this example ---
                    cand_rewards = [float(c.reward) for c in cands] if cands else []
                    avg_all = float(np.mean(cand_rewards)) if cand_rewards else 0.0
                    std_all = float(np.std(cand_rewards)) if cand_rewards else 0.0
                    uplift_all = float(best.reward) - float(avg_all)

                    valid_rewards = [float(c.reward) for c in cands if bool(getattr(c, "valid", False))] if cands else []
                    avg_valid = float(np.mean(valid_rewards)) if valid_rewards else None
                    uplift_valid = (float(best.reward) - float(avg_valid)) if avg_valid is not None else None

                    valid_frac = (
                        float(np.mean([1.0 if bool(getattr(c, "valid", False)) else 0.0 for c in cands]))
                        if cands else 0.0
                    )

                    cand_payload: List[Dict[str, Any]] = []
                    for c in cands:
                        cd: Dict[str, Any] = {
                            "clue": c.clue,
                            "num": int(c.num),
                            "valid": bool(c.valid),
                            "rejection_reason": c.rejection_reason,
                            "directness": float(c.directness),
                            "reward": float(c.reward),
                            "stats": c.stats,
                            "guess_words": c.guess_words,
                        }
                        if INCLUDE_RAW_TEXTS:
                            cd["raw_spymaster_text"] = c.raw_spymaster_text
                            cd["raw_guesser_text"] = c.raw_guesser_text
                        cand_payload.append(cd)

                    rec = {
                        "example_id": example_id,
                        "board_id": str(b["board_id"]),
                        "prompt": prompt,
                        "completion": completion,
                        "reward": float(best.reward),
                        "stats": {**best.stats, "directness": float(best.directness)},
                        "clue_meta": {
                            "clue": best.clue,
                            "num": int(best.num),
                            "valid": bool(best.valid),
                            "rejected_candidates": int(meta["rejected_total"]),
                            "rejection_counts": meta["rejection_counts"],
                        },
                        "debug": {"guess_words": best.guess_words},
                        # variance logging
                        "best_candidate_idx": best_idx,
                        "candidates": cand_payload,

                        # per-example best-of-N uplift fields
                        "candidate_reward_mean": float(avg_all),
                        "candidate_reward_std": float(std_all),
                        "candidate_valid_frac": float(valid_frac),
                        "best_minus_avg_reward": float(uplift_all),
                        "candidate_reward_mean_valid": float(avg_valid) if avg_valid is not None else None,
                        "best_minus_avg_reward_valid": float(uplift_valid) if uplift_valid is not None else None,
                    }

                    f_raw.write(json.dumps(rec, ensure_ascii=False) + "\n")
                    writes_since_flush += 1
                    if writes_since_flush >= FLUSH_EVERY:
                        f_raw.flush()
                        writes_since_flush = 0

                    done_ids.add(example_id)

                    # Update running best reward mean
                    running_sum += float(best.reward)
                    running_n += 1

                    # Update running uplift stats
                    cand_avg_sum += float(avg_all)
                    cand_std_sum += float(std_all)
                    cand_valid_frac_sum += float(valid_frac)
                    uplift_sum += float(uplift_all)
                    uplift_n += 1

                    if avg_valid is not None and uplift_valid is not None:
                        cand_avg_valid_sum += float(avg_valid)
                        uplift_valid_sum += float(uplift_valid)
                        uplift_valid_n += 1

                    if len(done_ids) % progress_every == 0:
                        mean_r = (running_sum / running_n) if running_n else None

                        mean_cand = (cand_avg_sum / uplift_n) if uplift_n else None
                        mean_uplift = (uplift_sum / uplift_n) if uplift_n else None
                        mean_std = (cand_std_sum / uplift_n) if uplift_n else None
                        mean_vfrac = (cand_valid_frac_sum / uplift_n) if uplift_n else None

                        mean_cand_valid = (cand_avg_valid_sum / uplift_valid_n) if uplift_valid_n else None
                        mean_uplift_valid = (uplift_valid_sum / uplift_valid_n) if uplift_valid_n else None

                        save_progress(
                            progress_path,
                            done=len(done_ids),
                            total=shard_total,  # keep per-shard total stable across resume
                            last_example_id=rec["example_id"],
                            last_board_id=rec["board_id"],
                            mean_reward=mean_r,
                            extra={
                                "n_candidates": int(n_candidates),
                                "max_resamples": int(cfg["decoding"]["max_resamples"]),
                                "batch_size": int(batch_size),
                                "shard_id": int(shard_id),
                                "num_shards": int(num_shards),
                                "include_raw_texts": bool(INCLUDE_RAW_TEXTS),
                                "status": "running",

                                # best-of-N vs avg stats (running)
                                "uplift_done": int(uplift_n),
                                "mean_candidate_reward_running": mean_cand,
                                "mean_best_minus_avg_reward_running": mean_uplift,
                                "candidate_reward_std_running": mean_std,
                                "candidate_valid_frac_running": mean_vfrac,

                                # valid-only variant
                                "uplift_valid_done": int(uplift_valid_n),
                                "mean_candidate_reward_valid_running": mean_cand_valid,
                                "mean_best_minus_avg_reward_valid_running": mean_uplift_valid,
                            },
                        )

        f_raw.flush()

    finally:
        try:
            f_raw.flush()
        except Exception:
            pass
        try:
            f_raw.close()
        except Exception:
            pass

    # Child workers stop here; master will merge + filter.
    if is_child_process() and num_shards > 1:
        print(f"[shard {shard_id}/{num_shards}] done; skipping global filter/merge (master will handle).")

        mean_r = (running_sum / running_n) if running_n else None

        mean_cand = (cand_avg_sum / uplift_n) if uplift_n else None
        mean_uplift = (uplift_sum / uplift_n) if uplift_n else None
        mean_std = (cand_std_sum / uplift_n) if uplift_n else None
        mean_vfrac = (cand_valid_frac_sum / uplift_n) if uplift_n else None

        mean_cand_valid = (cand_avg_valid_sum / uplift_valid_n) if uplift_valid_n else None
        mean_uplift_valid = (uplift_valid_sum / uplift_valid_n) if uplift_valid_n else None

        save_progress(
            progress_path,
            done=len(done_ids),
            total=shard_total,
            last_example_id=prev_prog.get("last_example_id"),
            last_board_id=prev_prog.get("last_board_id"),
            mean_reward=mean_r,
            extra={
                "status": "finished_shard",

                "uplift_done": int(uplift_n),
                "mean_candidate_reward_running": mean_cand,
                "mean_best_minus_avg_reward_running": mean_uplift,
                "candidate_reward_std_running": mean_std,
                "candidate_valid_frac_running": mean_vfrac,

                "uplift_valid_done": int(uplift_valid_n),
                "mean_candidate_reward_valid_running": mean_cand_valid,
                "mean_best_minus_avg_reward_valid_running": mean_uplift_valid,
            },
        )
        return

    # Single-process mode: filter and write final file
    raw_for_filter = read_jsonl(raw_path) if raw_path.exists() else []
    filtered = filter_examples(raw_for_filter, cfg)
    write_jsonl(cfg["paths"]["sft_turns_path"], filtered)
    print(f"Filtered {len(filtered)}/{len(raw_for_filter)} -> {cfg['paths']['sft_turns_path']}")

    mean_r = (running_sum / running_n) if running_n else None

    mean_cand = (cand_avg_sum / uplift_n) if uplift_n else None
    mean_uplift = (uplift_sum / uplift_n) if uplift_n else None
    mean_std = (cand_std_sum / uplift_n) if uplift_n else None
    mean_vfrac = (cand_valid_frac_sum / uplift_n) if uplift_n else None

    mean_cand_valid = (cand_avg_valid_sum / uplift_valid_n) if uplift_valid_n else None
    mean_uplift_valid = (uplift_valid_sum / uplift_valid_n) if uplift_valid_n else None

    save_progress(
        progress_path,
        done=len(done_ids),
        total=shard_total,
        last_example_id=None,
        last_board_id=None,
        mean_reward=mean_r,
        extra={
            "status": "finished",

            "uplift_done": int(uplift_n),
            "mean_candidate_reward_running": mean_cand,
            "mean_best_minus_avg_reward_running": mean_uplift,
            "candidate_reward_std_running": mean_std,
            "candidate_valid_frac_running": mean_vfrac,

            "uplift_valid_done": int(uplift_valid_n),
            "mean_candidate_reward_valid_running": mean_cand_valid,
            "mean_best_minus_avg_reward_valid_running": mean_uplift_valid,
        },
    )


if __name__ == "__main__":
    main()


================================================
FILE: src/guesser_prompt.py
================================================
from typing import Dict, List, Any

def build_guesser_messages(
    board_words: List[str],
    revealed_mask: List[bool],
    clue: str,
    num: int,
    cfg: Dict[str, Any],
) -> List[Dict[str, str]]:
    system = cfg.get("qwen", {}).get("system_guesser", "You are the Guesser in a Codenames-like game.")
    visible = [w for w, rev in zip(board_words, revealed_mask) if not rev]

    user = f"""You only see the board words and the clue.
 
 BOARD WORDS:
 {", ".join(visible)}

CLUE: {clue}
NUM: {num}

Return at least {num} guesses if possible.

OUTPUT FORMAT (exactly):
GUESSES: word1, word2, word3
"""
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]



================================================
FILE: src/io_utils.py
================================================
# src/io_utils.py
from __future__ import annotations

from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Optional

from .utils import read_jsonl


def shard_tag(shard_id: int, num_shards: int) -> str:
    return f"shard{shard_id:02d}-of-{num_shards:02d}"


def shard_path_insert_before_suffix(path: str | Path, shard_id: int, num_shards: int) -> Path:
    """
    per_board.jsonl -> per_board.shard00-of-02.jsonl
    (matches eval.py's existing convention)
    """
    p = Path(path)
    tag = shard_tag(shard_id, num_shards)
    if p.suffix:
        return p.with_name(f"{p.stem}.{tag}{p.suffix}")
    return p.with_name(f"{p.name}.{tag}")


def shard_path_append_to_suffix(path: str | Path, shard_id: int, num_shards: int) -> Path:
    """
    foo.jsonl -> foo.jsonl.shard00-of-02
    (matches generate_sft_data.py's existing convention)
    """
    p = Path(path)
    tag = shard_tag(shard_id, num_shards)
    if p.suffix:
        return p.with_suffix(p.suffix + f".{tag}")
    return p.with_name(f"{p.name}.{tag}")


def merge_jsonl_shards(
    shard_paths: Iterable[str | Path],
    *,
    sort_key: Optional[Callable[[Dict[str, Any]], Any]] = None,
) -> List[Dict[str, Any]]:
    combined: List[Dict[str, Any]] = []
    for sp in shard_paths:
        p = Path(sp)
        if p.exists():
            combined.extend(read_jsonl(p))
    if sort_key is not None:
        combined.sort(key=sort_key)
    return combined


================================================
FILE: src/make_boards.py
================================================
from __future__ import annotations

import argparse
import hashlib
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np

from .utils import load_yaml, set_global_seed, write_jsonl, ensure_dir


def load_vocab(path: str | Path) -> List[str]:
    vocab: List[str] = []
    seen = set()
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            w = line.strip()
            if not w:
                continue
            wl = w.lower()
            if wl in seen:
                continue
            seen.add(wl)
            vocab.append(w)
    return vocab


def sample_board(vocab: List[str], rng: np.random.Generator, board_size: int, allow_duplicates: bool) -> List[str]:
    if allow_duplicates:
        idx = rng.integers(0, len(vocab), size=board_size).tolist()
        return [vocab[i] for i in idx]
    else:
        if len(vocab) < board_size:
            raise ValueError(f"Vocab too small: {len(vocab)} < {board_size}")
        idx = rng.choice(len(vocab), size=board_size, replace=False).tolist()
        return [vocab[i] for i in idx]


def assign_labels(
    rng: np.random.Generator,
    board_size: int,
    n_team: int,
    n_opp: int,
    n_neu: int,
    n_assassin: int,
) -> List[str]:
    labels = (["TEAM"] * n_team) + (["OPP"] * n_opp) + (["NEU"] * n_neu) + (["ASSASSIN"] * n_assassin)
    if len(labels) != board_size:
        raise ValueError("Label counts do not sum to board_size.")
    rng.shuffle(labels)
    return labels


def board_hash(board_words: List[str]) -> str:
    s = "||".join(sorted([w.lower() for w in board_words]))
    return hashlib.sha256(s.encode("utf-8")).hexdigest()[:16]


def make_split(
    split_name: str,
    n_boards: int,
    vocab: List[str],
    cfg: Dict[str, Any],
    seed: int,
    global_seen_hashes: set[str],
) -> List[Dict[str, Any]]:
    bcfg = cfg["boards"]
    rng = np.random.default_rng(seed)
    records: List[Dict[str, Any]] = []
    local_seen = set()

    for i in range(n_boards):
        # try until unique (if requested)
        while True:
            board_seed = int(rng.integers(0, 2**31 - 1))
            brng = np.random.default_rng(board_seed)
            words = sample_board(vocab, brng, int(bcfg["board_size"]), bool(bcfg["allow_duplicates"]))
            h = board_hash(words)

            if bcfg.get("avoid_repeat_boards_within_split", True) and h in local_seen:
                continue
            if bcfg.get("avoid_repeat_boards_across_splits", True) and h in global_seen_hashes:
                continue

            local_seen.add(h)
            global_seen_hashes.add(h)
            labels = assign_labels(
                brng,
                int(bcfg["board_size"]),
                int(bcfg["n_team"]),
                int(bcfg["n_opp"]),
                int(bcfg["n_neu"]),
                int(bcfg["n_assassin"]),
            )
            rec = {
                "board_id": f"{split_name}_{i+1:06d}",
                "board_words": words,
                "labels": labels,
                "seed": board_seed,
                "board_hash": h,
            }
            records.append(rec)
            break

    return records


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()

    cfg = load_yaml(args.config)
    vocab = load_vocab(cfg["paths"]["vocab_path"])

    set_global_seed(0)

    seen_hashes: set[str] = set()
    train = make_split(
        "train",
        int(cfg["boards"]["n_train_boards"]),
        vocab,
        cfg,
        int(cfg["boards"]["seed_train"]),
        seen_hashes,
    )
    eval_ = make_split(
        "eval",
        int(cfg["boards"]["n_eval_boards"]),
        vocab,
        cfg,
        int(cfg["boards"]["seed_eval"]),
        seen_hashes,
    )

    write_jsonl(cfg["paths"]["boards_train_path"], train)
    write_jsonl(cfg["paths"]["boards_eval_path"], eval_)

    print(f"Wrote {len(train)} train boards -> {cfg['paths']['boards_train_path']}")
    print(f"Wrote {len(eval_)} eval boards  -> {cfg['paths']['boards_eval_path']}")


if __name__ == "__main__":
    main()


================================================
FILE: src/metrics.py
================================================
from __future__ import annotations

from typing import Any, Dict, List, Tuple
import math
import numpy as np


def compute_clue_diversity(clues: List[str]) -> Dict[str, Any]:
    clues = [c.strip().lower() for c in clues if c and c.strip()]
    n = len(clues)
    if n == 0:
        return {"n": 0, "distinct": 0, "distinct_rate": 0.0, "entropy": 0.0}

    from collections import Counter
    ctr = Counter(clues)
    distinct = len(ctr)
    probs = np.array([v / n for v in ctr.values()], dtype=np.float64)
    entropy = float(-(probs * np.log(probs + 1e-12)).sum())
    return {"n": n, "distinct": distinct, "distinct_rate": float(distinct / n), "entropy": entropy}


def bootstrap_ci(values: List[float], n: int = 1000, alpha: float = 0.05, seed: int = 0) -> Tuple[float, float]:
    rng = np.random.default_rng(seed)
    arr = np.array(values, dtype=np.float64)
    if len(arr) == 0:
        return (float("nan"), float("nan"))
    means = []
    for _ in range(n):
        samp = rng.choice(arr, size=len(arr), replace=True)
        means.append(float(np.mean(samp)))
    lo = float(np.quantile(means, alpha / 2))
    hi = float(np.quantile(means, 1 - alpha / 2))
    return lo, hi


def aggregate(per_board_records: List[Dict[str, Any]]) -> Dict[str, Any]:
    rewards = [float(r["reward"]) for r in per_board_records]
    n_team = [int(r["stats"].get("n_team", 0)) for r in per_board_records]
    n_opp = [int(r["stats"].get("n_opp", 0)) for r in per_board_records]
    n_neu = [int(r["stats"].get("n_neu", 0)) for r in per_board_records]
    assassin = [int(r["stats"].get("assassin", 0)) for r in per_board_records]
    directness = [float(r["stats"].get("directness", 0.0)) for r in per_board_records]
    clues = [r.get("clue", "") for r in per_board_records]

    assassin_rate = float(np.mean([1.0 if a > 0 else 0.0 for a in assassin])) if assassin else 0.0

    out: Dict[str, Any] = {
        "n_boards": len(per_board_records),
        "reward_mean": float(np.mean(rewards)) if rewards else 0.0,
        "reward_median": float(np.median(rewards)) if rewards else 0.0,
        "reward_ci95": bootstrap_ci(rewards, n=500, seed=1) if rewards else (0.0, 0.0),
        "team_mean": float(np.mean(n_team)) if n_team else 0.0,
        "opp_mean": float(np.mean(n_opp)) if n_opp else 0.0,
        "neu_mean": float(np.mean(n_neu)) if n_neu else 0.0,
        "assassin_rate": assassin_rate,
        "directness_mean": float(np.mean(directness)) if directness else 0.0,
        "directness_p90": float(np.quantile(directness, 0.9)) if directness else 0.0,
        "clue_diversity": compute_clue_diversity(clues),
    }
    return out


================================================
FILE: src/model_wrappers.py
================================================
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Protocol, runtime_checkable

import numpy as np


# -------------------------
# One canonical chat-template helper
# -------------------------

def apply_chat_template(
    tokenizer: Any,
    messages: List[Dict[str, str]],
    *,
    add_generation_prompt: bool = True,
    enable_thinking: bool = True,
) -> str:
    """
    Centralized compatibility wrapper for tokenizer.apply_chat_template signature drift.

    Some tokenizers (e.g., Qwen3) accept enable_thinking=...
    Others do not. This wrapper tries with enable_thinking and falls back cleanly.
    """
    try:
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=add_generation_prompt,
            enable_thinking=enable_thinking,
        )
    except TypeError:
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=add_generation_prompt,
        )


@dataclass
class GenerationConfig:
    temperature: float
    top_p: float
    max_new_tokens: int
    top_k: int | None = None


@runtime_checkable
class TextGenerator(Protocol):
    model_id: str

    def format_chat(
        self,
        messages: List[Dict[str, str]],
        *,
        add_generation_prompt: bool = True,
        enable_thinking: bool = True,
    ) -> str: ...

    def generate(
        self,
        prompt_or_messages: str | List[Dict[str, str]],
        gen_cfg: GenerationConfig,
        seed: Optional[int] = None,
        *,
        use_chat_template: bool = False,
        enable_thinking: bool = True,
    ) -> str: ...


class VLLMTextGenerator:
    """
    Thin wrapper around vLLM offline inference.
    """

    def __init__(
        self,
        model_id: str,
        *,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.90,
        max_model_len: int | None = None,
        dtype: str = "auto",
        trust_remote_code: bool = True,
        enable_lora: bool = False,
        max_lora_rank: int = 64,
    ):
        from transformers import AutoTokenizer
        from vllm import LLM

        self.model_id = model_id
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            use_fast=True,
            trust_remote_code=trust_remote_code,
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # vLLM engine (offline)
        self.llm = LLM(
            model=model_id,
            tokenizer=model_id,
            tensor_parallel_size=int(tensor_parallel_size),
            gpu_memory_utilization=float(gpu_memory_utilization),
            max_model_len=max_model_len,
            dtype=dtype,
            trust_remote_code=trust_remote_code,
            enable_lora=bool(enable_lora),
            max_lora_rank=int(max_lora_rank),
        )

        self._lora_request = None  # set by load_lora_on_generator()

    def format_chat(
        self,
        messages: List[Dict[str, str]],
        *,
        add_generation_prompt: bool = True,
        enable_thinking: bool = True,
    ) -> str:
        return apply_chat_template(
            self.tokenizer,
            messages,
            add_generation_prompt=add_generation_prompt,
            enable_thinking=enable_thinking,
        )

    def generate(
        self,
        prompt_or_messages: str | List[Dict[str, str]],
        gen_cfg: GenerationConfig,
        seed: Optional[int] = None,
        *,
        use_chat_template: bool = False,
        enable_thinking: bool = True,
    ) -> str:
        from vllm import SamplingParams

        if use_chat_template:
            assert isinstance(prompt_or_messages, list)
            prompt = self.format_chat(
                prompt_or_messages,
                add_generation_prompt=True,
                enable_thinking=enable_thinking,
            )
        else:
            assert isinstance(prompt_or_messages, str)
            prompt = prompt_or_messages

        sp = SamplingParams(
            temperature=float(gen_cfg.temperature),
            top_p=float(gen_cfg.top_p),
            top_k=int(gen_cfg.top_k) if gen_cfg.top_k is not None else -1,
            max_tokens=int(gen_cfg.max_new_tokens),
            seed=seed,
        )

        outs = self.llm.generate([prompt], sp, lora_request=self._lora_request)
        return outs[0].outputs[0].text

    def generate_batch(
        self,
        prompts: List[str],
        gen_cfg: GenerationConfig,
        seeds: Optional[List[Optional[int]]] = None,
    ) -> List[str]:
        from vllm import SamplingParams

        if seeds is None:
            seeds = [None] * len(prompts)
        assert len(seeds) == len(prompts)

        sps = [
            SamplingParams(
                temperature=float(gen_cfg.temperature),
                top_p=float(gen_cfg.top_p),
                top_k=int(gen_cfg.top_k) if gen_cfg.top_k is not None else -1,
                max_tokens=int(gen_cfg.max_new_tokens),
                seed=sd,
            )
            for sd in seeds
        ]

        outs = self.llm.generate(prompts, sps, lora_request=self._lora_request)
        return [o.outputs[0].text for o in outs]


class HFTextGenerator:
    """
    Thin wrapper around transformers generation for reproducible calls.
    """

    def __init__(self, model_id: str, device_map: str = "auto", torch_dtype: str | None = None):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        self.model_id = model_id

        dtype = None
        if torch_dtype:
            dtype = getattr(torch, torch_dtype)

        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map=device_map,
            torch_dtype=dtype,
        )
        self.model.eval()

    def format_chat(
        self,
        messages: List[Dict[str, str]],
        *,
        add_generation_prompt: bool = True,
        enable_thinking: bool = True,
    ) -> str:
        return apply_chat_template(
            self.tokenizer,
            messages,
            add_generation_prompt=add_generation_prompt,
            enable_thinking=enable_thinking,
        )

    def generate(
        self,
        prompt_or_messages: str | List[Dict[str, str]],
        gen_cfg: GenerationConfig,
        seed: Optional[int] = None,
        *,
        use_chat_template: bool = False,
        enable_thinking: bool = True,
    ) -> str:
        import torch

        if seed is not None:
            torch.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)

        if use_chat_template:
            assert isinstance(prompt_or_messages, list)
            text = self.format_chat(prompt_or_messages, add_generation_prompt=True, enable_thinking=enable_thinking)
        else:
            assert isinstance(prompt_or_messages, str)
            text = prompt_or_messages

        inputs = self.tokenizer(text, return_tensors="pt", padding=False)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        do_sample = gen_cfg.temperature is not None and gen_cfg.temperature > 1e-6
        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                do_sample=do_sample,
                temperature=float(gen_cfg.temperature) if do_sample else None,
                top_p=float(gen_cfg.top_p) if do_sample else None,
                top_k=int(gen_cfg.top_k) if (do_sample and gen_cfg.top_k is not None) else None,
                max_new_tokens=int(gen_cfg.max_new_tokens),
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        prompt_len = inputs["input_ids"].shape[1]
        gen_ids = out[0][prompt_len:]
        return self.tokenizer.decode(gen_ids, skip_special_tokens=True)


def load_lora_on_generator(base_generator: TextGenerator, adapter_dir: str) -> TextGenerator:
    """
    HF path: wraps with PeftModel
    vLLM path: sets a LoRARequest that will be used on each generate() call
    """
    if isinstance(base_generator, HFTextGenerator):
        from peft import PeftModel

        base_generator.model = PeftModel.from_pretrained(base_generator.model, adapter_dir)
        base_generator.model.eval()
        return base_generator

    if isinstance(base_generator, VLLMTextGenerator):
        from vllm.lora.request import LoRARequest

        lora_int_id = abs(hash(adapter_dir)) % (2**31)
        base_generator._lora_request = LoRARequest("codenames-lora", int(lora_int_id), adapter_dir)
        return base_generator

    raise TypeError(f"Unsupported generator type: {type(base_generator)}")


def make_text_generator(model_id: str, cfg: Dict[str, Any]) -> TextGenerator:
    backend = cfg.get("inference", {}).get("backend", "hf")
    if backend == "vllm":
        vcfg = cfg.get("inference", {}).get("vllm", {})
        return VLLMTextGenerator(model_id, **vcfg)
    return HFTextGenerator(model_id, device_map="auto")


class Embedder:
    """
    Embeds single tokens/words/short strings with caching.
    Uses sentence-transformers if available; otherwise uses a HF encoder with mean pooling.
    """

    def __init__(self, model_id: str, device: str = "cpu"):
        self.model_id = model_id
        self.device = device
        self._cache: Dict[str, np.ndarray] = {}

        self._mode = None
        self._st_model = None
        self._hf_tok = None
        self._hf_model = None

        try:
            from sentence_transformers import SentenceTransformer  # type: ignore

            self._st_model = SentenceTransformer(model_id, device=device)
            self._mode = "st"
        except Exception:
            self._mode = "hf"
            from transformers import AutoModel, AutoTokenizer

            self._hf_tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
            self._hf_model = AutoModel.from_pretrained(model_id)
            self._hf_model.to(device)
            self._hf_model.eval()

    def embed(self, text: str) -> np.ndarray:
        key = text.strip().lower()
        if key in self._cache:
            return self._cache[key]

        if self._mode == "st":
            vec = np.asarray(self._st_model.encode([text], normalize_embeddings=True)[0], dtype=np.float32)
        else:
            import torch

            assert self._hf_tok is not None and self._hf_model is not None
            with torch.no_grad():
                toks = self._hf_tok([text], return_tensors="pt", padding=True, truncation=True).to(self.device)
                out = self._hf_model(**toks)
                last = out.last_hidden_state
                mask = toks["attention_mask"].unsqueeze(-1)
                pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)
                vec = pooled[0].detach().cpu().numpy().astype(np.float32)
                n = np.linalg.norm(vec) + 1e-12
                vec = vec / n

        self._cache[key] = vec
        return vec

    @staticmethod
    def cosine(a: np.ndarray, b: np.ndarray) -> float:
        return float(np.dot(a, b) / ((np.linalg.norm(a) + 1e-12) * (np.linalg.norm(b) + 1e-12)))


================================================
FILE: src/mp_utils.py
================================================
# src/mp_utils.py
from __future__ import annotations
import os, sys, subprocess
from dataclasses import dataclass
from typing import List, Tuple, Optional

ENV_CHILD = "CODENAMES_CHILD"
ENV_SHARD = "CODENAMES_SHARD_ID"
ENV_NSHARDS = "CODENAMES_NUM_SHARDS"

def parse_visible_gpus() -> List[str]:
    cvd = os.environ.get("CUDA_VISIBLE_DEVICES", "").strip()
    if cvd:
        return [x.strip() for x in cvd.split(",") if x.strip()]
    try:
        import torch
        n = torch.cuda.device_count()
    except Exception:
        n = 0
    return [str(i) for i in range(n)]

def is_child_process() -> bool:
    return os.environ.get(ENV_CHILD, "") == "1"

def child_shard_info() -> Tuple[int, int]:
    sid = int(os.environ.get(ENV_SHARD, "0"))
    n = int(os.environ.get(ENV_NSHARDS, "1"))
    return sid, n

def launch_children(module: str, argv: List[str], num_procs: int) -> None:
    gpus = parse_visible_gpus()
    if len(gpus) < num_procs:
        raise RuntimeError(f"Need {num_procs} visible GPUs, but only see {len(gpus)}: {gpus}")

    procs: List[subprocess.Popen] = []
    for sid in range(num_procs):
        env = os.environ.copy()
        env[ENV_CHILD] = "1"
        env[ENV_SHARD] = str(sid)
        env[ENV_NSHARDS] = str(num_procs)
        env["CUDA_VISIBLE_DEVICES"] = gpus[sid]
        cmd = [sys.executable, "-m", module, *argv]
        procs.append(subprocess.Popen(cmd, env=env))

    rc = 0
    for p in procs:
        rc = rc or p.wait()
    if rc != 0:
        raise SystemExit(rc)


================================================
FILE: src/prompting.py
================================================
from __future__ import annotations

from typing import Any, Dict, List, Literal, Optional

from .model_wrappers import GenerationConfig, TextGenerator

Role = Literal["spymaster", "guesser", "generic"]


def _use_chat_template(cfg: Dict[str, Any]) -> bool:
    return bool(cfg.get("qwen", {}).get("use_chat_template", False))


def _enable_thinking(cfg: Dict[str, Any], role: Role) -> bool:
    q = cfg.get("qwen", {}) or {}
    if role == "spymaster":
        return bool(q.get("enable_thinking_spymaster", True))
    if role == "guesser":
        return bool(q.get("enable_thinking_guesser", True))
    return bool(q.get("enable_thinking", True))


def render_prompt(generator: Any, messages: List[Dict[str, str]], cfg: Dict[str, Any], *, role: Role) -> str:
    """
    Canonical prompt rendering for both batched & non-batched paths.
    If chat templates are disabled, returns the raw user content.
    Otherwise uses generator.format_chat(...) with role-specific enable_thinking.
    """
    if not _use_chat_template(cfg):
        return messages[-1]["content"]
    return generator.format_chat(
        messages,
        add_generation_prompt=True,
        enable_thinking=_enable_thinking(cfg, role),
    )


def generate_from_messages(
    generator: TextGenerator,
    messages: List[Dict[str, str]],
    gen_cfg: GenerationConfig,
    cfg: Dict[str, Any],
    *,
    role: Role,
    seed: Optional[int] = None,
) -> str:
    """
    Canonical "messages -> generator.generate(...)" wrapper.

    This removes duplicated logic scattered around the codebase:
      - chat_template vs raw string prompt
      - role-specific enable_thinking flags
      - passing the correct flags into generator.generate()
    """
    use_chat = _use_chat_template(cfg)
    if use_chat:
        return generator.generate(
            messages,
            gen_cfg,
            seed=seed,
            use_chat_template=True,
            enable_thinking=_enable_thinking(cfg, role),
        )
    return generator.generate(
        messages[-1]["content"],
        gen_cfg,
        seed=seed,
        use_chat_template=False,
        enable_thinking=False,  # ignored for raw prompts
    )


================================================
FILE: src/rollout.py
================================================
from __future__ import annotations

import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, overload, Literal

from .gen_cfg import guesser_gen_cfg, spymaster_gen_cfg
from .model_wrappers import Embedder, TextGenerator
from .prompting import render_prompt, generate_from_messages
from .rules import is_valid_clue, normalize_word, score_turn
from .spymaster_prompt import build_spymaster_messages
from .guesser_prompt import build_guesser_messages

# -------------------------
# Parsing helpers
# -------------------------

_CLUE_RE = re.compile(r"CLUE\s*:\s*(.+)", re.IGNORECASE)
_NUM_RE = re.compile(r"NUM\s*:\s*([0-9]+)", re.IGNORECASE)
_GUESSES_RE = re.compile(r"GUESSES\s*:\s*(.+)", re.IGNORECASE)


def parse_spymaster_output(text: str) -> Tuple[Optional[str], Optional[int]]:
    """
    Extract the last CLUE/NUM pair from a completion.
    """
    clues = _CLUE_RE.findall(text)
    nums = _NUM_RE.findall(text)
    clue = clues[-1].strip().splitlines()[0].strip() if clues else None
    num = int(nums[-1]) if nums else None
    return clue, num


def parse_guesser_output(text: str) -> List[str]:
    """
    Extract guesses from "GUESSES: a, b, c" (last match wins),
    otherwise fall back to first non-empty line.
    """
    ms = _GUESSES_RE.findall(text)
    raw = ms[-1] if ms else ""
    if not raw:
        raw = text.strip().splitlines()[0] if text.strip() else ""

    parts = [p.strip() for p in raw.replace("\n", " ").split(",")]
    return [p for p in parts if p]


def map_guesses_to_board(guesses: List[str], board_words: List[str]) -> List[str]:
    """
    Normalize and keep only guesses that match board words exactly after normalization.
    Ensures uniqueness while preserving order.

    Returns normalized board-word strings (consistent with score_turn / labels_by_word keys).
    """
    board_norm = {normalize_word(w) for w in board_words}
    out: List[str] = []
    seen = set()
    for g in guesses:
        gn = normalize_word(g)
        if gn in board_norm and gn not in seen:
            out.append(gn)
            seen.add(gn)
    return out


# -------------------------
# Core data structure
# -------------------------

@dataclass
class CandidateResult:
    clue: str
    num: int
    valid: bool
    rejection_reason: str
    directness: float
    reward: float
    stats: Dict[str, Any]
    guess_words: List[str]
    raw_spymaster_text: str
    raw_guesser_text: str


# -------------------------
# Single-candidate rollout
# -------------------------

def run_one_candidate(
    board_record: Dict[str, Any],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    seed: Optional[int] = None,
) -> CandidateResult:
    board_words = board_record["board_words"]
    labels = board_record["labels"]
    revealed_mask = [False] * len(board_words)

    target_words = [w for w, lab in zip(board_words, labels) if lab == "TEAM"]
    labels_by_word = {normalize_word(w): lab for w, lab in zip(board_words, labels)}

    # --- spymaster
    sp_msgs = build_spymaster_messages(board_words, labels, revealed_mask, cfg)
    sp_gen = spymaster_gen_cfg(cfg)

    sp_text = generate_from_messages(
        spymaster,
        sp_msgs,
        sp_gen,
        cfg,
        role="spymaster",
        seed=seed,
    )

    clue, num = parse_spymaster_output(sp_text)
    if clue is None or num is None:
        return CandidateResult(
            clue=clue or "",
            num=num or 0,
            valid=False,
            rejection_reason="parse_fail",
            directness=0.0,
            reward=0.0,
            stats={"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0},
            guess_words=[],
            raw_spymaster_text=sp_text,
            raw_guesser_text="",
        )

    valid, reason, d = is_valid_clue(clue, board_words, target_words, embedder, cfg)
    if not valid:
        return CandidateResult(
            clue=clue,
            num=int(num),
            valid=False,
            rejection_reason=reason,
            directness=float(d),
            reward=0.0,
            stats={"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0},
            guess_words=[],
            raw_spymaster_text=sp_text,
            raw_guesser_text="",
        )

    # --- guesser
    g_msgs = build_guesser_messages(board_words, revealed_mask, clue, int(num), cfg)
    g_gen = guesser_gen_cfg(cfg)

    g_text = generate_from_messages(
        guesser,
        g_msgs,
        g_gen,
        cfg,
        role="guesser",
        seed=seed,
    )

    guesses_raw = parse_guesser_output(g_text)
    guesses = map_guesses_to_board(guesses_raw, board_words)

    k = max(0, min(int(num), len(guesses)))
    scored = guesses[:k]
    reward, stats = score_turn(scored, labels_by_word, cfg)

    return CandidateResult(
        clue=clue,
        num=int(num),
        valid=True,
        rejection_reason="ok",
        directness=float(d),
        reward=float(reward),
        stats=stats,
        guess_words=scored,
        raw_spymaster_text=sp_text,
        raw_guesser_text=g_text,
    )


def run_turn(
    board_record: Dict[str, Any],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    n_candidates: int = 1,
    seed: Optional[int] = None,
) -> Tuple[List[CandidateResult], Dict[str, Any]]:
    """
    Samples up to n_candidates, with validity-enforced resampling.
    Returns candidates and metadata including rejected counts.
    """
    max_resamples = int(cfg["decoding"]["max_resamples"])
    candidates: List[CandidateResult] = []
    rejection_counts: Dict[str, int] = {}
    total_rejected = 0

    for ci in range(n_candidates):
        got: Optional[CandidateResult] = None
        last_res: Optional[CandidateResult] = None

        for ri in range(max_resamples):
            s = None if seed is None else (seed + ci * 1000 + ri)
            res = run_one_candidate(board_record, spymaster, guesser, embedder, cfg, seed=s)
            last_res = res

            if res.valid:
                got = res
                break

            total_rejected += 1
            rejection_counts[res.rejection_reason] = rejection_counts.get(res.rejection_reason, 0) + 1

        if got is None:
            got = last_res if last_res is not None else CandidateResult(
                clue="",
                num=0,
                valid=False,
                rejection_reason="exhausted",
                directness=0.0,
                reward=0.0,
                stats={"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0},
                guess_words=[],
                raw_spymaster_text="",
                raw_guesser_text="",
            )
        candidates.append(got)

    meta = {"rejected_total": total_rejected, "rejection_counts": rejection_counts}
    return candidates, meta


def select_best_candidate(candidates: List[CandidateResult]) -> CandidateResult:
    """
    Primary: reward
    Secondary: lower directness (more indirect) if tie
    """
    best = candidates[0]
    for c in candidates[1:]:
        if c.reward > best.reward:
            best = c
        elif c.reward == best.reward and c.directness < best.directness:
            best = c
    return best


# -------------------------
# Batched rollout (shared by eval + SFT generation)
# -------------------------

@overload
def run_turns_batched(
    boards_batch: List[Dict[str, Any]],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    *,
    n_candidates: int,
    return_candidates: Literal[False] = False,
) -> Tuple[List[CandidateResult], List[Dict[str, Any]]]: ...


@overload
def run_turns_batched(
    boards_batch: List[Dict[str, Any]],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    *,
    n_candidates: int,
    return_candidates: Literal[True],
) -> Tuple[List[CandidateResult], List[Dict[str, Any]], List[List[CandidateResult]]]: ...


def run_turns_batched(
    boards_batch: List[Dict[str, Any]],
    spymaster: TextGenerator,
    guesser: TextGenerator,
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
    *,
    n_candidates: int,
    return_candidates: bool = False,
) -> Any:
    """
    Returns (default):
      - best CandidateResult per board
      - meta per board: rejected_total + rejection_counts (matching run_turn)

    If return_candidates=True:
      - also returns all candidates per board (length == n_candidates)
        so you can analyze variance across candidates.
    """
    # Sequential fallback if no batch API
    if not (hasattr(spymaster, "generate_batch") and hasattr(guesser, "generate_batch")):
        bests: List[CandidateResult] = []
        metas: List[Dict[str, Any]] = []
        all_cands: List[List[CandidateResult]] = []
        for b in boards_batch:
            seed = int(b.get("seed", 0))
            cands, meta = run_turn(b, spymaster, guesser, embedder, cfg, n_candidates=n_candidates, seed=seed)
            all_cands.append(cands)
            bests.append(select_best_candidate(cands))
            metas.append(meta)
        if return_candidates:
            return bests, metas, all_cands
        return bests, metas

    sp_gen = spymaster_gen_cfg(cfg)
    g_gen = guesser_gen_cfg(cfg)
    max_resamples = int(cfg["decoding"]["max_resamples"])

    # Precompute prompts + bookkeeping per board
    sp_prompts: List[str] = []
    board_words_list: List[List[str]] = []
    target_words_list: List[List[str]] = []
    labels_by_word_list: List[Dict[str, str]] = []
    seeds0: List[int] = []

    for b in boards_batch:
        board_words = b["board_words"]
        labels = b["labels"]
        revealed_mask = [False] * len(board_words)

        sp_msgs = build_spymaster_messages(board_words, labels, revealed_mask, cfg)
        sp_prompts.append(render_prompt(spymaster, sp_msgs, cfg, role="spymaster"))

        target_words = [w for w, lab in zip(board_words, labels) if lab == "TEAM"]
        labels_by_word = {normalize_word(w): lab for w, lab in zip(board_words, labels)}

        board_words_list.append(board_words)
        target_words_list.append(target_words)
        labels_by_word_list.append(labels_by_word)
        seeds0.append(int(b.get("seed", 0)))

    # Storage
    all_candidates_per_board: List[List[CandidateResult]] = [[] for _ in boards_batch]
    rej_counts_per_board: List[Dict[str, int]] = [{} for _ in boards_batch]
    rej_total_per_board: List[int] = [0 for _ in boards_batch]

    for ci in range(n_candidates):
        got: List[Optional[Tuple[str, int, bool, str, float, str, int]]] = [None] * len(boards_batch)
        pending = list(range(len(boards_batch)))
        last_text: Dict[int, str] = {}
        last_seed: Dict[int, int] = {}

        # Batched spymaster resampling
        for ri in range(max_resamples):
            if not pending:
                break

            prompts = [sp_prompts[i] for i in pending]
            seeds = [seeds0[i] + ci * 1000 + ri for i in pending]
            texts = spymaster.generate_batch(prompts, sp_gen, seeds)  # type: ignore[attr-defined]

            for idx_in_list, bi in enumerate(pending):
                txt = texts[idx_in_list]
                s_used = seeds[idx_in_list]
                last_text[bi] = txt
                last_seed[bi] = s_used

                clue, num = parse_spymaster_output(txt)
                if clue is None or num is None:
                    reason = "parse_fail"
                    rej_total_per_board[bi] += 1
                    rej_counts_per_board[bi][reason] = rej_counts_per_board[bi].get(reason, 0) + 1
                    continue

                valid, reason, d = is_valid_clue(clue, board_words_list[bi], target_words_list[bi], embedder, cfg)
                if not valid:
                    rej_total_per_board[bi] += 1
                    rej_counts_per_board[bi][reason] = rej_counts_per_board[bi].get(reason, 0) + 1
                    continue

                got[bi] = (clue, int(num), True, "ok", float(d), txt, s_used)

            pending = [bi for bi in pending if got[bi] is None]

        # Fill any missing with last attempt (invalid)
        for bi in range(len(boards_batch)):
            if got[bi] is None:
                txt = last_text.get(bi, "")
                s_used = last_seed.get(bi, seeds0[bi] + ci * 1000)
                clue, num = parse_spymaster_output(txt)
                got[bi] = (
                    clue or "",
                    int(num) if num is not None else 0,
                    False,
                    "exhausted",
                    0.0,
                    txt,
                    s_used,
                )

        # Batched guesser for valid ones only
        valid_bis = [bi for bi in range(len(boards_batch)) if got[bi][2]]  # type: ignore[index]
        g_prompts: List[str] = []
        g_seeds: List[int] = []

        for bi in valid_bis:
            clue, num, _, _, _, _, s_used = got[bi]  # type: ignore[misc]
            revealed_mask = [False] * len(board_words_list[bi])
            g_msgs = build_guesser_messages(board_words_list[bi], revealed_mask, clue, int(num), cfg)
            g_prompts.append(render_prompt(guesser, g_msgs, cfg, role="guesser"))
            g_seeds.append(int(s_used))

        g_texts = guesser.generate_batch(g_prompts, g_gen, g_seeds) if g_prompts else []  # type: ignore[attr-defined]

        gi = 0
        for bi in range(len(boards_batch)):
            clue, num, is_valid, reason, d, sp_txt, _ = got[bi]  # type: ignore[misc]

            if not is_valid:
                all_candidates_per_board[bi].append(
                    CandidateResult(
                        clue=clue,
                        num=int(num),
                        valid=False,
                        rejection_reason=reason,
                        directness=float(d),
                        reward=0.0,
                        stats={"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0},
                        guess_words=[],
                        raw_spymaster_text=sp_txt,
                        raw_guesser_text="",
                    )
                )
                continue

            g_txt = g_texts[gi]
            gi += 1

            guesses_raw = parse_guesser_output(g_txt)
            guesses = map_guesses_to_board(guesses_raw, board_words_list[bi])

            k = max(0, min(int(num), len(guesses)))
            scored = guesses[:k]
            reward, stats = score_turn(scored, labels_by_word_list[bi], cfg)

            all_candidates_per_board[bi].append(
                CandidateResult(
                    clue=clue,
                    num=int(num),
                    valid=True,
                    rejection_reason="ok",
                    directness=float(d),
                    reward=float(reward),
                    stats=stats,
                    guess_words=scored,
                    raw_spymaster_text=sp_txt,
                    raw_guesser_text=g_txt,
                )
            )

    bests = [select_best_candidate(cands) for cands in all_candidates_per_board]
    metas = [
        {"rejected_total": rej_total_per_board[i], "rejection_counts": rej_counts_per_board[i]}
        for i in range(len(boards_batch))
    ]

    if return_candidates:
        return bests, metas, all_candidates_per_board
    return bests, metas


================================================
FILE: src/rules.py
================================================
from __future__ import annotations

import re
from typing import Dict, List, Tuple, Any, Optional

from .model_wrappers import Embedder


_WORD_RE = re.compile(r"^[A-Za-z][A-Za-z0-9_-]*$")


def normalize_word(s: str) -> str:
    return s.strip().lower()


def is_single_word(s: str) -> bool:
    s = s.strip()
    if not s:
        return False
    # “single word” in the practical sense: no whitespace
    return len(s.split()) == 1


def violates_board_overlap(clue: str, board_words: List[str]) -> bool:
    c = normalize_word(clue)
    bw = {normalize_word(w) for w in board_words}
    return c in bw


def violates_substring_ban(clue: str, board_words: List[str]) -> bool:
    c = normalize_word(clue)
    for w in board_words:
        ww = normalize_word(w)
        if not c or not ww:
            continue
        if c in ww or ww in c:
            return True
    return False


def looks_like_word_token(clue: str) -> bool:
    # avoid punctuation / multi-token-ish outputs
    c = clue.strip()
    return bool(_WORD_RE.match(c))


def directness_score(clue: str, target_words: List[str], embedder: Embedder) -> float:
    if embedder is None:
        return 0.0
    cvec = embedder.embed(clue)
    sims = []
    for t in target_words:
        tvec = embedder.embed(t)
        sims.append(Embedder.cosine(cvec, tvec))
    return float(max(sims) if sims else 0.0)


def is_valid_clue(
    clue: str,
    board_words: List[str],
    target_words: List[str],
    embedder: Optional[Embedder],
    cfg: Dict[str, Any],
) -> Tuple[bool, str, float]:
    cons = cfg["constraints"]
    clue = clue.strip()

    if cons.get("single_word_only", True) and not is_single_word(clue):
        return False, "not_single_word", 0.0

    if not looks_like_word_token(clue):
        return False, "bad_token_shape", 0.0

    if cons.get("ban_board_words", True) and violates_board_overlap(clue, board_words):
        return False, "board_word", 0.0

    if cons.get("ban_substrings", True) and violates_substring_ban(clue, board_words):
        return False, "substring", 0.0

    # Optional embedding-based directness constraint
    if not bool(cons.get("enable_directness_check", True)):
        return True, "ok", 0.0

    d = directness_score(clue, target_words, embedder)
    if d >= float(cons["tau_direct"]):
        return False, "too_direct", float(d)

    return True, "ok", float(d)


def score_turn(
    guesses: List[str],
    labels_by_word: Dict[str, str],
    cfg: Dict[str, Any],
) -> Tuple[float, Dict[str, Any]]:
    """
    guesses: normalized board words guessed in order (unique)
    labels_by_word: normalized_word -> label
    """
    r = cfg["reward"]
    stats = {"n_team": 0, "n_opp": 0, "n_neu": 0, "assassin": 0}

    reward = 0.0
    for g in guesses:
        lab = labels_by_word.get(g)
        if lab is None:
            continue
        if lab == "TEAM":
            reward += float(r["team_correct"])
            stats["n_team"] += 1
        elif lab == "OPP":
            reward += float(r["opp_wrong"])
            stats["n_opp"] += 1
        elif lab == "NEU":
            reward += float(r["neu_wrong"])
            stats["n_neu"] += 1
        elif lab == "ASSASSIN":
            reward += float(r["assassin_wrong"])
            stats["assassin"] += 1

    # optional shaping (off by default)
    reward += float(r.get("repeat_penalty", 0.0)) * 0.0
    reward += float(r.get("brevity_bonus", 0.0)) * 0.0

    return float(reward), stats


================================================
FILE: src/spymaster_prompt.py
================================================
from __future__ import annotations

from typing import Dict, List, Any


def build_spymaster_messages(
    board_words: List[str],
    labels: List[str],
    revealed_mask: List[bool],
    cfg: Dict[str, Any],
) -> List[Dict[str, str]]:
    system = cfg.get("qwen", {}).get("system_spymaster", "You are the Spymaster in a Codenames-like game.")

    words_lines = []
    for i, w in enumerate(board_words):
        tag = "REVEALED" if revealed_mask[i] else "HIDDEN"
        words_lines.append(f"{i+1:02d}. {w} [{tag}]")

    team_words = [w for w, lab, rev in zip(board_words, labels, revealed_mask) if (lab == "TEAM" and not rev)]
    opp_words = [w for w, lab, rev in zip(board_words, labels, revealed_mask) if (lab == "OPP" and not rev)]
    neu_words = [w for w, lab, rev in zip(board_words, labels, revealed_mask) if (lab == "NEU" and not rev)]
    ass_words = [w for w, lab, rev in zip(board_words, labels, revealed_mask) if (lab == "ASSASSIN" and not rev)]

    tau = cfg["constraints"]["tau_direct"]

    user = f"""
BOARD WORDS (25):
{chr(10).join(words_lines)}

YOUR TARGETS (TEAM) — choose a clue that points to some of these:
{", ".join(team_words)}

DANGEROUS / FORBIDDEN (avoid leading the guesser to these):
OPP: {", ".join(opp_words)}
NEU: {", ".join(neu_words)}
ASSASSIN: {", ".join(ass_words)}

CONSTRAINTS:
- Output exactly ONE clue word (single token-like word).
- The clue must NOT be any board word.
- Avoid substrings of board words and vice versa.

GOAL:
Produce an indirect but helpful clue that helps a frozen Guesser pick TEAM words while avoiding OPP/NEU/ASSASSIN.

OUTPUT FORMAT (exactly):
<think>
...
</think>
CLUE: <one_word>
NUM: <integer>

Now produce your clue.
"""
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]


================================================
FILE: src/think_utils.py
================================================
# src/think_utils.py
from __future__ import annotations

import re
from typing import Literal, Optional, Tuple

# Matches a CLOSED think block. Non-greedy so multiple blocks work.
_THINK_CLOSED_RE = re.compile(r"<think>(.*?)</think>", re.IGNORECASE | re.DOTALL)


Which = Literal["first", "last"]
Mode = Literal["inner", "full"]


def extract_think(
    text: str | None,
    *,
    mode: Mode = "inner",
    which: Which = "first",
    allow_partial: bool = False,
) -> str:
    """
    Extract content from <think>...</think>.

    mode:
      - "inner": returns content inside tags
      - "full": returns the whole block including <think>...</think>

    which:
      - "first" or "last" match when multiple blocks exist

    allow_partial:
      - If True, tolerates outputs with only <think> OR only </think>.
        * only </think>: treat everything up to </think> as thinking
        * only <think>: treat everything after <think> as thinking
    """
    t = text or ""
    if not t:
        return ""

    matches = list(_THINK_CLOSED_RE.finditer(t))
    if matches:
        m = matches[0] if which == "first" else matches[-1]
        if mode == "inner":
            return (m.group(1) or "").strip()
        return t[m.start() : m.end()].strip()

    if not allow_partial:
        return ""

    low = t.lower()
    lo = low.rfind("<think>")
    hi = low.rfind("</think>")

    # both present but regex didn't match (weird nesting/spacing) -> best-effort slice
    if lo != -1 and hi != -1 and hi > lo:
        start = lo if mode == "full" else (lo + len("<think>"))
        end = (hi + len("</think>")) if mode == "full" else hi
        return t[start:end].strip()

    # only closing tag present: keep everything up to it
    if hi != -1:
        end = hi + (len("</think>") if mode == "full" else 0)
        return t[:end].strip()

    # only opening tag present: keep everything after it
    if lo != -1:
        start = lo if mode == "full" else (lo + len("<think>"))
        return t[start:].strip()

    return ""


def strip_think_blocks(text: str | None, *, remove_dangling_tags: bool = True) -> str:
    """
    Remove all CLOSED <think>...</think> blocks.
    Optionally also remove dangling <think> or </think> tags.
    """
    t = text or ""
    if not t:
        return ""

    t = _THINK_CLOSED_RE.sub("", t)

    if remove_dangling_tags:
        # If a model emits a single dangling tag, remove it.
        t = re.sub(r"</?think>", "", t, flags=re.IGNORECASE)

    return t


def split_think_and_rest(
    text: str | None,
    *,
    which: Which = "first",
    allow_partial: bool = False,
) -> Tuple[str, str]:
    """
    Returns (think_inner, rest_without_closed_think_blocks).
    Useful when you want to log think separately but score/eval on the rest.
    """
    t = text or ""
    think = extract_think(t, mode="inner", which=which, allow_partial=allow_partial)
    rest = strip_think_blocks(t)
    return think, rest


================================================
FILE: src/train_lora_sft.py
================================================
# src/train_lora_sft.py
from __future__ import annotations

import argparse
import inspect
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List

import json
import math
import time

import torch
import torch.distributed as dist
from torch.utils.data import Dataset
from transformers.trainer_utils import get_last_checkpoint

from .utils import (
    load_yaml,
    read_jsonl,
    ensure_dir,
    save_config_snapshot,
    save_run_meta,
    set_global_seed,
)

from transformers import TrainerCallback
from .epoch_eval import (
    load_probe_samples,
    eval_gsm8k,
    eval_humaneval,
    eval_wikitext2_ppl,
    eval_codenames_subset,
    plot_epoch_history,
)


class SFTDataset(Dataset):
    """
    Dataset backed by pre-tokenized examples:
      item = {"input_ids": List[int], "labels": List[int]}
    """

    def __init__(self, tokenized: List[Dict[str, Any]]):
        self.items = tokenized

    def __len__(self) -> int:
        return len(self.items)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        ex = self.items[idx]
        input_ids = ex["input_ids"]
        labels = ex["labels"]
        attn = [1] * len(input_ids)

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long),
            "attention_mask": torch.tensor(attn, dtype=torch.long),
        }


@dataclass
class PadCollator:
    tokenizer: Any

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        input_ids = [b["input_ids"] for b in batch]
        labels = [b["labels"] for b in batch]
        attn = [b["attention_mask"] for b in batch]

        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)
        attn = torch.nn.utils.rnn.pad_sequence(attn, batch_first=True, padding_value=0)

        return {"input_ids": input_ids, "labels": labels, "attention_mask": attn}


def _load_or_build_tokcache(
    *,
    cache_path: Path,
    records: List[Dict[str, Any]],
    tokenizer,
    model_id: str,
    max_len: int,
) -> List[Dict[str, Any]]:
    """
    Cache format:
      {"model_id": str, "max_len": int, "tokenized": [{"input_ids": [...], "labels": [...]}, ...]}
    """
    if cache_path.exists():
        obj = torch.load(cache_path, map_location="cpu")
        if (
            isinstance(obj, dict)
            and obj.get("model_id") == model_id
            and int(obj.get("max_len", -1)) == int(max_len)
            and isinstance(obj.get("tokenized"), list)
        ):
            print(f"Loaded tokenization cache -> {cache_path}")
            return obj["tokenized"]

    tokenized: List[Dict[str, Any]] = []
    eos = tokenizer.eos_token_id

    for r in records:
        p = tokenizer(r["prompt"], add_special_tokens=False)["input_ids"]
        c = tokenizer(r["completion"], add_special_tokens=False)["input_ids"]

        input_ids = p + c + [eos]
        labels = [-100] * len(p) + c + [eos]

        input_ids = input_ids[:max_len]
        labels = labels[:max_len]

        tokenized.append({"input_ids": input_ids, "labels": labels})

    cache_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save({"model_id": model_id, "max_len": int(max_len), "tokenized": tokenized}, cache_path)
    print(f"Saved tokenization cache -> {cache_path}")
    return tokenized


def _disable_kv_cache(model: Any) -> None:
    """
    Disable KV cache during training to reduce VRAM usage.
    For decoder-only LMs, use_cache is useful for generation, not teacher-forced training.
    """
    try:
        if hasattr(model, "config") and hasattr(model.config, "use_cache"):
            model.config.use_cache = False
    except Exception:
        pass

    for attr in ("base_model", "model", "module"):
        try:
            m = getattr(model, attr, None)
            if m is not None and hasattr(m, "config") and hasattr(m.config, "use_cache"):
                m.config.use_cache = False
        except Exception:
            continue


def _normalize_attn_impl(raw: Any) -> str | None:
    """
    Transformers expects one of: eager, sdpa, flex_attention, flash_attention_2, flash_attention_3.
    Back-compat: map old kernels hub identifiers to the supported string.
    """
    if raw is None:
        return None
    s = str(raw).strip()
    if not s:
        return None

    low = s.lower()

    # Back-compat mapping (your old setting)
    if low in {"kernels-community/flash-attn2", "flash-attn2", "flash_attn2", "flashattn2"}:
        return "flash_attention_2"
    if low in {"kernels-community/flash-attn3", "flash-attn3", "flash_attn3", "flashattn3"}:
        return "flash_attention_3"

    if low in {"auto", "none", "null"}:
        return None

    # Pass-through for valid values
    allowed = {"eager", "sdpa", "flex_attention", "flash_attention_2", "flash_attention_3"}
    if s in allowed:
        return s
    if low in allowed:
        # preserve canonical lowercase form
        return low

    print(f"[warn] Unknown training.attn_implementation={s!r}; omitting attn_implementation.")
    return None


def _enable_gradient_checkpointing(model: Any, gc_kwargs: Dict[str, Any] | None) -> None:
    """
    Enable GC with compatibility across Transformers versions.
    Also ensures inputs require grads (important for PEFT+GC on some models).
    """
    try:
        if gc_kwargs is not None:
            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gc_kwargs)
        else:
            model.gradient_checkpointing_enable()
    except TypeError:
        # older signature: no kwargs
        model.gradient_checkpointing_enable()

    # PEFT + checkpointing often needs this for a grad path
    try:
        model.enable_input_require_grads()
    except Exception:
        pass


class EpochEvalCallback(TrainerCallback):
    """
    - Collects train loss / grad norm from on_log()
    - Runs GSM8K, HumanEval, WikiText-2 PPL, and Codenames subset eval at on_epoch_end()
    - Appends results to metrics_history.jsonl and rewrites plots

    IMPORTANT (DDP): we barrier at epoch boundaries so nonzero ranks don't
    run ahead into collectives while rank0 is doing long eval.
    """

    def __init__(self, cfg: Dict[str, Any], out_dir: Path):
        self.cfg = cfg
        self.out_dir = out_dir
        self.eval_dir = out_dir / "epoch_eval"
        self.history_path = self.eval_dir / "metrics_history.jsonl"
        self.plots_dir = self.eval_dir / "plots"
        self._tok = None

        # per-epoch log accumulation
        self._epoch_losses: List[float] = []
        self._epoch_grad_norms: List[float] = []

        self._ema = None
        self._ema_beta = 0.7  # epoch-level EMA smoothing

        # loaded once
        self.samples = None

    def _barrier(self) -> None:
        # Safe no-op when not running under torchrun/DDP
        if dist.is_available() and dist.is_initialized():
            dist.barrier()

    def on_train_begin(self, args, state, control, **kwargs):
        # Keep all ranks synchronized: rank0 may download datasets / do setup work.
        self._barrier()
        try:
            if not state.is_world_process_zero:
                return

            self.eval_dir.mkdir(parents=True, exist_ok=True)
            self.plots_dir.mkdir(parents=True, exist_ok=True)

            # probe sample sizes (you can tune these)
            seed = int(self.cfg["training"].get("seed", 0)) + 12345
            self.samples = load_probe_samples(
                seed=seed,
                gsm8k_n=int(self.cfg.get("epoch_eval", {}).get("gsm8k_n", 50)),
                humaneval_n=int(self.cfg.get("epoch_eval", {}).get("humaneval_n", 20)),
                wikitext_n=int(self.cfg.get("epoch_eval", {}).get("wikitext_n", 50)),
            )

            self._tok = kwargs.get("tokenizer") or kwargs.get("processing_class") or self._tok
        finally:
            # Release other ranks to start training
            self._barrier()

    def on_log(self, args, state, control, logs=None, **kwargs):
        if not state.is_world_process_zero:
            return
        if not logs:
            return
        # Trainer typically logs "loss" frequently
        if "loss" in logs:
            try:
                self._epoch_losses.append(float(logs["loss"]))
            except Exception:
                pass
        # grad_norm is only present if your Trainer/logging emits it
        if "grad_norm" in logs:
            try:
                self._epoch_grad_norms.append(float(logs["grad_norm"]))
            except Exception:
                pass

    def _append_history(self, row: Dict[str, Any]) -> None:
        self.history_path.parent.mkdir(parents=True, exist_ok=True)
        with self.history_path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    def on_epoch_end(self, args, state, control, **kwargs):
        # All ranks arrive here together.
        self._barrier()
        try:
            if not state.is_world_process_zero:
                # Nonzero ranks wait while rank0 runs the (potentially long) eval.
                return

            if self.samples is None:
                return

            model = kwargs.get("model", None)
            tokenizer = self._tok or kwargs.get("tokenizer") or kwargs.get("processing_class")
            if model is None or tokenizer is None:
                return

            # unwrap DDP
            if hasattr(model, "module"):
                model = model.module

            device = (
                model.device
                if hasattr(model, "device")
                else torch.device("cuda" if torch.cuda.is_available() else "cpu")
            )

            # Temporarily enable use_cache for faster generation (optional)
            orig_use_cache = None
            try:
                if hasattr(model, "config") and hasattr(model.config, "use_cache"):
                    orig_use_cache = model.config.use_cache
                    model.config.use_cache = True
            except Exception:
                pass

            # ---- train loss aggregation (from logs) ----
            loss_mean = (
                float(sum(self._epoch_losses) / max(1, len(self._epoch_losses)))
                if self._epoch_losses
                else float("nan")
            )
            self._epoch_losses.clear()

            if self._ema is None or math.isnan(self._ema):
                self._ema = loss_mean
            else:
                self._ema = self._ema_beta * self._ema + (1.0 - self._ema_beta) * loss_mean

            grad_mean = (
                float(sum(self._epoch_grad_norms) / max(1, len(self._epoch_grad_norms)))
                if self._epoch_grad_norms
                else float("nan")
            )
            self._epoch_grad_norms.clear()

            # ---- run probe evals ----
            # Keep these small so epoch end stays quick.
            gsm = eval_gsm8k(
                model=model,
                tokenizer=tokenizer,
                samples=self.samples.gsm8k,
                device=device,
                max_new_tokens=int(self.cfg.get("epoch_eval", {}).get("gsm8k_max_new_tokens", 384)),
            )
            he = eval_humaneval(
                model=model,
                tokenizer=tokenizer,
                samples=self.samples.humaneval,
                device=device,
                max_new_tokens=int(self.cfg.get("epoch_eval", {}).get("humaneval_max_new_tokens", 384)),
                timeout_s=int(self.cfg.get("epoch_eval", {}).get("humaneval_timeout_s", 3)),
            )
            wt = eval_wikitext2_ppl(
                model=model,
                tokenizer=tokenizer,
                texts=self.samples.wikitext,
                device=device,
                block_size=int(self.cfg.get("epoch_eval", {}).get("wikitext_block_size", 512)),
                max_blocks=int(self.cfg.get("epoch_eval", {}).get("wikitext_max_blocks", 80)),
            )

            cn = eval_codenames_subset(
                cfg=self.cfg,
                model=model,
                tokenizer=tokenizer,
                device=device,
                n_boards=int(self.cfg.get("epoch_eval", {}).get("codenames_n_boards", 100)),
                seed=int(self.cfg["training"].get("seed", 0)) + 999,
            )

            # restore cache
            try:
                if orig_use_cache is not None and hasattr(model, "config") and hasattr(model.config, "use_cache"):
                    model.config.use_cache = orig_use_cache
            except Exception:
                pass

            row = {
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "epoch": float(state.epoch) if state.epoch is not None else None,
                "train_loss_epoch_mean": loss_mean,
                "train_loss_epoch_ema": float(self._ema) if self._ema is not None else None,
                "grad_norm_epoch_mean": grad_mean if not math.isnan(grad_mean) else None,
                **gsm,
                **he,
                **wt,
                **cn,
            }

            self._append_history(row)
            plot_epoch_history(self.history_path, self.plots_dir)
        finally:
            # Release all other ranks (and keep barriers matched even if eval raises)
            self._barrier()


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True)
    args = ap.parse_args()

    cfg = load_yaml(args.config)
    tcfg = cfg["training"]
    set_global_seed(int(tcfg.get("seed", 0)))

    # -------- Gradient checkpointing config --------
    gc_enabled = bool(tcfg.get("gradient_checkpointing", False))
    gc_kwargs = None
    if gc_enabled:
        gc_kwargs = {"use_reentrant": bool(tcfg.get("gradient_checkpointing_use_reentrant", False))}

    # Load data
    records = read_jsonl(cfg["paths"]["sft_turns_path"])
    if not records:
        raise RuntimeError("No SFT records found. Did generate_sft_data produce an empty filtered set?")

    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
    from peft import LoraConfig, get_peft_model

    model_id = cfg["models"]["spymaster_model_id"]
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # dtype
    torch_dtype = None
    if bool(tcfg.get("bf16", False)) and torch.cuda.is_available():
        torch_dtype = torch.bfloat16
    elif bool(tcfg.get("fp16", False)) and torch.cuda.is_available():
        torch_dtype = torch.float16

    # Attention implementation
    attn_impl = _normalize_attn_impl(tcfg.get("attn_implementation", "sdpa"))

    # Build model
    model_kwargs: Dict[str, Any] = {"torch_dtype": torch_dtype}
    if attn_impl is not None:
        model_kwargs["attn_implementation"] = attn_impl

    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)

    # Disable KV cache for training (and for gradient checkpointing)
    _disable_kv_cache(model)

    # Enable gradient checkpointing on the base model (before PEFT wrap)
    if gc_enabled:
        _enable_gradient_checkpointing(model, gc_kwargs)

    # LoRA
    lora_cfg = LoraConfig(
        r=int(tcfg["r"]),
        lora_alpha=int(tcfg["alpha"]),
        lora_dropout=float(tcfg["dropout"]),
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=tcfg.get("target_modules", None),
    )
    model = get_peft_model(model, lora_cfg)

    if torch_dtype is not None:
        model = model.to(dtype=torch_dtype)

    # Disable KV cache again after wrapping + re-enable GC (some wrappers/models need it)
    _disable_kv_cache(model)
    if gc_enabled:
        _enable_gradient_checkpointing(model, gc_kwargs)

    out_dir = ensure_dir(tcfg["output_adapter_dir"])

    # -------------------------
    # Pre-tokenize once + cache
    # -------------------------
    max_len = int(tcfg["max_seq_len"])
    sft_path = Path(cfg["paths"]["sft_turns_path"])
    cache_path = sft_path.with_suffix(sft_path.suffix + f".tokcache.maxlen{max_len}.pt")

    tokenized = _load_or_build_tokcache(
        cache_path=cache_path,
        records=records,
        tokenizer=tokenizer,
        model_id=model_id,
        max_len=max_len,
    )

    ds = SFTDataset(tokenized)
    collator = PadCollator(tokenizer)

    # TrainingArguments: be compatible across versions wrt gradient_checkpointing_kwargs
    ta_kwargs: Dict[str, Any] = dict(
        output_dir=str(out_dir),
        per_device_train_batch_size=int(tcfg["batch_size"]),
        gradient_accumulation_steps=int(tcfg["grad_accum"]),
        num_train_epochs=float(tcfg["epochs"]),
        learning_rate=float(tcfg["lr"]),

        logging_strategy="epoch",
        logging_first_step=True,
        # logging_steps=20,  # optional; mostly ignored when logging_strategy="epoch"

        save_steps=200,
        save_total_limit=2,
        report_to=[],
        bf16=bool(tcfg.get("bf16", False)),
        fp16=bool(tcfg.get("fp16", False)),
        optim="adamw_torch",
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        ddp_find_unused_parameters=False,
        gradient_checkpointing=gc_enabled,
    )

    # Only pass gradient_checkpointing_kwargs if TrainingArguments supports it
    try:
        sig = inspect.signature(TrainingArguments.__init__)
        if gc_enabled and gc_kwargs is not None and "gradient_checkpointing_kwargs" in sig.parameters:
            ta_kwargs["gradient_checkpointing_kwargs"] = gc_kwargs
    except Exception:
        # If signature introspection fails, just don't pass kwargs
        pass

    args_tr = TrainingArguments(**ta_kwargs)

    trainer = Trainer(
        model=model,
        args=args_tr,
        train_dataset=ds,
        data_collator=collator,
        tokenizer=tokenizer,
    )

    # Add epoch-end evaluation + plotting
    trainer.add_callback(EpochEvalCallback(cfg, out_dir))

    last_ckpt = get_last_checkpoint(str(out_dir))
    trainer.train(resume_from_checkpoint=last_ckpt if last_ckpt else None)

    # Save adapter
    if trainer.is_world_process_zero():
        m = trainer.model
        if hasattr(m, "module"):  # DDP wrap
            m = m.module
        m.save_pretrained(str(out_dir))
        tokenizer.save_pretrained(str(out_dir))

        # Save config + run meta
        save_config_snapshot(cfg, out_dir)
        save_run_meta(out_dir, command=["python", "-m", "src.train_lora_sft", "--config", args.config])

    print(f"Saved LoRA adapter -> {out_dir}")


if __name__ == "__main__":
    main()


================================================
FILE: src/utils.py
================================================
import json
import os
import random
import subprocess
import time
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional

import numpy as np
import yaml

def now_iso() -> str:
    # Nice for logs/progress: 2026-02-20T13:45:12
    return time.strftime("%Y-%m-%dT%H:%M:%S")


def save_progress(
    progress_path: str | Path,
    *,
    done: int,
    total: int,
    last_example_id: Optional[str] = None,
    last_board_id: Optional[str] = None,
    mean_reward: Optional[float] = None,
    extra: Optional[Dict[str, Any]] = None,
) -> None:
    """
    Atomic JSON progress writer (safe for crashes).
    """
    progress_path = Path(progress_path)
    progress_path.parent.mkdir(parents=True, exist_ok=True)

    payload: Dict[str, Any] = {
        "timestamp": now_iso(),
        "done": int(done),
        "total": int(total),
        "pct": float(done / total) if total else 0.0,
        "last_example_id": last_example_id,
        "last_board_id": last_board_id,
        "mean_reward_running": float(mean_reward) if mean_reward is not None else None,
    }
    if extra:
        payload.update(extra)

    tmp = progress_path.with_suffix(progress_path.suffix + ".tmp")
    tmp.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    os.replace(tmp, progress_path)

def load_yaml(path: str | Path) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        obj = yaml.safe_load(f) or {}
    if not isinstance(obj, dict):
        raise ValueError(f"YAML config must be a mapping/dict: {path}")
    return obj


def ensure_dir(path: str | Path) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p


def read_jsonl(path: str | Path) -> List[Dict[str, Any]]:
    records: List[Dict[str, Any]] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            records.append(json.loads(line))
    return records


def write_jsonl(path: str | Path, records: Iterable[Dict[str, Any]]) -> None:
    path = Path(path)
    ensure_dir(path.parent)
    with open(path, "w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def append_jsonl(path: str | Path, obj: Dict[str, Any], *, do_fsync: bool = True) -> None:
    path = Path(path)
    if path.parent and str(path.parent) != "":
        ensure_dir(path.parent)

    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")
        f.flush()
        if do_fsync:
            os.fsync(f.fileno())

def set_global_seed(seed: int) -> None:
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    try:
        import torch
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    except Exception:
        pass


def now_ts() -> str:
    return time.strftime("%Y%m%d_%H%M%S")


def try_get_git_hash() -> Optional[str]:
    try:
        out = subprocess.check_output(["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL)
        return out.decode("utf-8").strip()
    except Exception:
        return None


def save_config_snapshot(cfg: Dict[str, Any], out_dir: str | Path, filename: str = "config_snapshot.yaml") -> None:
    out_dir = ensure_dir(out_dir)
    p = out_dir / filename
    with open(p, "w", encoding="utf-8") as f:
        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)


def save_run_meta(out_dir: str | Path, command: List[str] | None = None, extra: Dict[str, Any] | None = None) -> None:
    out_dir = ensure_dir(out_dir)
    meta: Dict[str, Any] = {
        "timestamp": now_ts(),
        "git_hash": try_get_git_hash(),
        "command": command,
    }
    if extra:
        meta.update(extra)
    with open(out_dir / "run_meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2)

